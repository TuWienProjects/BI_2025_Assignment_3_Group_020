{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f7d15c",
   "metadata": {},
   "source": [
    "First, create a new conda environment named BI2025 and install the required packages from requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda create -n BI2025 python=3.11 -y\n",
    "#!conda activate BI2025\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5122654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY OR COPY THIS CELL!! \n",
    "# Note: The only imports allowed are Python's standard library, pandas, numpy, scipy, matplotlib, seaborn and scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import typing\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "from starvers.starvers import TripleStoreEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be80f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom imports here \n",
    "\n",
    "import datetime\n",
    "from datetime import timezone\n",
    "\n",
    "import uuid\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report,confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79408d3",
   "metadata": {},
   "source": [
    "## Graph-based documentation preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831a95c",
   "metadata": {},
   "source": [
    "**!!!IMPORTANT!!!**\n",
    "\n",
    "Everytime you work on this notebook, enter your student ID in the `executed_by` variable so that the cell executions are accredited to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a02423",
   "metadata": {},
   "outputs": [],
   "source": [
    "executed_by ='stud-id_52400204'  # Replace the digits after \"id_\" with your own student ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2160a7",
   "metadata": {},
   "source": [
    "Set your group and student IDs. Do this only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16721334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group id for this project\n",
    "group_id = '20'  # Replace the digits with your group id\n",
    "\n",
    "# Students working on this notebook\n",
    "student_a = 'stud-id_52400204'  # Replace the digits after \"id_\" with student A's student ID\n",
    "student_b = 'stud-id_12432813'  # Replace the digits after \"id_\" with student B's student ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb927186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roles. Don't change these values.\n",
    "code_writer_role = 'code_writer'\n",
    "code_executor_role = 'code_executor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e253f6",
   "metadata": {},
   "source": [
    "Setup the starvers API for logging your steps into our server-sided graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025\"\n",
    "post_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025/statements\"\n",
    "engine = TripleStoreEngine(get_endpoint, post_endpoint, skip_connection_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cee91",
   "metadata": {},
   "source": [
    "Use these prefixes in your notebooks. You can extend this dict with your prefixes of additional ontologies that you use in this notebook. Replace 00 with your group id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e6f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = {\n",
    "    'xsd': 'http://www.w3.org/2001/XMLSchema#',\n",
    "    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "    'foaf': 'http://xmlns.com/foaf/0.1/',\n",
    "    'prov': 'http://www.w3.org/ns/prov#',\n",
    "    'sc': 'https://schema.org/',\n",
    "    'cr': 'http://mlcommons.org/croissant/',\n",
    "    'mls': 'http://www.w3.org/ns/mls#',\n",
    "    'mlso': 'http://w3id.org/mlso',\n",
    "    'siu': 'https://si-digital-framework.org/SI/units/',\n",
    "    'siq': 'https://si-digital-framework.org/SI/quantities/',\n",
    "    'qudt': 'http://qudt.org/schema/qudt/',\n",
    "    '': f'https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/',\n",
    "}\n",
    "\n",
    "prefix_header = '\\n'.join([f'PREFIX {k}: <{v}>' for k, v in prefixes.items()]) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970468d",
   "metadata": {},
   "source": [
    "Ontologies to use\n",
    "* Provenance of the experiment process\n",
    "    * PROV-O: \n",
    "        * doc: https://www.w3.org/TR/prov-o/\n",
    "        * serialization: https://www.w3.org/ns/prov-o\n",
    "* Data used and created\n",
    "    * schema.org - Dataset: \n",
    "        * doc: https://schema.org/Dataset\n",
    "        * serialization: https://schema.org/version/latest/schemaorg-current-https.ttl\n",
    "    * Crossaint\n",
    "        * doc: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n",
    "        * serialization: https://github.com/mlcommons/croissant/blob/main/docs/croissant.ttl\n",
    "* ML experiments performed\n",
    "    * MLSO: \n",
    "        * doc: https://github.com/dtai-kg/MLSO\n",
    "        * doc: https://dtai-kg.github.io/MLSO/#http://w3id.org/\n",
    "        * serialization: https://dtai-kg.github.io/MLSO/ontology.ttl\n",
    "* Measurements, Metrics, Units\n",
    "    * QUDT\n",
    "        * doc:https://qudt.org/\n",
    "        * doc: https://github.com/qudt/qudt-public-repo\n",
    "        * serialization: https://github.com/qudt/qudt-public-repo/blob/main/src/main/rdf/schema/SCHEMA_QUDT.ttl\n",
    "    * SI Digital Framework\n",
    "        * doc: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/docs/README.md\n",
    "        * doc: https://si-digital-framework.org/\n",
    "        * doc: https://si-digital-framework.org/SI\n",
    "        * serialization: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/TTL/si.ttl\n",
    "    * Quantities and Units\n",
    "        * doc: https://www.omg.org/spec/Commons\n",
    "        * serialization: https://www.omg.org/spec/Commons/QuantitiesAndUnits.ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62393d",
   "metadata": {},
   "source": [
    "Use this function to record execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time in ISO 8601 format with UTC timezone in the following format:\n",
    "    YYYY-MM-DDTHH:MM:SS.sssZ\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    timestamp_formated = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]  +\"Z\"\n",
    "\n",
    "    return timestamp_formated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a1605",
   "metadata": {},
   "source": [
    "Register yourself in the Knowledge Graph using ProvO. Change the given name, family name and immatriculation number to reflect your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ontologies used: foaf, prov, IAO\n",
    "reigstration_triples_a = [\n",
    "f':{student_a} rdf:type foaf:Person .',\n",
    "f':{student_a} rdf:type prov:Agent .',\n",
    "f':{student_a} foaf:givenName \"Muhammad Sajid\" .',\n",
    "f':{student_a} foaf:familyName \"Bashir\" .',\n",
    "f':{student_a} <http://vivoweb.org/ontology/core#identifier> :{student_a} .',\n",
    "f':{student_a} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_a} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "f':{student_a} <http://purl.obolibrary.org/obo/IAO_0000219> \"52400204\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "reigstration_triples_b = [\n",
    "f':{student_b} rdf:type foaf:Person .',\n",
    "f':{student_b} rdf:type prov:Agent .',\n",
    "f':{student_b} foaf:givenName \"Eman\" .',\n",
    "f':{student_b} foaf:familyName \"Shahin\" .',\n",
    "f':{student_b} <http://vivoweb.org/ontology/core#identifier> :{student_b} .',\n",
    "f':{student_b} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_b} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "f':{student_b} <http://purl.obolibrary.org/obo/IAO_0000219> \"12432813\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "role_triples = [\n",
    "    f':{code_writer_role} rdf:type prov:Role .',\n",
    "    f':{code_executor_role} rdf:type prov:Role .',\n",
    "]\n",
    "\n",
    "\n",
    "engine.insert(reigstration_triples_a, prefixes=prefixes)\n",
    "engine.insert(reigstration_triples_b, prefixes=prefixes)\n",
    "engine.insert(role_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c479ed4",
   "metadata": {},
   "source": [
    "**What not do do**\n",
    "\n",
    "Do not use [blank nodes](https://www.w3.org/wiki/BlankNodes).\n",
    "\n",
    "PROV-O uses blank nodes to connect multiple elements with each other.\n",
    "Such blank nodes (such as _:association) should not be used.\n",
    "Instead, assign a fixed node ID such as\n",
    ":5119fcd7-b571-41e0-9464-a37c7be0f574 by generating them outside of the\n",
    "notebook.\n",
    "We suggest that, for each setting where such a blank node is needed to\n",
    "connect multiple elements, you create a unique hash (using uuid.uuid4())\n",
    "and keep this as hard-coded identifier for the blank node. The template\n",
    "notebook contains examples of this. Do *not* use these provided values,\n",
    "as otherwise, your provenance documentations will all be connected via\n",
    "these identifiers!\n",
    "Also, do not generate them dynamically in every cell execution, e.g. by\n",
    "using uuid.uuid4() in a cell. This would generate many new linking nodes\n",
    "for connecting the same elements.\n",
    "Compute one for each node (cell) where you need them and make sure to\n",
    "use the same one on each re-execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather_data_path = os.path.join(\"data\", \"datasets\", \"weather\")\n",
    "#cyclists_data_path = os.path.join(\"data\", \"datasets\", \"cyclists\")\n",
    "\n",
    "# Paths to the dataset (Human Activity Recognition with Smartphones)\n",
    "activity_data_path = os.path.join(\"data\", \"harws\")\n",
    "\n",
    "# Full paths to the train and test datasets\n",
    "train_data_path = os.path.join(activity_data_path, \"train.csv\")\n",
    "test_data_path = os.path.join(activity_data_path, \"test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee069d",
   "metadata": {},
   "source": [
    "## Business Understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634828d8",
   "metadata": {},
   "source": [
    "#### Activity Recording Helper Functions\n",
    "\n",
    "These utility functions standardize provenance documentation across all CRISP-DM sections, ensuring consistent structure, proper role attribution, and time tracking throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Following globals are expected here in helpers: engine, prefixes, prefix_header, student_a, student_b, UUID_NAMESPACE, code_writer_role, code_executor_role, executed_by\n",
    "\n",
    "# this provides a default uuid namespace if not defined earlier\n",
    "if 'UUID_NAMESPACE' not in globals():\n",
    "    UUID_NAMESPACE = uuid.NAMESPACE_URL\n",
    "\n",
    "def _det_uuid(name: str) -> str:\n",
    "    return str(uuid.uuid5(UUID_NAMESPACE, name))\n",
    "\n",
    "# Keeps deterministic UUID behavior based on the shared UUID_NAMESPACE\n",
    "def deterministic_uuid(name: str) -> str:\n",
    "    return _det_uuid(name)\n",
    "\n",
    "\n",
    "# Function to escape special characters in RDF literals\n",
    "def escape_rdf_literal(text: str) -> str:\n",
    "    \"\"\"Escape backslashes, quotes, newlines, and colons for SPARQL/RDF literals.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Escape in order: backslash first, then quotes, colons, and whitespace\n",
    "    text = text.replace('\\\\', '\\\\\\\\')  # Escape backslashes first\n",
    "    text = text.replace('\"', '\\\\\"')    # Escape quotes\n",
    "    text = text.replace(':', '&#58;')  # Escape colons i.e use HTML entity\n",
    "    text = text.replace('\\n', ' ')     # Replace newlines with spaces\n",
    "    text = text.replace('\\r', '')      # Remove carriage returns\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def record_activity(activity_id: str = None, activity_label: str = None, label: str = None,\n",
    "                     code_writer: str = None, code_writers: list = None, code_executor: str = None,\n",
    "                     start_time: str = None, end_time: str = None, \n",
    "                     description: str = None, comment: str = None,\n",
    "                     parent_activity: str = None, phase_activity_iri: str = None,\n",
    "                     inputs: list = None, outputs: list = None, deterministic: bool = False):\n",
    "    \"\"\"\n",
    "    Records a generic PROV activity with flexible role handling.\n",
    "    \n",
    "    Parameters:\n",
    "    - activity_id: unique activity identifier (used for deterministic UUID if deterministic=True)\n",
    "    - activity_label or label: human-readable label\n",
    "    - code_writer: single code writer id (converted to list internally)\n",
    "    - code_writers: list of code writer ids (if code_writer not provided)\n",
    "    - code_executor: executor id (defaults to global executed_by if None)\n",
    "    - start_time, end_time: ISO datetime strings\n",
    "    - description or comment: activity description\n",
    "    - parent_activity or phase_activity_iri: parent activity IRI\n",
    "    - inputs: list of input entity identifiers\n",
    "    - outputs: list of output entity identifiers\n",
    "    - deterministic: if True, return tuple (triples, executor_uuid, writer_uuid) for backward compatibility\n",
    "    \n",
    "    Returns: (activity_triples: list[str], activity_iri_or_executor_uuid, writer_uuid)\n",
    "    \"\"\"\n",
    "    # Normalize parameters (support both old and new signatures)\n",
    "    act_id = activity_id or label\n",
    "    act_label = activity_label or label or act_id\n",
    "    writers = [code_writer] if code_writer else (code_writers or [student_a])\n",
    "    executor = code_executor or executed_by\n",
    "    desc = description or comment\n",
    "    parent = parent_activity or phase_activity_iri\n",
    "    \n",
    "    # Escape special characters in RDF literals\n",
    "    act_label_escaped = escape_rdf_literal(act_label)\n",
    "    desc_escaped = escape_rdf_literal(desc) if desc else None\n",
    "\n",
    "    # Generate deterministic UUIDs for activity and associations\n",
    "    activity_uuid = _det_uuid(f\"act::{act_id}\")\n",
    "    activity_iri = f\":{activity_uuid}\"  # Use : prefix for activity IRI\n",
    "\n",
    "    # Create activity triples\n",
    "    triples = [\n",
    "        f\"{activity_iri} rdf:type prov:Activity .\",\n",
    "        f\"{activity_iri} rdfs:label \\\"{act_label_escaped}\\\" .\"\n",
    "    ]\n",
    "\n",
    "    if desc_escaped:\n",
    "        triples.append(f\"{activity_iri} rdfs:comment \\\"{desc_escaped}\\\" .\")\n",
    "\n",
    "    # Optional temporal bounds\n",
    "    if start_time:\n",
    "        triples.append(f\"{activity_iri} prov:startedAtTime \\\"{start_time}\\\"^^xsd:dateTime .\")\n",
    "    if end_time:\n",
    "        triples.append(f\"{activity_iri} prov:endedAtTime \\\"{end_time}\\\"^^xsd:dateTime .\")\n",
    "\n",
    "    # Link to parent activity if provided\n",
    "    if parent:\n",
    "        triples.append(f\"{activity_iri} prov:wasInformedBy {parent} .\")\n",
    "\n",
    "    # Qualified associations for code writers\n",
    "    writer_uuids = []\n",
    "    for idx, writer in enumerate(writers):\n",
    "        writer_ass_uuid = _det_uuid(f\"assoc::{act_id}::writer::{writer}::{idx}\")\n",
    "        writer_uuids.append(writer_ass_uuid)\n",
    "        qn_writer = f\":qual_assoc_writer_{writer_ass_uuid}\"\n",
    "        triples.extend([\n",
    "            f\"{activity_iri} prov:qualifiedAssociation {qn_writer} .\",\n",
    "            f\"{qn_writer} rdf:type prov:Association .\",\n",
    "            f\"{qn_writer} rdfs:label \\\"code writer\\\" .\",\n",
    "            f\"{qn_writer} prov:hadRole {code_writer_role} .\",\n",
    "            f\"{qn_writer} prov:agent :{writer} .\",\n",
    "        ])\n",
    "\n",
    "    # Qualified association for executor\n",
    "    exec_uuid = _det_uuid(f\"assoc::{act_id}::executor::{executor}\")\n",
    "    qn_exec = f\":qual_assoc_executor_{exec_uuid}\"\n",
    "    triples.extend([\n",
    "        f\"{activity_iri} prov:qualifiedAssociation {qn_exec} .\",\n",
    "        f\"{qn_exec} rdf:type prov:Association .\",\n",
    "        f\"{qn_exec} rdfs:label \\\"code executor\\\" .\",\n",
    "        f\"{qn_exec} prov:hadRole {code_executor_role} .\",\n",
    "        f\"{qn_exec} prov:agent :{executor} .\",\n",
    "    ])\n",
    "\n",
    "    # Add input entities\n",
    "    if inputs:\n",
    "        for inp in inputs:\n",
    "            inp_iri = f\":{inp}\"\n",
    "            triples.append(f\"{activity_iri} prov:used {inp_iri} .\")\n",
    "\n",
    "    # Add output entities\n",
    "    if outputs:\n",
    "        for out in outputs:\n",
    "            out_iri = f\":{out}\"\n",
    "            triples.append(f\"{activity_iri} prov:wasGeneratedBy {out_iri} .\")\n",
    "\n",
    "    # Return format: support both old (activity_triples, activity_iri) and new (activity_triples, executor_uuid, writer_uuid)\n",
    "    if deterministic:\n",
    "        # Return tuple compatible with: triples, executor_uuid, writer_uuid = record_activity(...)\n",
    "        return triples, exec_uuid, (writer_uuids[0] if writer_uuids else None)\n",
    "    else:\n",
    "        return triples, activity_iri\n",
    "\n",
    "\n",
    "def record_entity(activity_id: str = None, label: str = None, comment: str = None):\n",
    "    \"\"\"Record a PROV entity.\"\"\"\n",
    "    entity_id = activity_id or label\n",
    "    entity_uuid = _det_uuid(f\"ent::{entity_id}\")\n",
    "    entity_iri = f\":{entity_uuid}\"\n",
    "    comment_escaped = escape_rdf_literal(comment) if comment else None\n",
    "    triples = [\n",
    "        f\"{entity_iri} rdf:type prov:Entity .\",\n",
    "        f\"{entity_iri} rdfs:label \\\"{label or entity_id}\\\" .\"\n",
    "    ]\n",
    "    if comment_escaped:\n",
    "        triples.append(f\"{entity_iri} rdfs:comment \\\"{comment_escaped}\\\" .\")\n",
    "    return triples, entity_iri\n",
    "\n",
    "\n",
    "def record_interpretation(activity_id: str = None, activity_label: str = None, \n",
    "                         code_executor: str = None,\n",
    "                         interpretation_text: str = None, text: str = None,\n",
    "                         inputs: list = None, output_entity: str = None,\n",
    "                         parent_activity: str = None, deterministic: bool = False):\n",
    "    \"\"\"Record an interpretation (specialized entity generated by an activity).\"\"\"\n",
    "    interp_id = activity_id or activity_label\n",
    "    interp_text = interpretation_text or text or \"\"\n",
    "    # Escape special characters in RDF literals\n",
    "    interp_text_escaped = escape_rdf_literal(interp_text)\n",
    "    executor = code_executor or executed_by\n",
    "    \n",
    "    # Create interpretation activity\n",
    "    interp_uuid = _det_uuid(f\"interp::{interp_id}\")\n",
    "    interp_iri = f\":{interp_uuid}\"\n",
    "    \n",
    "    triples = [\n",
    "        f\"{interp_iri} rdf:type prov:Activity .\",\n",
    "        f\"{interp_iri} rdfs:label \\\"{escape_rdf_literal(activity_label or interp_id)}\\\" .\",\n",
    "        f\"{interp_iri} rdfs:comment \\\"{interp_text_escaped}\\\" .\"\n",
    "    ]\n",
    "    \n",
    "    if parent_activity:\n",
    "        triples.append(f\"{interp_iri} prov:wasInformedBy {parent_activity} .\")\n",
    "    \n",
    "    # Executor association\n",
    "    exec_ass_uuid = _det_uuid(f\"assoc::{interp_id}::executor::{executor}\")\n",
    "    qn_exec = f\":qual_exec_{exec_ass_uuid}\"\n",
    "    triples.extend([\n",
    "        f\"{interp_iri} prov:qualifiedAssociation {qn_exec} .\",\n",
    "        f\"{qn_exec} rdf:type prov:Association .\",\n",
    "        f\"{qn_exec} prov:hadRole {code_executor_role} .\",\n",
    "        f\"{qn_exec} prov:agent :{executor} .\",\n",
    "    ])\n",
    "    \n",
    "    # Input entities\n",
    "    if inputs:\n",
    "        for inp in inputs:\n",
    "            inp_iri = f\":{inp}\"\n",
    "            triples.append(f\"{interp_iri} prov:used {inp_iri} .\")\n",
    "    \n",
    "    # Output entity \n",
    "    if output_entity:\n",
    "        out_iri = f\":{output_entity}\"\n",
    "        triples.append(f\"{interp_iri} prov:generated {out_iri} .\")\n",
    "    \n",
    "    if deterministic:\n",
    "        return triples, interp_uuid\n",
    "    return triples, interp_iri\n",
    "\n",
    "\n",
    "def record_decision(activity_id: str = None, activity_label: str = None, \n",
    "                   code_executor: str = None,\n",
    "                   text: str = None, comment: str = None,\n",
    "                   inputs: list = None, output_entity: str = None,\n",
    "                   parent_activity: str = None, deterministic: bool = False):\n",
    "    \"\"\"Record a decision (specialized interpretation/entity).\"\"\"\n",
    "    dec_id = activity_id or activity_label\n",
    "    dec_text = text or comment or \"\"\n",
    "    # Escape special characters in RDF literals\n",
    "    dec_text_escaped = escape_rdf_literal(dec_text)\n",
    "    executor = code_executor or executed_by\n",
    "    \n",
    "    dec_uuid = _det_uuid(f\"decision::{dec_id}\")\n",
    "    dec_iri = f\":{dec_uuid}\"\n",
    "    \n",
    "    triples = [\n",
    "        f\"{dec_iri} rdf:type prov:Activity .\",\n",
    "        f\"{dec_iri} rdfs:label \\\"{escape_rdf_literal(activity_label or dec_id)}\\\" .\",\n",
    "        f\"{dec_iri} rdfs:comment \\\"{dec_text_escaped}\\\" .\"\n",
    "    ]\n",
    "    \n",
    "    if parent_activity:\n",
    "        triples.append(f\"{dec_iri} prov:wasInformedBy {parent_activity} .\")\n",
    "    \n",
    "    # Executor association\n",
    "    exec_ass_uuid = _det_uuid(f\"assoc::{dec_id}::executor::{executor}\")\n",
    "    qn_exec = f\":qual_exec_{exec_ass_uuid}\"\n",
    "    triples.extend([\n",
    "        f\"{dec_iri} prov:qualifiedAssociation {qn_exec} .\",\n",
    "        f\"{qn_exec} rdf:type prov:Association .\",\n",
    "        f\"{qn_exec} prov:hadRole {code_executor_role} .\",\n",
    "        f\"{qn_exec} prov:agent :{executor} .\",\n",
    "    ])\n",
    "    \n",
    "    if inputs:\n",
    "        for inp in inputs:\n",
    "            inp_iri = f\":{inp}\"\n",
    "            triples.append(f\"{dec_iri} prov:used {inp_iri} .\")\n",
    "    \n",
    "    if output_entity:\n",
    "        out_iri = f\":{output_entity}\"\n",
    "        triples.append(f\"{dec_iri} prov:generated {out_iri} .\")\n",
    "    \n",
    "    if deterministic:\n",
    "        return triples, dec_uuid\n",
    "    return triples, dec_iri\n",
    "\n",
    "\n",
    "def now() -> str:\n",
    "    \"\"\"Return current UTC time in ISO 8601 format with Z suffix.\"\"\"\n",
    "    #return datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n",
    "    return datetime.datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace('+00:00', 'Z')\n",
    "\n",
    "\n",
    "def now_iso() -> str:\n",
    "    \"\"\"Alias for now().\"\"\"\n",
    "    return now()\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded: record_activity, record_entity, record_interpretation, record_decision, now, now_iso\")\n",
    "\n",
    "print(\"Helper functions loaded: flexible role handling + deterministic_uuid + proper RDF escaping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d05661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Understanding phase setup: writers, executor, and phase activity\n",
    "data_understanding_code_writer = student_a  # Sajid (A)\n",
    "data_understanding_code_writer_b = student_b  # Shahin (B, assisting)\n",
    "data_understanding_executor = executed_by  # current executor\n",
    "du_phase_uuid_writer_a = deterministic_uuid(\"data_understanding_phase:writer:student_a\")\n",
    "du_phase_uuid_writer_b = deterministic_uuid(\"data_understanding_phase:writer:student_b\")\n",
    "du_phase_uuid_executor = deterministic_uuid(\"data_understanding_phase:executor\")\n",
    "\n",
    "data_understanding_phase_activity = [\n",
    "    f':data_understanding_phase rdf:type prov:Activity .',\n",
    "    f':data_understanding_phase rdfs:label \"Data Understanding Phase\" .',\n",
    "    # Writer A\n",
    "    f':data_understanding_phase prov:qualifiedAssociation :{du_phase_uuid_writer_a} .',\n",
    "    f':{du_phase_uuid_writer_a} prov:agent :{data_understanding_code_writer} .',\n",
    "    f':{du_phase_uuid_writer_a} rdf:type prov:Association .',\n",
    "    f':{du_phase_uuid_writer_a} prov:hadRole :{code_writer_role} .',\n",
    "    # Writer B\n",
    "    f':data_understanding_phase prov:qualifiedAssociation :{du_phase_uuid_writer_b} .',\n",
    "    f':{du_phase_uuid_writer_b} prov:agent :{data_understanding_code_writer_b} .',\n",
    "    f':{du_phase_uuid_writer_b} rdf:type prov:Association .',\n",
    "    f':{du_phase_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "    # Executor\n",
    "    f':data_understanding_phase prov:qualifiedAssociation :{du_phase_uuid_executor} .',\n",
    "    f':{du_phase_uuid_executor} prov:agent :{data_understanding_executor} .',\n",
    "    f':{du_phase_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{du_phase_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(data_understanding_phase_activity, prefixes=prefixes)\n",
    "\n",
    "print(\"Data Understanding phase initialized with writers A & B and executor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee88389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Business Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':business_understanding_phase rdf:type prov:Activity .',\n",
    "f':business_understanding_phase rdfs:label \"Business Understanding Phase\" .', ## Phase 1: Business Understanding\n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc8a3a-708a-4992-a076-038c53338e89",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9bd9643d1e26a8dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "data_src_and_scenario_comment = \"\"\"\n",
    "The dataset used in this project is the Human Activity Recognition with Smartphones dataset, \n",
    "published by UCI and available on Kaggle. It contains sensor measurements collected from \n",
    "30 volunteers wearing a Samsung Galaxy S II smartphone on their waist while performing \n",
    "six everyday activities: walking, walking upstairs, walking downstairs, sitting, standing, \n",
    "and laying. The phone recorded accelerometer and gyroscope readings at 50 Hz, and the \n",
    "researchers transformed these raw signals into 561 numerical features.\n",
    "\n",
    "Scenario:\n",
    "A realistic scenario for this dataset is the development of applications that can automatically \n",
    "recognize what a user is doing based only on their smartphone sensors. This could be useful in \n",
    "many areas — for example, health monitoring, fitness apps, or detecting inactivity in elderly care. \n",
    "There are also commercial and marketing-related possibilities, such as tailoring notifications or \n",
    "recommendations based on what the user is currently doing, or understanding long-term activity \n",
    "patterns to improve customer segmentation. This dataset allows us to evaluate how well machine learning models can classify physical activities in these contexts.\n",
    "\"\"\"\n",
    "\n",
    "business_objectives_comment = \"\"\"\n",
    "The main goal of this project is to build a model that can reliably identify a person's activity \n",
    "based on smartphone sensor data. This kind of model can support different applications, such as \n",
    "health and wellness apps, fall-detection systems, or activity tracking tools.\n",
    "\n",
    "Beyond health-related uses, such a model can also support commercial goals. Companies might use \n",
    "activity recognition to send notifications or recommendations at the right moment, or to better \n",
    "understand user behavior for marketing and personalization. For example, an app might promote a \n",
    "fitness offer when the user is active, or delay non-urgent notifications until the user is not moving.\n",
    "\n",
    "Overall, the objective is to evaluate how well a data-driven model can classify activities and how \n",
    "such a system might offer practical value in both personal and commercial applications.\n",
    "\"\"\"\n",
    "\n",
    "business_success_criteria_comment = \"\"\"\n",
    "For this project to be considered successful from a business perspective, the activity recognition \n",
    "model should perform well enough to be useful in real applications. As a general guideline, an \n",
    "accuracy around 85% or higher would be a good sign that the model is reliable.\n",
    "\n",
    "It is also important that the model performs reasonably across all activities and not only on the \n",
    "most common ones. In addition, the system should be fast enough for real-time use on a smartphone.\n",
    "\n",
    "Success also depends on whether the predictions are useful for the intended application — whether \n",
    "that iss improving user engagement, supporting health monitoring, or enabling more personalized \n",
    "recommendations. If the results support these types of decisions in a meaningful way, we can \n",
    "consider the project successful.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_goals_comment = \"\"\"\n",
    "The main data mining goal is to build and evaluate a machine learning model that predicts one of \n",
    "six activity labels using the smartphone sensor features. This involves testing different algorithms \n",
    "and comparing their strengths and weaknesses.\n",
    "\n",
    "Another goal is to understand which features contribute most to distinguishing the activities and \n",
    "to check how stable the model is across different subjects. Since the dataset has many engineered \n",
    "features, part of the goal is also to explore the data structure and understand how the signals \n",
    "translate into recognizable activity patterns.\n",
    "\n",
    "In addition to building a classifier, the goal is to see whether the resulting model is good enough \n",
    "to support real-world scenarios, such as personalized services, smarter notification timing, or \n",
    "insights for customer profiling.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_success_criteria_comment = \"\"\"\n",
    "We consider the data mining process successful if the model achieves solid performance across \n",
    "the usual evaluation metrics, such as accuracy, precision, recall, and F1-score. Ideally, the \n",
    "model should reach at least around 85% accuracy on the test data and show balanced performance \n",
    "across all classes.\n",
    "\n",
    "The process should also be reproducible. This means documenting how the data was prepared, \n",
    "how the train/validation/test splits were done, and which parameters were used for training.\n",
    "\n",
    "Finally, success also depends on whether the model is stable and behaves consistently during \n",
    "cross-validation. The results should be understandable enough to support the practical or business-related \n",
    "applications described earlier\n",
    "\"\"\"\n",
    "\n",
    "ai_risk_aspects_comment = \"\"\"\n",
    "There are a few risks to consider when working with an activity recognition system. The dataset \n",
    "comes from only 30 young adults, so the model might not generalize well to older people, children, \n",
    "or individuals with movement limitations. This could introduce bias.\n",
    "\n",
    "Although the dataset does not include identifiable personal information, long-term activity patterns \n",
    "could still reveal sensitive details about a person’s routine or health. In commercial settings, this \n",
    "raises questions about consent, privacy, and how such information might be used.\n",
    "\n",
    "There is also the risk of misclassifying activities. In health-related applications, this could lead to \n",
    "missed warnings or incorrect assumptions about a user’s condition. Even in marketing contexts, \n",
    "poor predictions could result in irrelevant or poorly timed notifications.\n",
    "\n",
    "Finally, the study used a specific smartphone model placed at a fixed location on the waist. Real-world \n",
    "usage is more varied, and differences in device placement or sensor quality could affect performance.\n",
    "\"\"\"\n",
    "\n",
    "# Deterministic UUIDs for associations\n",
    "bu_code_writer_uuid = deterministic_uuid(\"business_understanding:writer:student_a\")\n",
    "bu_code_writer_uuid_b = deterministic_uuid(\"business_understanding:writer:student_b\")\n",
    "bu_code_executor_uuid = deterministic_uuid(\"business_understanding:executor\")\n",
    "\n",
    "# Set identities for writer(s) and executor\n",
    "business_understanding_code_writer = student_a  # Student A (Sajid)\n",
    "business_understanding_code_writer_b = student_b  # Student B (Eman)\n",
    "bu_code_executor = executed_by  # whoever is executing now\n",
    "\n",
    "# Ensure the parent phase activity exists (minimal definition)\n",
    "business_understanding_phase_activity = [\n",
    "    f':business_understanding_phase rdf:type prov:Activity .',\n",
    "    f':business_understanding_phase rdfs:label \"Business Understanding Phase\" .',\n",
    "]\n",
    "engine.insert(business_understanding_phase_activity, prefixes=prefixes)\n",
    "\n",
    "# Record the Business Understanding activity with both writers and the executor\n",
    "business_understanding_activity = [\n",
    "    f':business_understanding rdf:type prov:Activity .',\n",
    "    f':business_understanding rdfs:label \"1. Business Understanding\" .',\n",
    "    f':business_understanding sc:isPartOf :business_understanding_phase .',\n",
    "    # Code Writer A\n",
    "    f':business_understanding prov:qualifiedAssociation :{bu_code_writer_uuid} .',\n",
    "    f':{bu_code_writer_uuid} prov:agent :{business_understanding_code_writer} .',\n",
    "    f':{bu_code_writer_uuid} rdf:type prov:Association .',\n",
    "    f':{bu_code_writer_uuid} prov:hadRole :{code_writer_role} .',\n",
    "    # Code Writer B\n",
    "    f':business_understanding prov:qualifiedAssociation :{bu_code_writer_uuid_b} .',\n",
    "    f':{bu_code_writer_uuid_b} prov:agent :{business_understanding_code_writer_b} .',\n",
    "    f':{bu_code_writer_uuid_b} rdf:type prov:Association .',\n",
    "    f':{bu_code_writer_uuid_b} prov:hadRole :{code_writer_role} .',\n",
    "    # Code Executor\n",
    "    f':business_understanding prov:qualifiedAssociation :{bu_code_executor_uuid} .',\n",
    "    f':{bu_code_executor_uuid} prov:agent :{bu_code_executor} .',\n",
    "    f':{bu_code_executor_uuid} rdf:type prov:Association .',\n",
    "    f':{bu_code_executor_uuid} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(business_understanding_activity, prefixes=prefixes)\n",
    "\n",
    "# Entities for 1a–1f\n",
    "business_understanding_data_executor = [\n",
    "    # 1a\n",
    "    f':bu_data_source_and_scenario rdf:type prov:Entity .',\n",
    "    f':bu_data_source_and_scenario prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_data_source_and_scenario rdfs:label \"1a Data Source and Scenario\" .',\n",
    "    f':bu_data_source_and_scenario rdfs:comment \"\"\"{data_src_and_scenario_comment}\"\"\" .',\n",
    "    # 1b\n",
    "    f':bu_business_objectives rdf:type prov:Entity .',\n",
    "    f':bu_business_objectives prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_business_objectives rdfs:label \"1b Business Objectives\" .',\n",
    "    f':bu_business_objectives rdfs:comment \"\"\"{business_objectives_comment}\"\"\" .',\n",
    "    # 1c\n",
    "    f':bu_business_success_criteria rdf:type prov:Entity .',\n",
    "    f':bu_business_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_business_success_criteria rdfs:label \"1c Business Success Criteria\" .',\n",
    "    f':bu_business_success_criteria rdfs:comment \"\"\"{business_success_criteria_comment}\"\"\" .',\n",
    "    # 1d\n",
    "    f':bu_data_mining_goals rdf:type prov:Entity .',\n",
    "    f':bu_data_mining_goals prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_data_mining_goals rdfs:label \"1d Data Mining Goals\" .',\n",
    "    f':bu_data_mining_goals rdfs:comment \"\"\"{data_mining_goals_comment}\"\"\" .',\n",
    "    # 1e\n",
    "    f':bu_data_mining_success_criteria rdf:type prov:Entity .',\n",
    "    f':bu_data_mining_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_data_mining_success_criteria rdfs:label \"1e Data Mining Success Criteria\" .',\n",
    "    f':bu_data_mining_success_criteria rdfs:comment \"\"\"{data_mining_success_criteria_comment}\"\"\" .',\n",
    "    # 1f\n",
    "    f':bu_ai_risk_aspects rdf:type prov:Entity .',\n",
    "    f':bu_ai_risk_aspects prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_ai_risk_aspects rdfs:label \"1f AI risk aspects\" .',\n",
    "    f':bu_ai_risk_aspects rdfs:comment \"\"\"{ai_risk_aspects_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(business_understanding_data_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae9b28",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce717fb",
   "metadata": {},
   "source": [
    "The following pseudo-code & pseudo-documentation may be used as a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':data_understanding_phase rdf:type prov:Activity .',\n",
    "f':data_understanding_phase rdfs:label \"Data Understanding Phase\" .', \n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_har_data_code_writer = student_a\n",
    "\n",
    "def load_har_data() -> typing.Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    return train_df, test_df\n",
    "\n",
    "start_time_ld = now()\n",
    "train_df, test_df = load_har_data()\n",
    "end_time_ld = now()\n",
    "\n",
    "display(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0580e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provenance: Data loading activity\n",
    "load_har_data_code_writer = student_a\n",
    "\n",
    "ld_report = \"\"\"\n",
    "Loaded the Human Activity Recognition (HAR) training and test sets.\n",
    "The dataset contains sensor readings from 30 subjects performing 6 activities.\n",
    "\"\"\"\n",
    "\n",
    "# Generate deterministic UUIDs for associations (following the reference template)\n",
    "du_load_ass_uuid_executor = deterministic_uuid(\"load_har_data:executor:activity\")\n",
    "du_load_ass_uuid_writer = deterministic_uuid(\"load_har_data:writer:activity\")\n",
    "\n",
    "# Build activity triples following the reference template structure\n",
    "load_activity_triples = [\n",
    "    ':load_har_data rdf:type prov:Activity .',\n",
    "    ':load_har_data sc:isPartOf :data_understanding_phase .',\n",
    "    ':load_har_data rdfs:comment \"Load HAR Dataset\" .',\n",
    "    f':load_har_data rdfs:comment \"\"\"{ld_report}\"\"\" .',\n",
    "    f':load_har_data prov:startedAtTime \"{start_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_har_data prov:endedAtTime \"{end_time_ld}\"^^xsd:dateTime .',\n",
    "    # Writer association\n",
    "    f':load_har_data prov:qualifiedAssociation :{du_load_ass_uuid_writer} .',\n",
    "    f':{du_load_ass_uuid_writer} prov:agent :{load_har_data_code_writer} .',\n",
    "    f':{du_load_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{du_load_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    # Executor association\n",
    "    f':load_har_data prov:qualifiedAssociation :{du_load_ass_uuid_executor} .',\n",
    "    f':{du_load_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{du_load_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{du_load_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    # Input entities\n",
    "    ':load_har_data prov:used :har_train_file .',\n",
    "    ':load_har_data prov:used :har_test_file .',\n",
    "    ':har_train_file rdf:type prov:Entity .',\n",
    "    ':har_test_file rdf:type prov:Entity .',\n",
    "    # Output entities\n",
    "    ':train_df rdf:type prov:Entity .',\n",
    "    ':train_df prov:wasGeneratedBy :load_har_data .',\n",
    "    ':test_df rdf:type prov:Entity .',\n",
    "    ':test_df prov:wasGeneratedBy :load_har_data .',\n",
    "]\n",
    "\n",
    "engine.insert(load_activity_triples, prefixes=prefixes)\n",
    "\n",
    "# Document the HAR training dataset structure using Croissant schema\n",
    "har_dataset_triples = [\n",
    "    # Dataset description\n",
    "    ':train_df rdf:type sc:Dataset .',\n",
    "    ':train_df sc:name \"HAR Training Dataset\" .',\n",
    "    ':train_df sc:description \"Training set of the Human Activity Recognition dataset using smartphone accelerometer and gyroscope measurements.\" .',\n",
    "\n",
    "    # Explanation of field coverage\n",
    "    ':train_df rdfs:comment \"\"\"The HAR dataset contains 561 engineered features derived from smartphone accelerometer and gyroscope signals. To keep the provenance graph concise, detailed field semantics are documented in Section 2a where a representative subset is modeled.\"\"\" .',\n",
    "\n",
    "    # RecordSet definition\n",
    "    ':har_recordset rdf:type cr:RecordSet .',\n",
    "    ':har_recordset sc:name \"HAR Feature RecordSet\" .',\n",
    "    ':train_df cr:recordSet :har_recordset .',\n",
    "]\n",
    "\n",
    "engine.insert(har_dataset_triples, prefixes=prefixes)\n",
    "\n",
    "# Document the HAR test dataset\n",
    "har_test_dataset_triples = [\n",
    "    ':test_df rdf:type sc:Dataset .',\n",
    "    ':test_df sc:name \"HAR Test Dataset\" .',\n",
    "    ':test_df sc:description \"Test portion of the Human Activity Recognition dataset, used only for final evaluation.\" .',\n",
    "    ':test_df rdfs:comment \"\"\"The HAR test dataset contains the same 561 engineered features as the training dataset. Attribute semantics are documented once on the training split in Section 2a to avoid duplication.\"\"\" .',\n",
    "]\n",
    "\n",
    "engine.insert(har_test_dataset_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a0148",
   "metadata": {},
   "source": [
    "### 2a) Attribute Types, Units of Measurement, and Semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b4793-5fad-4c9a-89dd-abd662f916b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a) Attribute Types, Units of Measurement, and Semantics\n",
    "# Document all attributes with their types, units, and semantic meaning\n",
    "\n",
    "data_understanding_code_writer = student_a\n",
    "du_2a_code_writer = student_a\n",
    "\n",
    "# Generate deterministic UUIDs for 2a activity\n",
    "du_2a_ass_uuid_executor = deterministic_uuid(\"du_2a_attribute_types:executor:activity\")\n",
    "du_2a_ass_uuid_writer = deterministic_uuid(\"du_2a_attribute_types:writer:activity\")\n",
    "\n",
    "# Start time for 2a activity\n",
    "start_time_2a = now()\n",
    "\n",
    "# Section introduction\n",
    "section_2a_intro = \"\"\"The HAR dataset comprises 563 columns, including 561 engineered sensor features, subject identifiers, and activity labels. Given the substantial number of features, we document 31 representative attributes to demonstrate the variety of accelerometer and gyroscope measurements, their data types, units, and semantic meanings while maintaining knowledge graph conciseness.\"\"\"\n",
    "\n",
    "# Define attributes for the HAR training dataset using Croissant schema\n",
    "# HAR dataset has accelerometer and gyroscope features from smartphones\n",
    "har_2a_fields_triples = [\n",
    "    # Subject label field\n",
    "    ':subject_field rdf:type cr:Field .',\n",
    "    ':subject_field sc:name \"Subject\" .',\n",
    "    ':subject_field sc:description \"Identifier of the subject (1-30) who performed the activity. Smartphone worn on waist.\" .',\n",
    "    ':subject_field cr:dataType xsd:integer .',\n",
    "    ':subject_field qudt:unit qudt:Dimensionless .',\n",
    "    ':har_recordset cr:field :subject_field .',\n",
    "\n",
    "    # Activity label field\n",
    "    ':activity_field rdf:type cr:Field .',\n",
    "    ':activity_field sc:name \"Activity\" .',\n",
    "    ':activity_field sc:description \"Type of activity performed: WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING\" .',\n",
    "    ':activity_field cr:dataType xsd:string .',\n",
    "    ':har_recordset cr:field :activity_field .',\n",
    "\n",
    "    # Time-Domain Features (Accelerometer)\n",
    "    # Sample mean of acceleration signal (X, Y, Z axes)\n",
    "    ':tBodyAcc_mean_X_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyAcc_mean_X_field_2a sc:name \"tBodyAcc-mean()-X\" .',\n",
    "    ':tBodyAcc_mean_X_field_2a sc:description \"Mean of the body acceleration signal (time domain) along X-axis (normalized: -1 to 1).\" .',\n",
    "    ':tBodyAcc_mean_X_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyAcc_mean_X_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :tBodyAcc_mean_X_field_2a .',\n",
    "\n",
    "    ':tBodyAcc_mean_Y_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyAcc_mean_Y_field_2a sc:name \"tBodyAcc-mean()-Y\" .',\n",
    "    ':tBodyAcc_mean_Y_field_2a sc:description \"Mean of the body acceleration signal (time domain) along Y-axis (normalized: -1 to 1).\" .',\n",
    "    ':tBodyAcc_mean_Y_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyAcc_mean_Y_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :tBodyAcc_mean_Y_field_2a .',\n",
    "\n",
    "    ':tBodyAcc_mean_Z_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyAcc_mean_Z_field_2a sc:name \"tBodyAcc-mean()-Z\" .',\n",
    "    ':tBodyAcc_mean_Z_field_2a sc:description \"Mean of the body acceleration signal (time domain) along Z-axis (normalized: -1 to 1).\" .',\n",
    "    ':tBodyAcc_mean_Z_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyAcc_mean_Z_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :tBodyAcc_mean_Z_field_2a .',\n",
    "\n",
    "    # Standard deviation of acceleration\n",
    "    ':tBodyAcc_std_X_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyAcc_std_X_field_2a sc:name \"tBodyAcc-std()-X\" .',\n",
    "    ':tBodyAcc_std_X_field_2a sc:description \"Standard deviation of body acceleration signal (time domain) along X-axis (normalized: -1 to 1).\" .',\n",
    "    ':tBodyAcc_std_X_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyAcc_std_X_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :tBodyAcc_std_X_field_2a .',\n",
    "\n",
    "    ':tBodyAcc_std_Y_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyAcc_std_Y_field_2a sc:name \"tBodyAcc-std()-Y\" .',\n",
    "    ':tBodyAcc_std_Y_field_2a sc:description \"Standard deviation of body acceleration signal (time domain) along Y-axis (normalized: -1 to 1).\" .',\n",
    "    ':tBodyAcc_std_Y_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyAcc_std_Y_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :tBodyAcc_std_Y_field_2a .',\n",
    "\n",
    "    ':tBodyAcc_std_Z_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyAcc_std_Z_field_2a sc:name \"tBodyAcc-std()-Z\" .',\n",
    "    ':tBodyAcc_std_Z_field_2a sc:description \"Standard deviation of body acceleration signal (time domain) along Z-axis (normalized: -1 to 1).\" .',\n",
    "    ':tBodyAcc_std_Z_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyAcc_std_Z_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :tBodyAcc_std_Z_field_2a .',\n",
    "\n",
    "    # Gyroscope Features (Angular velocity)\n",
    "    ':tBodyGyro_mean_X_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyGyro_mean_X_field_2a sc:name \"tBodyGyro-mean()-X\" .',\n",
    "    ':tBodyGyro_mean_X_field_2a sc:description \"Mean angular velocity of body (time domain) around X-axis (normalized: -1 to 1). Units: radians/second.\" .',\n",
    "    ':tBodyGyro_mean_X_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyGyro_mean_X_field_2a qudt:unit siu:radian_per_second .',\n",
    "    ':har_recordset cr:field :tBodyGyro_mean_X_field_2a .',\n",
    "\n",
    "    ':tBodyGyro_mean_Y_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyGyro_mean_Y_field_2a sc:name \"tBodyGyro-mean()-Y\" .',\n",
    "    ':tBodyGyro_mean_Y_field_2a sc:description \"Mean angular velocity of body (time domain) around Y-axis (normalized: -1 to 1). Units: radians/second.\" .',\n",
    "    ':tBodyGyro_mean_Y_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyGyro_mean_Y_field_2a qudt:unit siu:radian_per_second .',\n",
    "    ':har_recordset cr:field :tBodyGyro_mean_Y_field_2a .',\n",
    "\n",
    "    ':tBodyGyro_mean_Z_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyGyro_mean_Z_field_2a sc:name \"tBodyGyro-mean()-Z\" .',\n",
    "    ':tBodyGyro_mean_Z_field_2a sc:description \"Mean angular velocity of body (time domain) around Z-axis (normalized: -1 to 1). Units: radians/second.\" .',\n",
    "    ':tBodyGyro_mean_Z_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyGyro_mean_Z_field_2a qudt:unit siu:radian_per_second .',\n",
    "    ':har_recordset cr:field :tBodyGyro_mean_Z_field_2a .',\n",
    "\n",
    "    # Frequency-Domain Features (FFT of signals)\n",
    "    ':fBodyAcc_mean_X_field_2a rdf:type cr:Field .',\n",
    "    ':fBodyAcc_mean_X_field_2a sc:name \"fBodyAcc-mean()-X\" .',\n",
    "    ':fBodyAcc_mean_X_field_2a sc:description \"Mean frequency-domain body acceleration along X-axis (normalized: -1 to 1). Computed via FFT.\" .',\n",
    "    ':fBodyAcc_mean_X_field_2a cr:dataType xsd:double .',\n",
    "    ':fBodyAcc_mean_X_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :fBodyAcc_mean_X_field_2a .',\n",
    "\n",
    "    ':fBodyAcc_mean_Y_field_2a rdf:type cr:Field .',\n",
    "    ':fBodyAcc_mean_Y_field_2a sc:name \"fBodyAcc-mean()-Y\" .',\n",
    "    ':fBodyAcc_mean_Y_field_2a sc:description \"Mean frequency-domain body acceleration along Y-axis (normalized: -1 to 1). Computed via FFT.\" .',\n",
    "    ':fBodyAcc_mean_Y_field_2a cr:dataType xsd:double .',\n",
    "    ':fBodyAcc_mean_Y_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :fBodyAcc_mean_Y_field_2a .',\n",
    "\n",
    "    ':fBodyAcc_mean_Z_field_2a rdf:type cr:Field .',\n",
    "    ':fBodyAcc_mean_Z_field_2a sc:name \"fBodyAcc-mean()-Z\" .',\n",
    "    ':fBodyAcc_mean_Z_field_2a sc:description \"Mean frequency-domain body acceleration along Z-axis (normalized: -1 to 1). Computed via FFT.\" .',\n",
    "    ':fBodyAcc_mean_Z_field_2a cr:dataType xsd:double .',\n",
    "    ':fBodyAcc_mean_Z_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :fBodyAcc_mean_Z_field_2a .',\n",
    "\n",
    "    # Jerk Signals (rate of change of acceleration)\n",
    "    ':tBodyAccJerk_mean_X_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyAccJerk_mean_X_field_2a sc:name \"tBodyAccJerk-mean()-X\" .',\n",
    "    ':tBodyAccJerk_mean_X_field_2a sc:description \"Mean jerk signal (time domain, first derivative of acceleration) along X-axis (normalized: -1 to 1).\" .',\n",
    "    ':tBodyAccJerk_mean_X_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyAccJerk_mean_X_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :tBodyAccJerk_mean_X_field_2a .',\n",
    "\n",
    "    ':tBodyAccJerk_mean_Y_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyAccJerk_mean_Y_field_2a sc:name \"tBodyAccJerk-mean()-Y\" .',\n",
    "    ':tBodyAccJerk_mean_Y_field_2a sc:description \"Mean jerk signal (time domain, first derivative of acceleration) along Y-axis (normalized: -1 to 1).\" .',\n",
    "    ':tBodyAccJerk_mean_Y_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyAccJerk_mean_Y_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :tBodyAccJerk_mean_Y_field_2a .',\n",
    "\n",
    "    ':tBodyAccJerk_mean_Z_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyAccJerk_mean_Z_field_2a sc:name \"tBodyAccJerk-mean()-Z\" .',\n",
    "    ':tBodyAccJerk_mean_Z_field_2a sc:description \"Mean jerk signal (time domain, first derivative of acceleration) along Z-axis (normalized: -1 to 1).\" .',\n",
    "    ':tBodyAccJerk_mean_Z_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyAccJerk_mean_Z_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :tBodyAccJerk_mean_Z_field_2a .',\n",
    "\n",
    "    # Magnitude of signals\n",
    "    ':tBodyAccMag_mean_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyAccMag_mean_field_2a sc:name \"tBodyAccMag-mean()\" .',\n",
    "    ':tBodyAccMag_mean_field_2a sc:description \"Mean magnitude of body acceleration signal (time domain, computed using Euclidean norm) (normalized: -1 to 1).\" .',\n",
    "    ':tBodyAccMag_mean_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyAccMag_mean_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :tBodyAccMag_mean_field_2a .',\n",
    "\n",
    "    ':tBodyAccMag_std_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyAccMag_std_field_2a sc:name \"tBodyAccMag-std()\" .',\n",
    "    ':tBodyAccMag_std_field_2a sc:description \"Standard deviation of body acceleration magnitude (time domain, computed using Euclidean norm) (normalized: -1 to 1).\" .',\n",
    "    ':tBodyAccMag_std_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyAccMag_std_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :tBodyAccMag_std_field_2a .',\n",
    "\n",
    "    ':tGravityAccMag_mean_field_2a rdf:type cr:Field .',\n",
    "    ':tGravityAccMag_mean_field_2a sc:name \"tGravityAccMag-mean()\" .',\n",
    "    ':tGravityAccMag_mean_field_2a sc:description \"Mean magnitude of gravity acceleration signal (time domain) (normalized: -1 to 1).\" .',\n",
    "    ':tGravityAccMag_mean_field_2a cr:dataType xsd:double .',\n",
    "    ':tGravityAccMag_mean_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :tGravityAccMag_mean_field_2a .',\n",
    "\n",
    "    # Angular velocity magnitude\n",
    "    ':tBodyGyroMag_mean_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyGyroMag_mean_field_2a sc:name \"tBodyGyroMag-mean()\" .',\n",
    "    ':tBodyGyroMag_mean_field_2a sc:description \"Mean magnitude of angular velocity (time domain, computed using Euclidean norm) (normalized: -1 to 1).\" .',\n",
    "    ':tBodyGyroMag_mean_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyGyroMag_mean_field_2a qudt:unit siu:radian_per_second .',\n",
    "    ':har_recordset cr:field :tBodyGyroMag_mean_field_2a .',\n",
    "\n",
    "    ':tBodyGyroMag_std_field_2a rdf:type cr:Field .',\n",
    "    ':tBodyGyroMag_std_field_2a sc:name \"tBodyGyroMag-std()\" .',\n",
    "    ':tBodyGyroMag_std_field_2a sc:description \"Standard deviation of angular velocity magnitude (time domain) (normalized: -1 to 1).\" .',\n",
    "    ':tBodyGyroMag_std_field_2a cr:dataType xsd:double .',\n",
    "    ':tBodyGyroMag_std_field_2a qudt:unit siu:radian_per_second .',\n",
    "    ':har_recordset cr:field :tBodyGyroMag_std_field_2a .',\n",
    "\n",
    "    # Frequency-domain magnitude features\n",
    "    ':fBodyAccMag_mean_field_2a rdf:type cr:Field .',\n",
    "    ':fBodyAccMag_mean_field_2a sc:name \"fBodyAccMag-mean()\" .',\n",
    "    ':fBodyAccMag_mean_field_2a sc:description \"Mean magnitude of frequency-domain body acceleration (FFT) (normalized: -1 to 1).\" .',\n",
    "    ':fBodyAccMag_mean_field_2a cr:dataType xsd:double .',\n",
    "    ':fBodyAccMag_mean_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :fBodyAccMag_mean_field_2a .',\n",
    "\n",
    "    ':fBodyAccMag_std_field_2a rdf:type cr:Field .',\n",
    "    ':fBodyAccMag_std_field_2a sc:name \"fBodyAccMag-std()\" .',\n",
    "    ':fBodyAccMag_std_field_2a sc:description \"Standard deviation of frequency-domain body acceleration magnitude (FFT) (normalized: -1 to 1).\" .',\n",
    "    ':fBodyAccMag_std_field_2a cr:dataType xsd:double .',\n",
    "    ':fBodyAccMag_std_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :fBodyAccMag_std_field_2a .',\n",
    "\n",
    "    ':fBodyBodyAccJerkMag_mean_field_2a rdf:type cr:Field .',\n",
    "    ':fBodyBodyAccJerkMag_mean_field_2a sc:name \"fBodyBodyAccJerkMag-mean()\" .',\n",
    "    ':fBodyBodyAccJerkMag_mean_field_2a sc:description \"Mean magnitude of frequency-domain body jerk signal (FFT) (normalized: -1 to 1).\" .',\n",
    "    ':fBodyBodyAccJerkMag_mean_field_2a cr:dataType xsd:double .',\n",
    "    ':fBodyBodyAccJerkMag_mean_field_2a qudt:unit qudt:StandardAccelerationOfGravity .',\n",
    "    ':har_recordset cr:field :fBodyBodyAccJerkMag_mean_field_2a .',\n",
    "\n",
    "    ':fBodyBodyGyroMag_mean_field_2a rdf:type cr:Field .',\n",
    "    ':fBodyBodyGyroMag_mean_field_2a sc:name \"fBodyBodyGyroMag-mean()\" .',\n",
    "    ':fBodyBodyGyroMag_mean_field_2a sc:description \"Mean magnitude of frequency-domain angular velocity (FFT) (normalized: -1 to 1).\" .',\n",
    "    ':fBodyBodyGyroMag_mean_field_2a cr:dataType xsd:double .',\n",
    "    ':fBodyBodyGyroMag_mean_field_2a qudt:unit siu:radian_per_second .',\n",
    "    ':har_recordset cr:field :fBodyBodyGyroMag_mean_field_2a .',\n",
    "\n",
    "    # Angle features (computed angles between vectors)\n",
    "    ':angle_tBodyAccMean_gravity_field_2a rdf:type cr:Field .',\n",
    "    ':angle_tBodyAccMean_gravity_field_2a sc:name \"angle(tBodyAccMean,gravity)\" .',\n",
    "    ':angle_tBodyAccMean_gravity_field_2a sc:description \"Angle between mean body acceleration vector and gravity (normalized: -1 to 1). Units: radians.\" .',\n",
    "    ':angle_tBodyAccMean_gravity_field_2a cr:dataType xsd:double .',\n",
    "    ':angle_tBodyAccMean_gravity_field_2a qudt:unit siu:radian .',\n",
    "    ':har_recordset cr:field :angle_tBodyAccMean_gravity_field_2a .',\n",
    "\n",
    "    ':angle_tBodyAccJerkMean_gravityMean_field_2a rdf:type cr:Field .',\n",
    "    ':angle_tBodyAccJerkMean_gravityMean_field_2a sc:name \"angle(tBodyAccJerkMean,gravityMean)\" .',\n",
    "    ':angle_tBodyAccJerkMean_gravityMean_field_2a sc:description \"Angle between mean body jerk vector and mean gravity vector (normalized: -1 to 1). Units: radians.\" .',\n",
    "    ':angle_tBodyAccJerkMean_gravityMean_field_2a cr:dataType xsd:double .',\n",
    "    ':angle_tBodyAccJerkMean_gravityMean_field_2a qudt:unit siu:radian .',\n",
    "    ':har_recordset cr:field :angle_tBodyAccJerkMean_gravityMean_field_2a .',\n",
    "\n",
    "    ':angle_X_gravityMean_field_2a rdf:type cr:Field .',\n",
    "    ':angle_X_gravityMean_field_2a sc:name \"angle(X,gravityMean)\" .',\n",
    "    ':angle_X_gravityMean_field_2a sc:description \"Angle between X-axis and mean gravity vector (normalized: -1 to 1). Units: radians.\" .',\n",
    "    ':angle_X_gravityMean_field_2a cr:dataType xsd:double .',\n",
    "    ':angle_X_gravityMean_field_2a qudt:unit siu:radian .',\n",
    "    ':har_recordset cr:field :angle_X_gravityMean_field_2a .',\n",
    "\n",
    "    ':angle_Y_gravityMean_field_2a rdf:type cr:Field .',\n",
    "    ':angle_Y_gravityMean_field_2a sc:name \"angle(Y,gravityMean)\" .',\n",
    "    ':angle_Y_gravityMean_field_2a sc:description \"Angle between Y-axis and mean gravity vector (normalized: -1 to 1). Units: radians.\" .',\n",
    "    ':angle_Y_gravityMean_field_2a cr:dataType xsd:double .',\n",
    "    ':angle_Y_gravityMean_field_2a qudt:unit siu:radian .',\n",
    "    ':har_recordset cr:field :angle_Y_gravityMean_field_2a .',\n",
    "\n",
    "    ':angle_Z_gravityMean_field_2a rdf:type cr:Field .',\n",
    "    ':angle_Z_gravityMean_field_2a sc:name \"angle(Z,gravityMean)\" .',\n",
    "    ':angle_Z_gravityMean_field_2a sc:description \"Angle between Z-axis and mean gravity vector (normalized: -1 to 1). Units: radians.\" .',\n",
    "    ':angle_Z_gravityMean_field_2a cr:dataType xsd:double .',\n",
    "    ':angle_Z_gravityMean_field_2a qudt:unit siu:radian .',\n",
    "    ':har_recordset cr:field :angle_Z_gravityMean_field_2a .',\n",
    "]\n",
    "\n",
    "engine.insert(har_2a_fields_triples, prefixes=prefixes)\n",
    "\n",
    "# End time for 2a activity\n",
    "end_time_2a = now()\n",
    "\n",
    "# Update activity with end time\n",
    "du_2a_end_triples = [\n",
    "    f':du_2a_attribute_types prov:endedAtTime \"{end_time_2a}\"^^xsd:dateTime .',\n",
    "]\n",
    "engine.insert(du_2a_end_triples, prefixes=prefixes)\n",
    "\n",
    "# Summary\n",
    "print(\"2a) Attribute Types, Units of Measurement, and Semantics:\")\n",
    "print(f\"  - Documented {len([t for t in har_2a_fields_triples if 'rdf:type cr:Field' in t])} attributes with types, units, and descriptions\")\n",
    "print(f\"  - Attribute data types: xsd:integer (Subject), xsd:string (Activity), xsd:double (signals)\")\n",
    "print(f\"  - Units: StandardAccelerationOfGravity (acceleration), radian/second (angular velocity), radian (angles)\")\n",
    "print(f\"  - All signal values normalized to [-1, 1] range\")\n",
    "print(f\"  - Provenance recorded for 2a activity\")\n",
    "\n",
    "# Attribute categories summary with actual counts\n",
    "attribute_summary_text = f\"\"\"The HAR dataset contains 563 total columns comprising 561 sensor-derived features, 1 subject identifier, and 1 activity label. The 561 engineered features are organized into several categories:\n",
    "\n",
    "Time-Domain Features: Mean, standard deviation, and statistical measures computed from raw accelerometer and gyroscope signals (e.g., tBodyAcc-mean()-X, tBodyGyro-std()-Y). These capture temporal characteristics of motion patterns.\n",
    "\n",
    "Frequency-Domain Features: FFT-transformed signals providing frequency components of motion (e.g., fBodyAcc-mean()-Z, fBodyGyro-energy()). These reveal periodic patterns in activities.\n",
    "\n",
    "Jerk Signals: First derivatives of acceleration and angular velocity representing rate of change of motion (e.g., tBodyAccJerk-mean()-X, tBodyGyroJerk-std()-Z).\n",
    "\n",
    "Magnitude Features: Euclidean norms of 3-axial signals providing overall intensity measures (e.g., tBodyAccMag-mean(), tGravityAccMag-std()).\n",
    "\n",
    "Angle Features: Angular relationships between motion vectors and gravity (e.g., angle(tBodyAccMean,gravity), angle(X,gravityMean)).\n",
    "\n",
    "All features are normalized and bounded within the [-1, 1] range, with measurements expressed in standard accelerometric units (g) for acceleration and radians per second for gyroscopic data. Angular features use radian measurements.\"\"\"\n",
    "\n",
    "# Section conclusion\n",
    "section_2a_conclusion = \"\"\"Comprehensive attribute documentation reveals sophisticated feature engineering applied to raw accelerometer and gyroscope data. The combination of time and frequency domain characteristics, jerk signals, magnitude computations, and angular relationships creates a multi-faceted representation of human motion patterns well-suited for activity classification. The normalized feature space and consistent use of SI-derived units facilitate downstream machine learning applications.\"\"\"\n",
    "\n",
    "# Document 2a activity with intro, attribute summary, and conclusion\n",
    "du_2a_activity_triples = [\n",
    "    ':du_2a_attribute_types rdf:type prov:Activity .',\n",
    "    ':du_2a_attribute_types sc:isPartOf :data_understanding_phase .',\n",
    "    ':du_2a_attribute_types rdfs:label \"Attribute Types and Semantics\" .',\n",
    "    f':du_2a_attribute_types rdfs:comment \"\"\"{section_2a_intro}\"\"\" .',\n",
    "    f':du_2a_attribute_types prov:startedAtTime \"{start_time_2a}\"^^xsd:dateTime .',\n",
    "    f':du_2a_attribute_types prov:endedAtTime \"{end_time_2a}\"^^xsd:dateTime .',\n",
    "    f':du_2a_attribute_types prov:qualifiedAssociation :{du_2a_ass_uuid_writer} .',\n",
    "    f':{du_2a_ass_uuid_writer} prov:agent :{du_2a_code_writer} .',\n",
    "    f':{du_2a_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{du_2a_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':du_2a_attribute_types prov:qualifiedAssociation :{du_2a_ass_uuid_executor} .',\n",
    "    f':{du_2a_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{du_2a_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{du_2a_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':du_2a_attribute_types prov:used :raw_data .',\n",
    "    \n",
    "    # Attribute categories summary\n",
    "    ':du_2a_attribute_summary rdf:type prov:Entity .',\n",
    "    ':du_2a_attribute_summary sc:isPartOf :du_2a_attribute_types .',\n",
    "    f':du_2a_attribute_summary rdfs:label \"Attribute Categories and Types\" .',\n",
    "    f':du_2a_attribute_summary rdfs:comment \"\"\"{attribute_summary_text}\"\"\" .',\n",
    "    ':du_2a_attribute_summary prov:wasGeneratedBy :du_2a_attribute_types .',\n",
    "    \n",
    "    # Conclusion\n",
    "    ':du_2a_conclusion rdf:type prov:Entity .',\n",
    "    ':du_2a_conclusion sc:isPartOf :du_2a_attribute_types .',\n",
    "    f':du_2a_conclusion rdfs:label \"Attribute Documentation Conclusion\" .',\n",
    "    f':du_2a_conclusion rdfs:comment \"\"\"{section_2a_conclusion}\"\"\" .',\n",
    "    ':du_2a_conclusion prov:wasGeneratedBy :du_2a_attribute_types .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(du_2a_activity_triples, prefixes=prefixes)\n",
    "    print(\"Section 2a activity with intro, summary, and conclusion recorded\")\n",
    "except:\n",
    "    print(\"Note: Could not insert 2a activity to graph DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c2dd6",
   "metadata": {},
   "source": [
    "### 2b) Statistical Properties and Correlations\n",
    "Interpretation and decision are logged after computing outputs (class counts, correlations, descriptive stats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c876c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b) Statistical Properties and Correlations\n",
    "data_understanding_2b_code_writer = student_a\n",
    "\n",
    "start_time_2b = now()\n",
    "\n",
    "# Class distribution\n",
    "activity_counts = train_df['Activity'].value_counts()\n",
    "print(\"Activity counts:\")\n",
    "print(activity_counts)\n",
    "print(f\"Balance ratio: {activity_counts.max() / activity_counts.min():.2f}\\n\")\n",
    "\n",
    "# Basic statistics for all numeric features\n",
    "numeric_features = train_df.select_dtypes(include=[np.number]).columns\n",
    "descriptive_stats = train_df[numeric_features].describe()\n",
    "print(\"Descriptive statistics (sample):\")\n",
    "print(descriptive_stats.iloc[:, :8])\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "correlation_matrix = train_df[numeric_features].corr()\n",
    "upper_triangle = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)\n",
    "upper_corr = correlation_matrix.where(upper_triangle)\n",
    "\n",
    "corr_pairs = []\n",
    "for col in upper_corr.columns:\n",
    "    for idx in upper_corr.index:\n",
    "        value = upper_corr.loc[idx, col]\n",
    "        if pd.notna(value) and abs(value) > 0.8:\n",
    "            corr_pairs.append({\n",
    "                'feature_1': idx,\n",
    "                'feature_2': col,\n",
    "                'correlation': value\n",
    "            })\n",
    "\n",
    "top_corr_df = pd.DataFrame(corr_pairs).sort_values('correlation', key=abs, ascending=False)\n",
    "print(f\"\\nHighly correlated pairs (|r| > 0.8): {len(top_corr_df)}\")\n",
    "print(top_corr_df.head(10))\n",
    "\n",
    "# Feature variance\n",
    "feature_variances = train_df[numeric_features].var().sort_values(ascending=False)\n",
    "print(f\"\\nTop 10 features by variance:\")\n",
    "print(feature_variances.head(10))\n",
    "\n",
    "end_time_2b = now()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65b8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b) Visualizations - Split into separate files\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.join('output', 'figures'), exist_ok=True)\n",
    "\n",
    "# 1. Activity Class Distribution\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
    "ax1.bar(range(len(activity_counts)), activity_counts.values, color='steelblue', edgecolor='navy', alpha=0.7)\n",
    "ax1.set_xticks(range(len(activity_counts)))\n",
    "ax1.set_xticklabels(activity_counts.index, rotation=45, ha='right')\n",
    "ax1.set_title('Activity Class Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_xlabel('Activity', fontsize=12)\n",
    "for i, v in enumerate(activity_counts.values):\n",
    "    ax1.text(i, v + 20, str(v), ha='center', fontsize=9)\n",
    "plt.tight_layout()\n",
    "fig1.savefig(os.path.join('output', 'figures', '2b_01_activity_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "print(\"Saved: 2b_01_activity_distribution.png\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Top 10 Features Correlation Heatmap\n",
    "top_features_list = []\n",
    "for _, row in top_corr_df.head(10).iterrows():\n",
    "    if row['feature_1'] not in top_features_list:\n",
    "        top_features_list.append(row['feature_1'])\n",
    "    if row['feature_2'] not in top_features_list:\n",
    "        top_features_list.append(row['feature_2'])\n",
    "    if len(top_features_list) >= 10:\n",
    "        break\n",
    "\n",
    "top_features_corr = train_df[top_features_list[:10]].corr()\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(top_features_corr, annot=True, fmt='.2f', cmap='RdYlBu_r', center=0,\n",
    "            square=True, ax=ax2, cbar_kws={'shrink': 0.8}, vmin=-1, vmax=1, linewidths=0.5)\n",
    "ax2.set_title('Top 10 Features Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right', fontsize=9)\n",
    "ax2.set_yticklabels(ax2.get_yticklabels(), fontsize=9)\n",
    "plt.tight_layout()\n",
    "fig2.savefig(os.path.join('output', 'figures', '2b_02_top_features_correlation.png'), dpi=300, bbox_inches='tight')\n",
    "print(\"Saved: 2b_02_top_features_correlation.png\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Top 20 Features by Variance\n",
    "top_20_vars = feature_variances.head(20)\n",
    "fig3, ax3 = plt.subplots(figsize=(10, 8))\n",
    "ax3.barh(range(len(top_20_vars)), top_20_vars.values, color='coral', edgecolor='darkred', alpha=0.7)\n",
    "ax3.set_yticks(range(len(top_20_vars)))\n",
    "ax3.set_yticklabels(top_20_vars.index, fontsize=9)\n",
    "ax3.set_xlabel('Variance', fontsize=12)\n",
    "ax3.set_title('Top 20 Features by Variance', fontsize=14, fontweight='bold')\n",
    "ax3.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "fig3.savefig(os.path.join('output', 'figures', '2b_03_top_variance_features.png'), dpi=300, bbox_inches='tight')\n",
    "print(\"Saved: 2b_03_top_variance_features.png\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Distribution of Correlation Coefficients\n",
    "all_corr_values = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)]\n",
    "fig4, ax4 = plt.subplots(figsize=(10, 6))\n",
    "ax4.hist(all_corr_values, bins=50, color='mediumpurple', edgecolor='darkviolet', alpha=0.7)\n",
    "ax4.axvline(x=0.8, color='red', linestyle='--', linewidth=2, label='0.8 threshold')\n",
    "ax4.axvline(x=-0.8, color='red', linestyle='--', linewidth=2)\n",
    "ax4.set_xlabel('Correlation Coefficient', fontsize=12)\n",
    "ax4.set_ylabel('Frequency', fontsize=12)\n",
    "ax4.set_title('Distribution of Correlation Coefficients', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "plt.tight_layout()\n",
    "fig4.savefig(os.path.join('output', 'figures', '2b_04_correlation_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "print(\"Saved: 2b_04_correlation_distribution.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated visualizations for statistical exploration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4badd2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section intro\n",
    "section_2b_intro = \"\"\"Understanding class balance, feature correlations, and variance patterns is fundamental to selecting appropriate modeling strategies. We analyze the distribution of activity classes to assess sampling requirements, examine feature correlations to identify redundancy and inform dimensionality reduction, and evaluate variance patterns to discover discriminative features for classification tasks.\"\"\"\n",
    "\n",
    "# Figure 1: Activity Class Distribution with actual data\n",
    "fig_2b_01_label = \"Figure 1: Activity Class Distribution in Training Dataset\"\n",
    "fig_2b_01_text = f\"\"\"Class Balance Analysis: The training dataset contains 6 activity classes with the following distribution:\n",
    "- LAYING: {activity_counts.get('LAYING', 0)} samples (19.1%)\n",
    "- STANDING: {activity_counts.get('STANDING', 0)} samples (18.2%)\n",
    "- SITTING: {activity_counts.get('SITTING', 0)} samples (17.5%)\n",
    "- WALKING: {activity_counts.get('WALKING', 0)} samples (16.7%)\n",
    "- WALKING_UPSTAIRS: {activity_counts.get('WALKING_UPSTAIRS', 0)} samples (16.4%)\n",
    "- WALKING_DOWNSTAIRS: {activity_counts.get('WALKING_DOWNSTAIRS', 0)} samples (13.4%)\n",
    "\n",
    "The distribution shows moderate imbalance with a ratio of {activity_counts.max() / activity_counts.min():.2f}:1 between most and least frequent classes. This level of imbalance is manageable for classification modeling. Static activities (LAYING, STANDING, SITTING) collectively represent 54.8% of samples while dynamic activities (WALKING variants) represent 46.5%.\"\"\"\n",
    "\n",
    "# Figure 2: Correlation Matrix with actual findings\n",
    "fig_2b_02_label = \"Figure 2: Correlation Heatmap of Highly Correlated Features\"\n",
    "fig_2b_02_text = f\"\"\"Feature Redundancy Analysis: The correlation analysis identified {len(top_corr_df)} feature pairs with absolute correlation exceeding 0.8. The heatmap displays the correlation matrix for 10 representative features selected from the most highly correlated pairs.\n",
    "\n",
    "Key findings include near-perfect correlations (r > 0.95) between body and gravity acceleration magnitude features, indicating these metrics capture overlapping information. Many jerk magnitude and standard magnitude measures also show very high correlation. This substantial redundancy suggests that dimensionality reduction techniques could significantly compress the feature space without substantial information loss.\"\"\"\n",
    "# Figure 3: Variance Analysis with actual top features\n",
    "top_5_var_features = ', '.join([f\"{feat} ({var:.4f})\" for feat, var in feature_variances.head(5).items()])\n",
    "fig_2b_03_label = \"Figure 3: Top 20 Features Ranked by Variance\"\n",
    "fig_2b_03_text = f\"\"\"Feature Discriminative Power: This visualization ranks features by their variance, which indicates the spread of values and potential discriminative power for classification.\n",
    "\n",
    "Top 5 highest variance features: {top_5_var_features}\n",
    "\n",
    "Entropy-based features dominate the high-variance rankings, particularly those derived from jerk signals and frequency-domain transformations. Gravity-related correlation terms also exhibit high variance. These features likely capture the most distinctive patterns between different activity types. In contrast, low-variance features may contribute less to classification performance and could be candidates for removal during feature selection.\"\"\"\n",
    "\n",
    "# Figure 4: Correlation Distribution\n",
    "fig_2b_04_label = \"Figure 4: Distribution of All Pairwise Feature Correlations\"\n",
    "fig_2b_04_text = f\"\"\"Overall Correlation Structure: This histogram displays the distribution of all {len(all_corr_values)} pairwise correlation coefficients in the dataset. Red dashed lines mark the ±0.8 thresholds commonly used to identify high correlations.\n",
    "\n",
    "The distribution is bimodal with a large peak near zero correlation and a substantial tail extending toward high positive and negative correlations. Approximately {len(top_corr_df)} feature pairs ({len(top_corr_df)/len(all_corr_values)*100:.1f}% of all pairs) exceed the 0.8 correlation threshold. This extensive redundancy confirms that many features encode similar information, reinforcing the recommendation for dimensionality reduction in data preparation.\"\"\"\n",
    "\n",
    "# Section conclusion\n",
    "section_2b_conclusion = \"\"\"Statistical analysis reveals moderate class imbalance that can be addressed through standard classification techniques, extensive feature redundancy with 23,058 pairs exceeding 0.8 correlation suggesting strong candidates for dimensionality reduction, and variance-based feature hierarchies with entropy and jerk features showing highest discriminative potential. These findings directly inform data preparation strategies including correlation-based feature selection and variance thresholding to optimize model performance.\"\"\"\n",
    "\n",
    "# Generate deterministic UUIDs for associations\n",
    "du_2b_uuid_executor = deterministic_uuid(\"du_2b_statistical_analysis:executor:activity\")\n",
    "du_2b_uuid_writer = deterministic_uuid(\"du_2b_statistical_analysis:writer:activity\")\n",
    "\n",
    "# Build activity triples with intro, figures with detailed text, and conclusion\n",
    "activity_2b_triples = [\n",
    "    ':du_2b_statistical_analysis rdf:type prov:Activity .',\n",
    "    ':du_2b_statistical_analysis sc:isPartOf :data_understanding_phase .',\n",
    "    ':du_2b_statistical_analysis rdfs:label \"Statistical Analysis\" .',\n",
    "    f':du_2b_statistical_analysis rdfs:comment \"\"\"{section_2b_intro}\"\"\" .',\n",
    "    f':du_2b_statistical_analysis prov:startedAtTime \"{start_time_2b}\"^^xsd:dateTime .',\n",
    "    f':du_2b_statistical_analysis prov:endedAtTime \"{end_time_2b}\"^^xsd:dateTime .',\n",
    "    f':du_2b_statistical_analysis prov:qualifiedAssociation :{du_2b_uuid_writer} .',\n",
    "    f':{du_2b_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{du_2b_uuid_writer} prov:agent :{data_understanding_2b_code_writer} .',\n",
    "    f':{du_2b_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':du_2b_statistical_analysis prov:qualifiedAssociation :{du_2b_uuid_executor} .',\n",
    "    f':{du_2b_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{du_2b_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{du_2b_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':du_2b_statistical_analysis prov:used :raw_data .',\n",
    "    \n",
    "    # Figure 1: Activity Class Distribution (with detailed analysis)\n",
    "    ':du_2b_fig_01 rdf:type prov:Entity .',\n",
    "    ':du_2b_fig_01 rdf:type sc:Figure .',\n",
    "    ':du_2b_fig_01 sc:isPartOf :du_2b_statistical_analysis .',\n",
    "    f':du_2b_fig_01 rdfs:label \"{fig_2b_01_label}\" .',\n",
    "    f':du_2b_fig_01 rdfs:comment \"\"\"{fig_2b_01_text}\"\"\" .',\n",
    "    ':du_2b_fig_01 prov:wasGeneratedBy :du_2b_statistical_analysis .',\n",
    "    \n",
    "    # Figure 2: Correlation Matrix (with detailed analysis)\n",
    "    ':du_2b_fig_02 rdf:type prov:Entity .',\n",
    "    ':du_2b_fig_02 rdf:type sc:Figure .',\n",
    "    ':du_2b_fig_02 sc:isPartOf :du_2b_statistical_analysis .',\n",
    "    f':du_2b_fig_02 rdfs:label \"{fig_2b_02_label}\" .',\n",
    "    f':du_2b_fig_02 rdfs:comment \"\"\"{fig_2b_02_text}\"\"\" .',\n",
    "    ':du_2b_fig_02 prov:wasGeneratedBy :du_2b_statistical_analysis .',\n",
    "    \n",
    "    # Figure 3: Variance Features (with detailed analysis)\n",
    "    ':du_2b_fig_03 rdf:type prov:Entity .',\n",
    "    ':du_2b_fig_03 rdf:type sc:Figure .',\n",
    "    ':du_2b_fig_03 sc:isPartOf :du_2b_statistical_analysis .',\n",
    "    f':du_2b_fig_03 rdfs:label \"{fig_2b_03_label}\" .',\n",
    "    f':du_2b_fig_03 rdfs:comment \"\"\"{fig_2b_03_text}\"\"\" .',\n",
    "    ':du_2b_fig_03 prov:wasGeneratedBy :du_2b_statistical_analysis .',\n",
    "    \n",
    "    # Figure 4: Correlation Distribution (with detailed analysis)\n",
    "    ':du_2b_fig_04 rdf:type prov:Entity .',\n",
    "    ':du_2b_fig_04 rdf:type sc:Figure .',\n",
    "    ':du_2b_fig_04 sc:isPartOf :du_2b_statistical_analysis .',\n",
    "    f':du_2b_fig_04 rdfs:label \"{fig_2b_04_label}\" .',\n",
    "    f':du_2b_fig_04 rdfs:comment \"\"\"{fig_2b_04_text}\"\"\" .',\n",
    "    ':du_2b_fig_04 prov:wasGeneratedBy :du_2b_statistical_analysis .',\n",
    "    \n",
    "    # Overall conclusion\n",
    "    ':du_2b_conclusion rdf:type prov:Entity .',\n",
    "    ':du_2b_conclusion sc:isPartOf :du_2b_statistical_analysis .',\n",
    "    f':du_2b_conclusion rdfs:label \"Statistical Analysis Conclusion\" .',\n",
    "    f':du_2b_conclusion rdfs:comment \"\"\"{section_2b_conclusion}\"\"\" .',\n",
    "    ':du_2b_conclusion prov:wasGeneratedBy :du_2b_statistical_analysis .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(activity_2b_triples, prefixes=prefixes)\n",
    "    print(\"Section 2b complete - Statistical analysis with 4 figures and comprehensive interpretations recorded\")\n",
    "except:\n",
    "    print(\"Note: Could not insert to graph DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed2488",
   "metadata": {},
   "source": [
    "### 2c) Data Quality Analysis\n",
    "Check for missing values, duplicates, outliers, data types, and value plausibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06acec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c) Data Quality Analysis - Computation\n",
    "data_understanding_2c_code_writer = student_a\n",
    "start_time_2c = now()\n",
    "\n",
    "# Separate sensor features from identifier\n",
    "numeric_features = train_df.select_dtypes(include=[np.number]).columns\n",
    "sensor_features = numeric_features.drop('subject', errors='ignore')\n",
    "\n",
    "# 1. Missing values check\n",
    "missing_counts = train_df.isnull().sum()\n",
    "total_missing = missing_counts.sum()\n",
    "print(\"\\nMissing Values\\n\")\n",
    "print(f\"Total missing values: {total_missing}\")\n",
    "if total_missing > 0:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "else:\n",
    "    print(\"No missing values found - dataset is complete\")\n",
    "\n",
    "# 2. Duplicate rows check\n",
    "duplicate_count = train_df.duplicated().sum()\n",
    "print(f\"\\n Duplicate Rows\\n\")\n",
    "print(f\"Number of duplicates: {duplicate_count}\")\n",
    "print(f\"Percentage: {duplicate_count / len(train_df) * 100:.2f}%\")\n",
    "\n",
    "# 3. Value range and plausibility check (sensor features only)\n",
    "print(f\"\\n Value Range Plausibility (Sensor Features)\\n\")\n",
    "print(f\"Sensor features count: {len(sensor_features)}\")\n",
    "print(f\"Overall min: {train_df[sensor_features].min().min():.4f}\")\n",
    "print(f\"Overall max: {train_df[sensor_features].max().max():.4f}\")\n",
    "print(\"Expected range for normalized sensor data: approximately [-1, 1]\")\n",
    "\n",
    "# 4. Outlier detection using IQR method\n",
    "print(f\"\\n Outlier Detection (IQR method on sample features) \\n\")\n",
    "sample_features = ['tBodyAcc-mean()-X', 'tBodyAcc-std()-X', 'tGravityAcc-mean()-X']\n",
    "outlier_counts = {}\n",
    "for feature in sample_features:\n",
    "    Q1 = train_df[feature].quantile(0.25)\n",
    "    Q3 = train_df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = ((train_df[feature] < lower_bound) | (train_df[feature] > upper_bound)).sum()\n",
    "    outlier_counts[feature] = outliers\n",
    "    print(f\"{feature}: {outliers} outliers ({outliers/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "# 5. Uneven distributions - check skewness\n",
    "print(f\"\\n Distribution Analysis (Skewness) \\n\")\n",
    "skewness_sample = train_df[sample_features].skew()\n",
    "print(\"Skewness of sample features:\")\n",
    "for feature, skew in skewness_sample.items():\n",
    "    print(f\"{feature}: {skew:.3f}\")\n",
    "print(\"(|skew| > 1 indicates high skewness, |skew| < 0.5 is fairly symmetric)\")\n",
    "\n",
    "# 6. Check for constant or near-constant columns\n",
    "constant_columns = []\n",
    "low_variance_threshold = 0.01\n",
    "low_variance_cols = []\n",
    "for col in sensor_features:\n",
    "    if train_df[col].nunique() == 1:\n",
    "        constant_columns.append(col)\n",
    "    elif train_df[col].var() < low_variance_threshold:\n",
    "        low_variance_cols.append(col)\n",
    "        \n",
    "print(f\"\\n Constant and Low Variance Features \\n\")\n",
    "print(f\"Constant columns: {len(constant_columns)}\")\n",
    "print(f\"Low variance columns (var < {low_variance_threshold}): {len(low_variance_cols)}\")\n",
    "\n",
    "# 7. Data type consistency\n",
    "print(f\"\\n Data Type Distribution \\n\")\n",
    "print(train_df.dtypes.value_counts())\n",
    "\n",
    "end_time_2c = now()\n",
    "print(f\"\\nData quality analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nNote: 'subject' is an identifier (1–30) and is excluded from sensor plausibility/variance checks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c) Record data quality analysis with actual metrics and findings\n",
    "\n",
    "du_2c_uuid_executor = deterministic_uuid(\"du_2c_quality_analysis:executor:activity\")\n",
    "du_2c_uuid_writer = deterministic_uuid(\"du_2c_quality_analysis:writer:activity\")\n",
    "\n",
    "# Section introduction  \n",
    "section_2c_intro = \"\"\"To establish trust in subsequent modeling, we evaluate data quality across seven dimensions: completeness (missing values), uniqueness (duplicate records), validity (range plausibility), outliers, distribution characteristics, feature variance, and type consistency. These checks verify the dataset meets the standards required for reliable machine learning outcomes.\"\"\"\n",
    "\n",
    "# Quality findings with actual metrics from the analysis\n",
    "quality_finding_01_text = f\"\"\"Missing Values: A completeness check was performed on all features and observations. No missing values were detected. The dataset is fully complete and does not require any imputation.\n",
    "\n",
    "Total missing values: {total_missing}\n",
    "No missing values found - dataset is complete\"\"\"\n",
    "\n",
    "quality_finding_02_text = f\"\"\"Duplicate Records: No duplicate records were found. Each observation corresponds to a unique sliding window of sensor measurements, confirming correct data segmentation.\n",
    "\n",
    "Number of duplicates: {duplicate_count}\n",
    "Percentage: {duplicate_count / len(train_df) * 100:.2f}%\"\"\"\n",
    "\n",
    "quality_finding_03_text = f\"\"\"Value Range and Plausibility: All feature values fall within the expected normalized range of approximately −1 to 1. These ranges are consistent with the documented preprocessing of accelerometer and gyroscope signals. No implausible or invalid values were observed.\n",
    "\n",
    "Sensor features count: {len(sensor_features)}\n",
    "Overall min: {train_df[sensor_features].min().min():.4f}\n",
    "Overall max: {train_df[sensor_features].max().max():.4f}\n",
    "Expected range for normalized sensor data: approximately [-1, 1]\"\"\"\n",
    "\n",
    "outlier_details = \"\\n\".join([f\"{feat}: {count} outliers ({count/len(train_df)*100:.1f}%)\" for feat, count in outlier_counts.items()])\n",
    "quality_finding_04_text = f\"\"\"Outlier Detection: Outliers were identified in selected features using the interquartile range (IQR) method. Given the sensor-based nature of the data, these values are considered valid extreme movements rather than data errors. All detected outliers remain within valid normalized bounds.\n",
    "\n",
    "Outlier Detection (IQR method on sample features):\n",
    "{outlier_details}\"\"\"\n",
    "\n",
    "skew_details = \"\\n\".join([f\"{feat}: {skew:.3f}\" for feat, skew in skewness_sample.items()])\n",
    "quality_finding_05_text = f\"\"\"Distribution and Skewness: Most features exhibit approximately symmetric distributions, with skewness values generally within acceptable limits. Moderate skewness in some variables reflects natural asymmetry in human activity patterns and does not indicate data quality issues.\n",
    "\n",
    "Skewness of sample features:\n",
    "{skew_details}\n",
    "(|skew| > 1 indicates high skewness, |skew| < 0.5 is fairly symmetric)\"\"\"\n",
    "\n",
    "quality_finding_06_text = f\"\"\"Constant and Low-Variance Features: No constant features were detected. A limited number of features show low variance below the selected threshold. This behavior is expected for engineered features and does not compromise data quality.\n",
    "\n",
    "Constant columns: {len(constant_columns)}\n",
    "Low variance columns (var < {low_variance_threshold}): {len(low_variance_cols)}\"\"\"\n",
    "\n",
    "quality_finding_07_text = f\"\"\"Data Type Consistency: All feature variables are numerical and consistently typed. Activity labels are categorical and subject identifiers are integer-encoded. No inconsistencies in data types were found.\n",
    "\n",
    "Data Type Distribution:\n",
    "float64: 561\n",
    "int64: 1\n",
    "object: 1\"\"\"\n",
    "\n",
    "# Overall conclusion\n",
    "section_2c_conclusion = \"\"\"Quality assessment across seven dimensions establishes high data integrity for downstream modeling. Complete absence of missing values and duplicates confirms rigorous preprocessing by dataset creators. Normalized value ranges (-0.9977 to 1.0000) align with documented bounds without implausible outliers. Distribution patterns show moderate skewness typical of human motion data. Low-variance features (37 columns below 0.01 threshold) represent candidates for dimensionality reduction. Consistent numerical typing throughout facilitates immediate progression to feature engineering and model training without additional data cleaning requirements.\"\"\"\n",
    "\n",
    "# Main activity with findings (each with actual metrics) and conclusion\n",
    "activity_2c_triples = [\n",
    "    ':du_2c_quality_analysis rdf:type prov:Activity .',\n",
    "    ':du_2c_quality_analysis sc:isPartOf :data_understanding_phase .',\n",
    "    f':du_2c_quality_analysis rdfs:label \"Section 2c - Data Quality Analysis\" .',\n",
    "    f':du_2c_quality_analysis rdfs:comment \"\"\"{section_2c_intro}\"\"\" .',\n",
    "    f':du_2c_quality_analysis prov:startedAtTime \"{start_time_2c}\"^^xsd:dateTime .',\n",
    "    f':du_2c_quality_analysis prov:endedAtTime \"{end_time_2c}\"^^xsd:dateTime .',\n",
    "    f':du_2c_quality_analysis prov:qualifiedAssociation :{du_2c_uuid_writer} .',\n",
    "    f':{du_2c_uuid_writer} prov:agent :{data_understanding_2c_code_writer} .',\n",
    "    f':{du_2c_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{du_2c_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':du_2c_quality_analysis prov:qualifiedAssociation :{du_2c_uuid_executor} .',\n",
    "    f':{du_2c_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{du_2c_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{du_2c_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':du_2c_quality_analysis prov:used :train_df .',\n",
    "    \n",
    "    # Quality Finding 1: Missing Values (with actual metrics)\n",
    "    ':du_2c_finding_01 rdf:type prov:Entity .',\n",
    "    ':du_2c_finding_01 rdf:type sc:QualityReport .',\n",
    "    ':du_2c_finding_01 sc:isPartOf :du_2c_quality_analysis .',\n",
    "    f':du_2c_finding_01 rdfs:label \"Missing Values\" .',\n",
    "    f':du_2c_finding_01 rdfs:comment \"\"\"{quality_finding_01_text}\"\"\" .',\n",
    "    ':du_2c_finding_01 prov:wasGeneratedBy :du_2c_quality_analysis .',\n",
    "    \n",
    "    # Quality Finding 2: Duplicates (with actual metrics)\n",
    "    ':du_2c_finding_02 rdf:type prov:Entity .',\n",
    "    ':du_2c_finding_02 rdf:type sc:QualityReport .',\n",
    "    ':du_2c_finding_02 sc:isPartOf :du_2c_quality_analysis .',\n",
    "    f':du_2c_finding_02 rdfs:label \"Duplicate Records\" .',\n",
    "    f':du_2c_finding_02 rdfs:comment \"\"\"{quality_finding_02_text}\"\"\" .',\n",
    "    ':du_2c_finding_02 prov:wasGeneratedBy :du_2c_quality_analysis .',\n",
    "    \n",
    "    # Quality Finding 3: Value Range (with actual metrics)\n",
    "    ':du_2c_finding_03 rdf:type prov:Entity .',\n",
    "    ':du_2c_finding_03 rdf:type sc:QualityReport .',\n",
    "    ':du_2c_finding_03 sc:isPartOf :du_2c_quality_analysis .',\n",
    "    f':du_2c_finding_03 rdfs:label \"Value Range and Plausibility\" .',\n",
    "    f':du_2c_finding_03 rdfs:comment \"\"\"{quality_finding_03_text}\"\"\" .',\n",
    "    ':du_2c_finding_03 prov:wasGeneratedBy :du_2c_quality_analysis .',\n",
    "    \n",
    "    # Quality Finding 4: Outliers (with actual metrics)\n",
    "    ':du_2c_finding_04 rdf:type prov:Entity .',\n",
    "    ':du_2c_finding_04 rdf:type sc:QualityReport .',\n",
    "    ':du_2c_finding_04 sc:isPartOf :du_2c_quality_analysis .',\n",
    "    f':du_2c_finding_04 rdfs:label \"Outlier Detection\" .',\n",
    "    f':du_2c_finding_04 rdfs:comment \"\"\"{quality_finding_04_text}\"\"\" .',\n",
    "    ':du_2c_finding_04 prov:wasGeneratedBy :du_2c_quality_analysis .',\n",
    "    \n",
    "    # Quality Finding 5: Distribution (with actual metrics)\n",
    "    ':du_2c_finding_05 rdf:type prov:Entity .',\n",
    "    ':du_2c_finding_05 rdf:type sc:QualityReport .',\n",
    "    ':du_2c_finding_05 sc:isPartOf :du_2c_quality_analysis .',\n",
    "    f':du_2c_finding_05 rdfs:label \"Distribution and Skewness\" .',\n",
    "    f':du_2c_finding_05 rdfs:comment \"\"\"{quality_finding_05_text}\"\"\" .',\n",
    "    ':du_2c_finding_05 prov:wasGeneratedBy :du_2c_quality_analysis .',\n",
    "    \n",
    "    # Quality Finding 6: Variance (with actual metrics)\n",
    "    ':du_2c_finding_06 rdf:type prov:Entity .',\n",
    "    ':du_2c_finding_06 rdf:type sc:QualityReport .',\n",
    "    ':du_2c_finding_06 sc:isPartOf :du_2c_quality_analysis .',\n",
    "    f':du_2c_finding_06 rdfs:label \"Constant and Low-Variance Features\" .',\n",
    "    f':du_2c_finding_06 rdfs:comment \"\"\"{quality_finding_06_text}\"\"\" .',\n",
    "    ':du_2c_finding_06 prov:wasGeneratedBy :du_2c_quality_analysis .',\n",
    "    \n",
    "    # Quality Finding 7: Data Types (with actual metrics)\n",
    "    ':du_2c_finding_07 rdf:type prov:Entity .',\n",
    "    ':du_2c_finding_07 rdf:type sc:QualityReport .',\n",
    "    ':du_2c_finding_07 sc:isPartOf :du_2c_quality_analysis .',\n",
    "    f':du_2c_finding_07 rdfs:label \"Data Type Consistency\" .',\n",
    "    f':du_2c_finding_07 rdfs:comment \"\"\"{quality_finding_07_text}\"\"\" .',\n",
    "    ':du_2c_finding_07 prov:wasGeneratedBy :du_2c_quality_analysis .',\n",
    "    \n",
    "    # Overall conclusion\n",
    "    ':du_2c_conclusion rdf:type prov:Entity .',\n",
    "    ':du_2c_conclusion sc:isPartOf :du_2c_quality_analysis .',\n",
    "    f':du_2c_conclusion rdfs:label \"Quality Analysis Conclusion\" .',\n",
    "    f':du_2c_conclusion rdfs:comment \"\"\"{section_2c_conclusion}\"\"\" .',\n",
    "    ':du_2c_conclusion prov:wasGeneratedBy :du_2c_quality_analysis .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(activity_2c_triples, prefixes=prefixes)\n",
    "    print(\"Section 2c complete - Data quality analysis with 7 findings and conclusion recorded\")\n",
    "except:\n",
    "    print(\"Note: Could not insert to graph DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b69d9c",
   "metadata": {},
   "source": [
    "### 2d) Visual Exploration and Hypotheses\n",
    "Create visualizations to explore data properties, patterns, and test hypotheses about activity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d) Visual Exploration - Create 6 separate visualizations\n",
    "import os\n",
    "os.makedirs('output/figures', exist_ok=True)\n",
    "\n",
    "data_understanding_2d_code_writer = student_a\n",
    "start_time_2d = now()\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# 1. Activity class distribution\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
    "activity_counts = train_df['Activity'].value_counts()\n",
    "ax1.bar(range(len(activity_counts)), activity_counts.values, color='steelblue', edgecolor='navy')\n",
    "ax1.set_xticks(range(len(activity_counts)))\n",
    "ax1.set_xticklabels(activity_counts.index, rotation=45, ha='right', fontsize=11)\n",
    "ax1.set_title('Activity Class Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Sample Count', fontsize=12)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/figures/2d_01_activity_distribution.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"Saved: 2d_01_activity_distribution.png\")\n",
    "\n",
    "# 2. Subject participation distribution\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "subject_counts = train_df['subject'].value_counts().sort_index()\n",
    "ax2.bar(subject_counts.index, subject_counts.values, color='coral', edgecolor='darkred')\n",
    "ax2.set_title('Samples per Subject', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Subject ID', fontsize=12)\n",
    "ax2.set_ylabel('Sample Count', fontsize=12)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/figures/2d_02_subject_distribution.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"Saved: 2d_02_subject_distribution.png\")\n",
    "\n",
    "# 3. Static vs Dynamic activities variance\n",
    "fig3, ax3 = plt.subplots(figsize=(8, 6))\n",
    "static_activities = ['SITTING', 'STANDING', 'LAYING']\n",
    "dynamic_activities = ['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS']\n",
    "static_variance = train_df[train_df['Activity'].isin(static_activities)]['tBodyAcc-std()-X'].mean()\n",
    "dynamic_variance = train_df[train_df['Activity'].isin(dynamic_activities)]['tBodyAcc-std()-X'].mean()\n",
    "ax3.bar(['Static Activities', 'Dynamic Activities'], [static_variance, dynamic_variance], \n",
    "        color=['lightblue', 'orange'], edgecolor='black', width=0.6)\n",
    "ax3.set_title('Body Acceleration Variance: Static vs Dynamic', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylabel('Mean Standard Deviation', fontsize=12)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/figures/2d_03_static_vs_dynamic.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"Saved: 2d_03_static_vs_dynamic.png\")\n",
    "\n",
    "# 4. Feature distribution - body acceleration\n",
    "fig4, ax4 = plt.subplots(figsize=(10, 6))\n",
    "sample_features = ['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z']\n",
    "colors = ['red', 'green', 'blue']\n",
    "for feature, color in zip(sample_features, colors):\n",
    "    ax4.hist(train_df[feature], bins=40, alpha=0.5, label=feature, color=color, edgecolor='black')\n",
    "ax4.set_title('Body Acceleration Distribution (X, Y, Z)', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Normalized Value', fontsize=12)\n",
    "ax4.set_ylabel('Frequency', fontsize=12)\n",
    "ax4.legend(fontsize=11)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/figures/2d_04_acceleration_distribution.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"Saved: 2d_04_acceleration_distribution.png\")\n",
    "\n",
    "# 5. Activity comparison - mean acceleration\n",
    "fig5, ax5 = plt.subplots(figsize=(12, 6))\n",
    "activity_means = train_df.groupby('Activity')[['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z']].mean()\n",
    "x_pos = range(len(activity_means))\n",
    "width = 0.25\n",
    "ax5.bar([p - width for p in x_pos], activity_means['tBodyAcc-mean()-X'], width, label='X-axis', color='red', alpha=0.7)\n",
    "ax5.bar(x_pos, activity_means['tBodyAcc-mean()-Y'], width, label='Y-axis', color='green', alpha=0.7)\n",
    "ax5.bar([p + width for p in x_pos], activity_means['tBodyAcc-mean()-Z'], width, label='Z-axis', color='blue', alpha=0.7)\n",
    "ax5.set_xticks(x_pos)\n",
    "ax5.set_xticklabels(activity_means.index, rotation=45, ha='right', fontsize=10)\n",
    "ax5.set_title('Mean Acceleration by Activity', fontsize=14, fontweight='bold')\n",
    "ax5.set_ylabel('Mean Value', fontsize=12)\n",
    "ax5.legend(fontsize=11)\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/figures/2d_05_mean_acceleration_by_activity.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"Saved: 2d_05_mean_acceleration_by_activity.png\")\n",
    "\n",
    "# 6. Correlation heatmap\n",
    "fig6, ax6 = plt.subplots(figsize=(10, 8))\n",
    "high_corr_features = ['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-std()-X', \n",
    "                      'tGravityAcc-mean()-X', 'tGravityAcc-mean()-Y', 'tBodyAccMag-mean()']\n",
    "corr_matrix = train_df[high_corr_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, ax=ax6, cbar_kws={'shrink': 0.8}, vmin=-1, vmax=1)\n",
    "ax6.set_title('Selected Features Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/figures/2d_06_feature_correlation_heatmap.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"Saved: 2d_06_feature_correlation_heatmap.png\")\n",
    "\n",
    "end_time_2d = now()\n",
    "print(\"\\n\" )\n",
    "print(\"Section 2d: All 6 visualizations exported as well\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1797c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d) Record visual exploration with integrated figure names and interpretations\n",
    "\n",
    "du_2d_uuid_executor = deterministic_uuid(\"du_2d_visual_exploration:executor:activity\")\n",
    "du_2d_uuid_writer = deterministic_uuid(\"du_2d_visual_exploration:writer:activity\")\n",
    "\n",
    "# Section intro\n",
    "section_2d_intro = \"\"\"Visual exploration validates statistical findings and reveals patterns not immediately apparent from numerical analysis. We generate visualizations to confirm class distribution balance, verify subject representation, test hypotheses about activity-specific movement signatures, examine acceleration signal properties, and explore feature relationships. These visual insights guide modeling choices and feature engineering decisions.\"\"\"\n",
    "\n",
    "# Figure 1: Activity Class Distribution\n",
    "fig_2d_01_label = \"Figure 1: Activity Class Distribution Across Training Dataset\"\n",
    "fig_2d_01_text = f\"\"\"Activity Distribution Analysis: The training dataset contains 7352 samples distributed across 6 activity classes:\n",
    "- LAYING: {activity_counts.get('LAYING', 0)} samples (19.1%)\n",
    "- STANDING: {activity_counts.get('STANDING', 0)} samples (18.2%)\n",
    "- SITTING: {activity_counts.get('SITTING', 0)} samples (17.5%)\n",
    "- WALKING: {activity_counts.get('WALKING', 0)} samples (16.7%)\n",
    "- WALKING_UPSTAIRS: {activity_counts.get('WALKING_UPSTAIRS', 0)} samples (16.4%)\n",
    "- WALKING_DOWNSTAIRS: {activity_counts.get('WALKING_DOWNSTAIRS', 0)} samples (13.4%)\n",
    "\n",
    "The bar chart confirms moderate balance with no extreme class dominance. The imbalance ratio of {activity_counts.max() / activity_counts.min():.2f}:1 between most (LAYING) and least (WALKING_DOWNSTAIRS) frequent classes is manageable for standard classification approaches.\"\"\"\n",
    "\n",
    "# Figure 2: Subject Participation\n",
    "fig_2d_02_label = \"Figure 2: Sample Distribution Across 21 Training Subjects\"\n",
    "fig_2d_02_text = f\"\"\"Subject Representation Analysis: The 21 training subjects contribute between {subject_counts.min()} and {subject_counts.max()} samples each, with a balance ratio of {subject_counts.max() / subject_counts.min():.2f}:1. The bar chart shows relatively even participation across all subjects, indicating no single subject dominates the training data. This balanced representation reduces subject-specific bias and supports model generalization to new individuals. The average contribution is {subject_counts.mean():.0f} samples per subject with standard deviation of {subject_counts.std():.0f} samples.\"\"\"\n",
    "\n",
    "# Figure 3: Static vs Dynamic Variance\n",
    "fig_2d_03_label = \"Figure 3: Movement Variance Comparison Between Activity Types\"\n",
    "fig_2d_03_text = f\"\"\"Activity-Specific Variance Analysis: Comparing mean acceleration standard deviation (tBodyAcc-std()-X) between static activities (SITTING, STANDING, LAYING) and dynamic activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS) shows distinct magnitudes. Static activities exhibit mean standard deviation of {static_variance:.4f} while dynamic activities show {dynamic_variance:.4f}. Dynamic values are {dynamic_variance/static_variance:.2f}x the static magnitude, indicating static postures carry stronger negative standard-deviation signatures on this axis. Despite the lower magnitude for dynamic motions, the obvious separation between activity groups still supports variance-based discrimination.\"\"\"\n",
    "\n",
    "# Figure 4: Body Acceleration Distribution\n",
    "fig_2d_04_label = \"Figure 4: Body Acceleration Signal Distributions Along Three Axes\"\n",
    "fig_2d_04_text = \"\"\"Acceleration Signal Properties: Histograms of body acceleration components (tBodyAcc-mean()-X, Y, Z) show bell-shaped symmetric distributions centered near zero for all three axes. The X-axis distribution ranges approximately from -0.6 to 0.8, Y-axis from -0.5 to 0.4, and Z-axis from -0.7 to 0.6. These patterns indicate proper normalization of accelerometer data and removal of gravity components. The symmetric shape suggests minimal systematic measurement bias. Overlapping but distinct distributions across axes demonstrate that each axis captures unique movement information relevant for activity classification.\"\"\"\n",
    "\n",
    "# Figure 5: Mean Acceleration by Activity\n",
    "fig_2d_05_label = \"Figure 5: Activity-Specific Acceleration Patterns Across Axes\"\n",
    "fig_2d_05_text = f\"\"\"Activity Signature Analysis: Grouped bar charts comparing mean acceleration values across X, Y, Z axes for each activity reveal distinctive patterns. LAYING exhibits characteristic negative Y-axis values (approximately -0.4) reflecting horizontal body orientation with gravity acting along the body length. STANDING and SITTING show similar patterns with near-zero X and Y values and slightly positive Z values indicating upright posture. Walking activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS) demonstrate higher mean acceleration magnitudes with distinct patterns distinguishing upward (positive Z bias) from downward (negative Z bias) stair movement. These activity-specific signatures validate that mean acceleration features effectively separate activity classes.\"\"\"\n",
    "\n",
    "# Figure 6: Feature Correlation\n",
    "fig_2d_06_label = \"Figure 6: Correlation Structure of Representative Acceleration Features\"\n",
    "fig_2d_06_text = \"\"\"Feature Relationship Patterns: The correlation heatmap of six selected acceleration features (tBodyAcc-mean()-X, tBodyAcc-mean()-Y, tBodyAcc-std()-X, tGravityAcc-mean()-X, tGravityAcc-mean()-Y, tBodyAccMag-mean()) reveals mixed correlation structures. Body and gravity acceleration components along the same axis show weak to moderate correlations (r = 0.2 to 0.5) as expected from signal separation processing. Standard deviation features show low correlation with mean features (r < 0.3) indicating they capture independent variance information. The strongest correlation appears between tBodyAccMag and component features (r ≈ 0.8) reflecting magnitude's mathematical dependency on component vectors. This moderate correlation structure confirms that while some features share information, most provide complementary discriminative power for classification.\"\"\"\n",
    "\n",
    "# Section conclusion\n",
    "section_2d_conclusion = f\"\"\"Visual exploration confirms statistical findings and reveals activity-specific patterns crucial for modeling. The balanced class distribution ({activity_counts.max() / activity_counts.min():.2f}:1 imbalance ratio) and subject representation ({subject_counts.max() / subject_counts.min():.2f}:1 balance ratio) support robust model training. Static activities show stronger tBodyAcc-std()-X magnitude than dynamic activities (dynamic values are {dynamic_variance/static_variance:.2f}x the static level), yet the separation remains obvious enough to leverage variance features as discriminators. Activity-specific acceleration signatures, particularly LAYING with characteristic Y-axis orientation, provide distinctive patterns for classification. Normalized signal distributions confirm data quality while moderate feature correlations suggest opportunities for dimensionality reduction without significant information loss.\"\"\"\n",
    "\n",
    "# Build activity triples with intro, figures with detailed text, and conclusion\n",
    "activity_2d_triples = [\n",
    "    ':du_2d_visual_exploration rdf:type prov:Activity .',\n",
    "    ':du_2d_visual_exploration sc:isPartOf :data_understanding_phase .',\n",
    "    ':du_2d_visual_exploration rdfs:label \"Visual Exploration\" .',\n",
    "    f':du_2d_visual_exploration rdfs:comment \"\"\"{section_2d_intro}\"\"\" .',\n",
    "    f':du_2d_visual_exploration prov:startedAtTime \"{start_time_2d}\"^^xsd:dateTime .',\n",
    "    f':du_2d_visual_exploration prov:endedAtTime \"{end_time_2d}\"^^xsd:dateTime .',\n",
    "    f':du_2d_visual_exploration prov:qualifiedAssociation :{du_2d_uuid_writer} .',\n",
    "    f':{du_2d_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{du_2d_uuid_writer} prov:agent :{data_understanding_2d_code_writer} .',\n",
    "    f':{du_2d_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':du_2d_visual_exploration prov:qualifiedAssociation :{du_2d_uuid_executor} .',\n",
    "    f':{du_2d_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{du_2d_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{du_2d_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':du_2d_visual_exploration prov:used :train_df .',\n",
    "    \n",
    "    # Figure 1: Activity Class Distribution (with detailed analysis)\n",
    "    ':du_2d_fig_01 rdf:type prov:Entity .',\n",
    "    ':du_2d_fig_01 rdf:type sc:Figure .',\n",
    "    ':du_2d_fig_01 sc:isPartOf :du_2d_visual_exploration .',\n",
    "    f':du_2d_fig_01 rdfs:label \"{fig_2d_01_label}\" .',\n",
    "    f':du_2d_fig_01 rdfs:comment \"\"\"{fig_2d_01_text}\"\"\" .',\n",
    "    ':du_2d_fig_01 prov:wasGeneratedBy :du_2d_visual_exploration .',\n",
    "    \n",
    "    # Figure 2: Subject Participation (with detailed analysis)\n",
    "    ':du_2d_fig_02 rdf:type prov:Entity .',\n",
    "    ':du_2d_fig_02 rdf:type sc:Figure .',\n",
    "    ':du_2d_fig_02 sc:isPartOf :du_2d_visual_exploration .',\n",
    "    f':du_2d_fig_02 rdfs:label \"{fig_2d_02_label}\" .',\n",
    "    f':du_2d_fig_02 rdfs:comment \"\"\"{fig_2d_02_text}\"\"\" .',\n",
    "    ':du_2d_fig_02 prov:wasGeneratedBy :du_2d_visual_exploration .',\n",
    "    \n",
    "    # Figure 3: Static vs Dynamic Variance (with detailed analysis)\n",
    "    ':du_2d_fig_03 rdf:type prov:Entity .',\n",
    "    ':du_2d_fig_03 rdf:type sc:Figure .',\n",
    "    ':du_2d_fig_03 sc:isPartOf :du_2d_visual_exploration .',\n",
    "    f':du_2d_fig_03 rdfs:label \"{fig_2d_03_label}\" .',\n",
    "    f':du_2d_fig_03 rdfs:comment \"\"\"{fig_2d_03_text}\"\"\" .',\n",
    "    ':du_2d_fig_03 prov:wasGeneratedBy :du_2d_visual_exploration .',\n",
    "    \n",
    "    # Figure 4: Body Acceleration Distribution (with detailed analysis)\n",
    "    ':du_2d_fig_04 rdf:type prov:Entity .',\n",
    "    ':du_2d_fig_04 rdf:type sc:Figure .',\n",
    "    ':du_2d_fig_04 sc:isPartOf :du_2d_visual_exploration .',\n",
    "    f':du_2d_fig_04 rdfs:label \"{fig_2d_04_label}\" .',\n",
    "    f':du_2d_fig_04 rdfs:comment \"\"\"{fig_2d_04_text}\"\"\" .',\n",
    "    ':du_2d_fig_04 prov:wasGeneratedBy :du_2d_visual_exploration .',\n",
    "    \n",
    "    # Figure 5: Mean Acceleration by Activity (with detailed analysis)\n",
    "    ':du_2d_fig_05 rdf:type prov:Entity .',\n",
    "    ':du_2d_fig_05 rdf:type sc:Figure .',\n",
    "    ':du_2d_fig_05 sc:isPartOf :du_2d_visual_exploration .',\n",
    "    f':du_2d_fig_05 rdfs:label \"{fig_2d_05_label}\" .',\n",
    "    f':du_2d_fig_05 rdfs:comment \"\"\"{fig_2d_05_text}\"\"\" .',\n",
    "    ':du_2d_fig_05 prov:wasGeneratedBy :du_2d_visual_exploration .',\n",
    "    \n",
    "    # Figure 6: Feature Correlation (with detailed analysis)\n",
    "    ':du_2d_fig_06 rdf:type prov:Entity .',\n",
    "    ':du_2d_fig_06 rdf:type sc:Figure .',\n",
    "    ':du_2d_fig_06 sc:isPartOf :du_2d_visual_exploration .',\n",
    "    f':du_2d_fig_06 rdfs:label \"{fig_2d_06_label}\" .',\n",
    "    f':du_2d_fig_06 rdfs:comment \"\"\"{fig_2d_06_text}\"\"\" .',\n",
    "    ':du_2d_fig_06 prov:wasGeneratedBy :du_2d_visual_exploration .',\n",
    "    \n",
    "    # Overall conclusion\n",
    "    ':du_2d_conclusion rdf:type prov:Entity .',\n",
    "    ':du_2d_conclusion sc:isPartOf :du_2d_visual_exploration .',\n",
    "    f':du_2d_conclusion rdfs:label \"Visual Exploration Conclusion\" .',\n",
    "    f':du_2d_conclusion rdfs:comment \"\"\"{section_2d_conclusion}\"\"\" .',\n",
    "    ':du_2d_conclusion prov:wasGeneratedBy :du_2d_visual_exploration .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(activity_2d_triples, prefixes=prefixes)\n",
    "    print(\"Section 2d complete - Visual exploration with 6 figures and comprehensive interpretations recorded\")\n",
    "except:\n",
    "    print(\"Note: Could not insert to graph DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5fa03",
   "metadata": {},
   "source": [
    "### 2e) Ethical Sensitivity and Bias Evaluation\n",
    "Analyze ethically sensitive attributes, minority classes, underrepresented groups, and bias in distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c2638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2e) Ethical sensitivity analysis - Examine class balance and representation\n",
    "start_time_2e = now()\n",
    "\n",
    "# 1. Class balance analysis for bias assessment\n",
    "activity_distribution = train_df['Activity'].value_counts()\n",
    "total_samples = len(train_df)\n",
    "activity_percentages = (activity_distribution / total_samples * 100).round(2)\n",
    "\n",
    "print(\"Class Distribution Analysis\")\n",
    "for activity, count in activity_distribution.items():\n",
    "    percentage = activity_percentages[activity]\n",
    "    print(f\"{activity}: {count} samples ({percentage}%)\")\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "max_class = activity_distribution.max()\n",
    "min_class = activity_distribution.min()\n",
    "imbalance_ratio = max_class / min_class\n",
    "print(f\"\\nImbalance ratio (max/min): {imbalance_ratio:.2f}\")\n",
    "\n",
    "# 2. Identify minority and majority classes\n",
    "minority_threshold = total_samples / len(activity_distribution) * 0.9\n",
    "minority_classes = activity_distribution[activity_distribution < minority_threshold].index.tolist()\n",
    "majority_classes = activity_distribution[activity_distribution >= minority_threshold].index.tolist()\n",
    "\n",
    "print(f\"\\nMinority classes (below 90% of average): {minority_classes}\")\n",
    "print(f\"Majority classes: {majority_classes}\")\n",
    "\n",
    "# 3. Subject representation analysis\n",
    "subject_distribution = train_df['subject'].value_counts()\n",
    "print(f\"\\nSubject Representation\")\n",
    "print(f\"Number of subjects: {len(subject_distribution)}\")\n",
    "print(f\"Samples per subject - Min: {subject_distribution.min()}, Max: {subject_distribution.max()}\")\n",
    "print(f\"Subject balance ratio: {subject_distribution.max() / subject_distribution.min():.2f}\")\n",
    "\n",
    "# 4. Activity type grouping for bias assessment\n",
    "static_activities = ['SITTING', 'STANDING', 'LAYING']\n",
    "dynamic_activities = ['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS']\n",
    "\n",
    "static_count = train_df[train_df['Activity'].isin(static_activities)].shape[0]\n",
    "dynamic_count = train_df[train_df['Activity'].isin(dynamic_activities)].shape[0]\n",
    "\n",
    "print(f\"\\n Activity Type Balance\")\n",
    "print(f\"Static activities: {static_count} samples ({static_count/total_samples*100:.1f}%)\")\n",
    "print(f\"Dynamic activities: {dynamic_count} samples ({dynamic_count/total_samples*100:.1f}%)\")\n",
    "print(f\"Static/Dynamic ratio: {static_count/dynamic_count:.2f}\")\n",
    "\n",
    "# 5. Check for demographic information\n",
    "available_columns = train_df.columns.tolist()\n",
    "demographic_fields = ['age', 'gender', 'height', 'weight', 'ethnicity']\n",
    "available_demographics = [field for field in demographic_fields if field in available_columns]\n",
    "\n",
    "print(f\"\\nDemographic Information \")\n",
    "if available_demographics:\n",
    "    print(f\"Available demographic fields: {available_demographics}\")\n",
    "else:\n",
    "    print(\"No demographic information available in dataset\")\n",
    "    print(\"Subject IDs are anonymized without age, gender, or other personal attributes\")\n",
    "\n",
    "end_time_2e = now()\n",
    "print(\"\\nEthical sensitivity analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a177b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2e) Record ethical sensitivity evaluation\n",
    "\n",
    "# Section intro\n",
    "section_2e_intro = \"\"\"Ethical considerations and bias assessment are essential responsibilities in machine learning applications involving human behavior data. We evaluate class representation to identify minority groups at risk of underperformance, examine subject participation balance to detect potential demographic biases, assess activity type balance between static and dynamic behaviors, verify absence of sensitive attributes to protect privacy, and consider generalization limitations to underrepresented populations not included in training data.\"\"\"\n",
    "\n",
    "# Ethical finding 1: Class imbalance and minority representation\n",
    "ethical_finding_01_label = \"Finding 1: Moderate Class Imbalance and Minority Activity Representation\"\n",
    "ethical_finding_01_text = f\"\"\"Class Distribution Assessment: The 6 activity classes range from {activity_distribution.min()} (WALKING_DOWNSTAIRS, 13.4%) to {activity_distribution.max()} (LAYING, 19.1%) samples. The imbalance ratio of {activity_distribution.max() / activity_distribution.min():.2f}:1 indicates moderate imbalance. WALKING_DOWNSTAIRS and WALKING_UPSTAIRS are minority classes with fewer samples than other activities. This imbalance creates risk that models may achieve higher accuracy on majority classes like LAYING while underperforming on minority classes. Evaluation must report per-class metrics to ensure minority activities receive adequate performance attention rather than being obscured by overall accuracy metrics dominated by majority classes.\"\"\"\n",
    "\n",
    "# Ethical finding 2: Subject representation balance\n",
    "ethical_finding_02_label = \"Finding 2: Balanced Subject Participation Without Demographic Context\"\n",
    "ethical_finding_02_text = f\"\"\"Subject Representation Analysis: All 21 training subjects contribute between {subject_counts.min()} to {subject_counts.max()} samples with balance ratio of {subject_counts.max() / subject_counts.min():.2f}:1. This reasonable balance prevents single-subject dominance and reduces subject-specific bias. However the dataset provides no demographic metadata (age, gender, ethnicity, body type, fitness level). This absence prevents assessment of whether specific demographic groups are underrepresented. Without this information we cannot verify if elderly individuals, children, people with disabilities, or diverse body types are adequately represented. Models trained on this data should not be deployed to populations substantially different from the likely young adult university student cohort without additional validation.\"\"\"\n",
    "\n",
    "# Ethical finding 3: Activity type imbalance\n",
    "ethical_finding_03_label = \"Finding 3: Static Activity Overrepresentation Relative to Dynamic Activities\"\n",
    "static_count = train_df[train_df['Activity'].isin(['SITTING', 'STANDING', 'LAYING'])].shape[0]\n",
    "dynamic_count = train_df[train_df['Activity'].isin(['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS'])].shape[0]\n",
    "ethical_finding_03_text = f\"\"\"Activity Type Balance: Static activities (LAYING, STANDING, SITTING) comprise {static_count} samples ({static_count/len(train_df)*100:.1f}%) while dynamic activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS) comprise {dynamic_count} samples ({dynamic_count/len(train_df)*100:.1f}%). The static to dynamic ratio of {static_count/dynamic_count:.2f}:1 indicates noticeable imbalance favoring sedentary behaviors. This imbalance could lead to models that perform better on static activities at the expense of dynamic movement recognition. Applications focused on physical activity monitoring or rehabilitation requiring accurate dynamic activity detection may suffer reduced performance due to this training bias.\"\"\"\n",
    "\n",
    "# Ethical finding 4: Sensitive attributes and privacy\n",
    "ethical_finding_04_label = \"Finding 4: Absence of Explicitly Sensitive Attributes with Anonymized Identifiers\"\n",
    "ethical_finding_04_text = \"\"\"Sensitive Attribute Assessment: The dataset contains no explicitly sensitive demographic or health attributes. Subject identifiers are anonymized integers from 1 to 30 providing privacy protection. No fields capture age, gender, ethnicity, disability status, medical conditions, or other protected characteristics. However movement patterns captured by accelerometer and gyroscope sensors may indirectly reveal information about physical capability, fitness level, gait abnormalities, or mobility limitations. These inferred characteristics could be considered sensitive in contexts like employment screening or insurance assessment. Deployment contexts must consider whether movement pattern analysis could enable discrimination against individuals with reduced mobility or physical differences.\"\"\"\n",
    "\n",
    "# Ethical finding 5: Underrepresented populations\n",
    "ethical_finding_05_label = \"Finding 5: Likely Underrepresentation of Diverse Demographics and Mobility Profiles\"\n",
    "ethical_finding_05_text = \"\"\"Generalization Limitations: The dataset originates from 30 volunteers performing activities in controlled laboratory conditions. While exact demographics are undocumented, the recruitment from a research institution suggests predominant representation of young healthy adults. This likely excludes elderly individuals with altered gait patterns, children with different body proportions and movement styles, people with mobility impairments using assistive devices, individuals with neurological conditions affecting movement, pregnant women with modified movement patterns, and people with diverse body types affecting sensor placement and signal characteristics. Models trained exclusively on able-bodied young adults will likely exhibit performance degradation when applied to these underrepresented populations. Deployment to these groups without additional validation creates ethical risk of inadequate service quality.\"\"\"\n",
    "\n",
    "# Section conclusion\n",
    "section_2e_conclusion = f\"\"\"Ethical sensitivity analysis identifies moderate class imbalance ({activity_distribution.max() / activity_distribution.min():.2f}:1 ratio) requiring per-class performance monitoring, balanced subject participation ({subject_counts.max() / subject_counts.min():.2f}:1 ratio) without demographic verification, static activity overrepresentation ({static_count/dynamic_count:.2f}:1 ratio) potentially biasing models toward sedentary behaviors, absence of explicit sensitive attributes but potential for indirect inference of physical capabilities, and likely underrepresentation of elderly, disabled, and diverse populations limiting generalization. Ethical deployment requires stratified evaluation ensuring minority class performance, documentation of population limitations, fairness testing on diverse demographics when possible, and informed consent acknowledging movement data sensitivity.\"\"\"\n",
    "\n",
    "# Generate deterministic UUIDs\n",
    "du_2e_uuid_executor = deterministic_uuid(\"du_2e_ethical_sensitivity:executor:activity\")\n",
    "du_2e_uuid_writer = deterministic_uuid(\"du_2e_ethical_sensitivity:writer:activity\")\n",
    "\n",
    "# Build activity triples with intro, ethical findings, and conclusion\n",
    "activity_2e_triples = [\n",
    "    ':du_2e_ethical_sensitivity rdf:type prov:Activity .',\n",
    "    ':du_2e_ethical_sensitivity sc:isPartOf :data_understanding_phase .',\n",
    "    ':du_2e_ethical_sensitivity rdfs:label \"Ethical Sensitivity and Bias Evaluation\" .',\n",
    "    f':du_2e_ethical_sensitivity rdfs:comment \"\"\"{section_2e_intro}\"\"\" .',\n",
    "    f':du_2e_ethical_sensitivity prov:startedAtTime \"{start_time_2e}\"^^xsd:dateTime .',\n",
    "    f':du_2e_ethical_sensitivity prov:endedAtTime \"{end_time_2e}\"^^xsd:dateTime .',\n",
    "    f':du_2e_ethical_sensitivity prov:qualifiedAssociation :{du_2e_uuid_writer} .',\n",
    "    f':{du_2e_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{du_2e_uuid_writer} prov:agent :{data_understanding_2d_code_writer} .',\n",
    "    f':{du_2e_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':du_2e_ethical_sensitivity prov:qualifiedAssociation :{du_2e_uuid_executor} .',\n",
    "    f':{du_2e_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{du_2e_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{du_2e_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':du_2e_ethical_sensitivity prov:used :train_df .',\n",
    "    \n",
    "    # Ethical Finding 1: Class imbalance (with detailed analysis)\n",
    "    ':du_2e_finding_01 rdf:type prov:Entity .',\n",
    "    ':du_2e_finding_01 rdf:type sc:EthicalAssessment .',\n",
    "    ':du_2e_finding_01 sc:isPartOf :du_2e_ethical_sensitivity .',\n",
    "    f':du_2e_finding_01 rdfs:label \"{ethical_finding_01_label}\" .',\n",
    "    f':du_2e_finding_01 rdfs:comment \"\"\"{ethical_finding_01_text}\"\"\" .',\n",
    "    ':du_2e_finding_01 prov:wasGeneratedBy :du_2e_ethical_sensitivity .',\n",
    "    \n",
    "    # Ethical Finding 2: Subject representation (with detailed analysis)\n",
    "    ':du_2e_finding_02 rdf:type prov:Entity .',\n",
    "    ':du_2e_finding_02 rdf:type sc:EthicalAssessment .',\n",
    "    ':du_2e_finding_02 sc:isPartOf :du_2e_ethical_sensitivity .',\n",
    "    f':du_2e_finding_02 rdfs:label \"{ethical_finding_02_label}\" .',\n",
    "    f':du_2e_finding_02 rdfs:comment \"\"\"{ethical_finding_02_text}\"\"\" .',\n",
    "    ':du_2e_finding_02 prov:wasGeneratedBy :du_2e_ethical_sensitivity .',\n",
    "    \n",
    "    # Ethical Finding 3: Activity type balance (with detailed analysis)\n",
    "    ':du_2e_finding_03 rdf:type prov:Entity .',\n",
    "    ':du_2e_finding_03 rdf:type sc:EthicalAssessment .',\n",
    "    ':du_2e_finding_03 sc:isPartOf :du_2e_ethical_sensitivity .',\n",
    "    f':du_2e_finding_03 rdfs:label \"{ethical_finding_03_label}\" .',\n",
    "    f':du_2e_finding_03 rdfs:comment \"\"\"{ethical_finding_03_text}\"\"\" .',\n",
    "    ':du_2e_finding_03 prov:wasGeneratedBy :du_2e_ethical_sensitivity .',\n",
    "    \n",
    "    # Ethical Finding 4: Sensitive attributes (with detailed analysis)\n",
    "    ':du_2e_finding_04 rdf:type prov:Entity .',\n",
    "    ':du_2e_finding_04 rdf:type sc:EthicalAssessment .',\n",
    "    ':du_2e_finding_04 sc:isPartOf :du_2e_ethical_sensitivity .',\n",
    "    f':du_2e_finding_04 rdfs:label \"{ethical_finding_04_label}\" .',\n",
    "    f':du_2e_finding_04 rdfs:comment \"\"\"{ethical_finding_04_text}\"\"\" .',\n",
    "    ':du_2e_finding_04 prov:wasGeneratedBy :du_2e_ethical_sensitivity .',\n",
    "    \n",
    "    # Ethical Finding 5: Underrepresented populations (with detailed analysis)\n",
    "    ':du_2e_finding_05 rdf:type prov:Entity .',\n",
    "    ':du_2e_finding_05 rdf:type sc:EthicalAssessment .',\n",
    "    ':du_2e_finding_05 sc:isPartOf :du_2e_ethical_sensitivity .',\n",
    "    f':du_2e_finding_05 rdfs:label \"{ethical_finding_05_label}\" .',\n",
    "    f':du_2e_finding_05 rdfs:comment \"\"\"{ethical_finding_05_text}\"\"\" .',\n",
    "    ':du_2e_finding_05 prov:wasGeneratedBy :du_2e_ethical_sensitivity .',\n",
    "    \n",
    "    # Overall conclusion\n",
    "    ':du_2e_conclusion rdf:type prov:Entity .',\n",
    "    ':du_2e_conclusion sc:isPartOf :du_2e_ethical_sensitivity .',\n",
    "    f':du_2e_conclusion rdfs:label \"Ethical Sensitivity Conclusion\" .',\n",
    "    f':du_2e_conclusion rdfs:comment \"\"\"{section_2e_conclusion}\"\"\" .',\n",
    "    ':du_2e_conclusion prov:wasGeneratedBy :du_2e_ethical_sensitivity .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(activity_2e_triples, prefixes=prefixes)\n",
    "    print(\"Section 2e complete - Ethical sensitivity evaluation with 5 findings and comprehensive assessment recorded\")\n",
    "except:\n",
    "    print(\"Note: Could not insert to graph DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bacedb",
   "metadata": {},
   "source": [
    "### 2f) Risks and Potential Biases\n",
    "\n",
    "This section identifies potential risks and biases in the dataset and formulates expert questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca91717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2f) Analysis of potential risks and biases - examine data characteristics and model risks\n",
    "start_time_2f = now()\n",
    "\n",
    "# 1. Dimensionality analysis for overfitting risk\n",
    "num_features = train_df.shape[1] - 2  # Exclude Activity and subject columns\n",
    "num_samples = train_df.shape[0]\n",
    "feature_to_sample_ratio = num_features / num_samples\n",
    "\n",
    "print(\" Dimensionality and Overfitting Risk\")\n",
    "print(f\"Number of features: {num_features}\")\n",
    "print(f\"Number of training samples: {num_samples}\")\n",
    "print(f\"Feature to sample ratio: {feature_to_sample_ratio:.4f}\")\n",
    "print(f\"Interpretation: {'High risk of overfitting' if feature_to_sample_ratio > 0.01 else 'Moderate dimensionality'}\")\n",
    "\n",
    "# 2. Class imbalance analysis for bias risk\n",
    "print(f\"\\nClass Imbalance Risk Quantification \")\n",
    "print(f\"Activity distribution:\")\n",
    "for activity, count in activity_distribution.items():\n",
    "    percentage = (count / num_samples * 100)\n",
    "    print(f\"  {activity}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# 3. Sensor and device risk analysis\n",
    "print(f\"\\nSensor and Measurement Risk \")\n",
    "print(f\"Data source: Smartphone accelerometer and gyroscope\")\n",
    "print(f\"Assumed placement: Waist-mounted (subject-specific attachment)\")\n",
    "print(f\"Potential risks:\")\n",
    "print(f\"  - Different phone models may have different sensor characteristics\")\n",
    "print(f\"  - Sensor calibration varies between devices\")\n",
    "print(f\"  - Subject positioning and attachment method not documented\")\n",
    "print(f\"  - No mention of accelerometer noise or drift\")\n",
    "\n",
    "# 4. Collection context analysis\n",
    "print(f\"\\n Data Collection Context Risk \")\n",
    "print(f\"Participants: 21 subjects (likely university students)\")\n",
    "print(f\"Collection setting: Controlled laboratory environment\")\n",
    "print(f\"Activity execution: Performed on command for 5-6 minute intervals\")\n",
    "print(f\"Generalization concerns:\")\n",
    "print(f\"  - Laboratory setting may not reflect real-world movement variability\")\n",
    "print(f\"  - Subjects performing activities on request may move differently than naturally\")\n",
    "print(f\"  - No environmental factors (weather, terrain, obstacles)\")\n",
    "\n",
    "# 5. Feature redundancy analysis\n",
    "print(f\"\\n Feature Redundancy and Correlation\")\n",
    "corr_matrix = train_df[sensor_features].corr()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_count = (upper.abs() > 0.95).sum().sum()\n",
    "\n",
    "print(f\"High correlation pairs (threshold > 0.95): {high_corr_count}\")\n",
    "print(f\"Potential issues:\")\n",
    "print(f\"  - Many correlated features increase multicollinearity\")\n",
    "print(f\"  - Redundant features do not add information but increase model complexity\")\n",
    "print(f\"  - May worsen overfitting risk\")\n",
    "\n",
    "end_time_2f = now()\n",
    "print(\"\\nRisk and bias analysis completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2f) Record risk and bias evaluation\n",
    "\n",
    "# Section intro\n",
    "section_2f_intro = \"\"\"Risk assessment and bias identification are necessary for responsible model development and deployment. We analyze dimensionality to quantify overfitting risk, evaluate class imbalance effects on prediction bias, assess sensor and device dependencies affecting generalization, examine data collection context limitations restricting real-world applicability, and identify feature redundancy contributing to model complexity. These risk factors inform model selection, regularization strategies, and deployment constraints.\"\"\"\n",
    "\n",
    "# Risk finding 1: Dimensionality and overfitting\n",
    "risk_finding_01_label = \"Finding 1: High Dimensionality Creating Substantial Overfitting Risk\"\n",
    "risk_finding_01_text = f\"\"\"Dimensionality Analysis: The dataset contains {num_features} features with {num_samples} training samples, yielding a feature-to-sample ratio of {feature_to_sample_ratio:.4f} ({num_features}/{num_samples}). This ratio indicates high dimensionality relative to sample size, creating substantial overfitting risk especially for complex nonlinear models. Additionally {high_corr_count} feature pairs exceed 0.95 correlation threshold and over 23,000 pairs show correlation above 0.8. This extensive multicollinearity compounds overfitting risk by increasing model coefficient variance. Without regularization or dimensionality reduction, models may memorize training patterns rather than learning generalizable activity signatures.\"\"\"\n",
    "\n",
    "# Risk finding 2: Class imbalance prediction bias\n",
    "risk_finding_02_label = \"Finding 2: Moderate Class Imbalance Risk Affecting Minority Activity Performance\"\n",
    "risk_finding_02_text = f\"\"\"Class Imbalance Risk: The 6 activity classes exhibit imbalance ratio of {imbalance_ratio:.2f}:1 between most frequent (LAYING with {activity_counts.max()} samples, 19.1%) and least frequent (WALKING_DOWNSTAIRS with {activity_counts.min()} samples, 13.4%). This moderate imbalance creates prediction bias where models may achieve higher accuracy on majority classes while underperforming on minority classes. Standard accuracy metrics can obscure this disparity as correct predictions on frequent classes dominate the overall score. Minority classes like WALKING_DOWNSTAIRS suffer from reduced training examples limiting the model's ability to learn their characteristic patterns.\"\"\"\n",
    "\n",
    "# Risk finding 3: Sensor and device generalization\n",
    "risk_finding_03_label = \"Finding 3: Sensor Hardware and Calibration Dependencies Limiting Cross-Device Generalization\"\n",
    "risk_finding_03_text = \"\"\"Sensor Device Risk: Data collection used smartphone accelerometer and gyroscope sensors with waist-mounted placement. Sensor characteristics vary substantially across smartphone models including sampling rate accuracy, noise levels, dynamic range, and calibration procedures. The dataset does not document which smartphone model was used, sensor calibration methods, or attachment consistency across subjects. This creates dependency risk where models trained on one device's sensor characteristics may perform poorly on different hardware. Real-world deployment encounters diverse smartphone models, inconsistent placement (pocket versus belt versus arm), and varying sensor quality. Without device-agnostic features or multi-device training data, generalization to heterogeneous deployment environments is uncertain.\"\"\"\n",
    "\n",
    "# Risk finding 4: Collection context limitations\n",
    "risk_finding_04_label = \"Finding 4: Artificial Laboratory Context Reducing Real-World Applicability\"\n",
    "risk_finding_04_text = \"\"\"Data Collection Context Risk: The 21 subjects performed activities in controlled laboratory settings on command for fixed 5-6 minute intervals. This artificial context differs fundamentally from natural behavior where activities vary spontaneously, transitions occur unpredictably, and environmental factors influence movement. Laboratory conditions lack terrain variation (stairs versus ramps versus escalators), weather effects (slippery surfaces, wind resistance), obstacles requiring navigation adjustments, concurrent activities (carrying objects, conversing), and natural fatigue progression. Subjects performing prescribed activities may exhibit more stereotyped movement patterns than spontaneous behavior. Models trained on this constrained data may struggle with the variability inherent in unconstrained real-world settings.\"\"\"\n",
    "\n",
    "# Risk finding 5: Feature redundancy complexity\n",
    "risk_finding_05_label = \"Finding 5: Extensive Feature Redundancy Increasing Model Complexity and Training Instability\"\n",
    "risk_finding_05_text = f\"\"\"Feature Redundancy Risk: Correlation analysis identified {high_corr_count} feature pairs exceeding 0.95 correlation and over 23,000 pairs above 0.8 correlation. This extensive redundancy means many features encode nearly identical information, contributing minimal discriminative value while increasing model complexity. For linear models, multicollinearity inflates coefficient variance reducing interpretability and stability. For tree-based models, redundant features dilute feature importance scores and increase training time. The high dimensionality combined with redundancy creates search space inefficiency during hyperparameter tuning. Feature selection or dimensionality reduction is strongly recommended to eliminate redundant features while preserving discriminative information.\"\"\"\n",
    "\n",
    "# Section conclusion\n",
    "section_2f_conclusion = f\"\"\"Risk assessment reveals high dimensionality ({feature_to_sample_ratio:.4f} feature-to-sample ratio) creating substantial overfitting risk requiring regularization or dimensionality reduction, moderate class imbalance ({imbalance_ratio:.2f}:1 ratio) necessitating stratified evaluation and minority class monitoring, sensor hardware dependencies limiting cross-device generalization without multi-device validation, artificial laboratory context reducing real-world applicability requiring field testing before deployment, and extensive feature redundancy ({high_corr_count} pairs > 0.95 correlation) motivating feature selection to reduce model complexity. Mitigation strategies must address these interconnected risks through appropriate regularization, evaluation protocols, deployment constraints, and feature engineering.\"\"\"\n",
    "\n",
    "# Generate deterministic UUIDs\n",
    "du_2f_uuid_executor = deterministic_uuid(\"du_2f_risks_biases:executor:activity\")\n",
    "du_2f_uuid_writer = deterministic_uuid(\"du_2f_risks_biases:writer:activity\")\n",
    "data_understanding_2f_code_writer = student_a\n",
    "\n",
    "# Build activity triples with intro, risk findings, and conclusion\n",
    "activity_2f_triples = [\n",
    "    ':du_2f_risks_biases rdf:type prov:Activity .',\n",
    "    ':du_2f_risks_biases sc:isPartOf :data_understanding_phase .',\n",
    "    ':du_2f_risks_biases rdfs:label \"Risks and Potential Biases\" .',\n",
    "    f':du_2f_risks_biases rdfs:comment \"\"\"{section_2f_intro}\"\"\" .',\n",
    "    f':du_2f_risks_biases prov:startedAtTime \"{start_time_2f}\"^^xsd:dateTime .',\n",
    "    f':du_2f_risks_biases prov:endedAtTime \"{end_time_2f}\"^^xsd:dateTime .',\n",
    "    f':du_2f_risks_biases prov:qualifiedAssociation :{du_2f_uuid_writer} .',\n",
    "    f':{du_2f_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{du_2f_uuid_writer} prov:agent :{data_understanding_2f_code_writer} .',\n",
    "    f':{du_2f_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':du_2f_risks_biases prov:qualifiedAssociation :{du_2f_uuid_executor} .',\n",
    "    f':{du_2f_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{du_2f_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{du_2f_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':du_2f_risks_biases prov:used :train_df .',\n",
    "    \n",
    "    # Risk Finding 1: Dimensionality (with detailed analysis)\n",
    "    ':du_2f_risk_01 rdf:type prov:Entity .',\n",
    "    ':du_2f_risk_01 rdf:type sc:RiskAssessment .',\n",
    "    ':du_2f_risk_01 sc:isPartOf :du_2f_risks_biases .',\n",
    "    f':du_2f_risk_01 rdfs:label \"{risk_finding_01_label}\" .',\n",
    "    f':du_2f_risk_01 rdfs:comment \"\"\"{risk_finding_01_text}\"\"\" .',\n",
    "    ':du_2f_risk_01 prov:wasGeneratedBy :du_2f_risks_biases .',\n",
    "    \n",
    "    # Risk Finding 2: Class imbalance (with detailed analysis)\n",
    "    ':du_2f_risk_02 rdf:type prov:Entity .',\n",
    "    ':du_2f_risk_02 rdf:type sc:RiskAssessment .',\n",
    "    ':du_2f_risk_02 sc:isPartOf :du_2f_risks_biases .',\n",
    "    f':du_2f_risk_02 rdfs:label \"{risk_finding_02_label}\" .',\n",
    "    f':du_2f_risk_02 rdfs:comment \"\"\"{risk_finding_02_text}\"\"\" .',\n",
    "    ':du_2f_risk_02 prov:wasGeneratedBy :du_2f_risks_biases .',\n",
    "    \n",
    "    # Risk Finding 3: Sensor dependencies (with detailed analysis)\n",
    "    ':du_2f_risk_03 rdf:type prov:Entity .',\n",
    "    ':du_2f_risk_03 rdf:type sc:RiskAssessment .',\n",
    "    ':du_2f_risk_03 sc:isPartOf :du_2f_risks_biases .',\n",
    "    f':du_2f_risk_03 rdfs:label \"{risk_finding_03_label}\" .',\n",
    "    f':du_2f_risk_03 rdfs:comment \"\"\"{risk_finding_03_text}\"\"\" .',\n",
    "    ':du_2f_risk_03 prov:wasGeneratedBy :du_2f_risks_biases .',\n",
    "    \n",
    "    # Risk Finding 4: Collection context (with detailed analysis)\n",
    "    ':du_2f_risk_04 rdf:type prov:Entity .',\n",
    "    ':du_2f_risk_04 rdf:type sc:RiskAssessment .',\n",
    "    ':du_2f_risk_04 sc:isPartOf :du_2f_risks_biases .',\n",
    "    f':du_2f_risk_04 rdfs:label \"{risk_finding_04_label}\" .',\n",
    "    f':du_2f_risk_04 rdfs:comment \"\"\"{risk_finding_04_text}\"\"\" .',\n",
    "    ':du_2f_risk_04 prov:wasGeneratedBy :du_2f_risks_biases .',\n",
    "    \n",
    "    # Risk Finding 5: Feature redundancy (with detailed analysis)\n",
    "    ':du_2f_risk_05 rdf:type prov:Entity .',\n",
    "    ':du_2f_risk_05 rdf:type sc:RiskAssessment .',\n",
    "    ':du_2f_risk_05 sc:isPartOf :du_2f_risks_biases .',\n",
    "    f':du_2f_risk_05 rdfs:label \"{risk_finding_05_label}\" .',\n",
    "    f':du_2f_risk_05 rdfs:comment \"\"\"{risk_finding_05_text}\"\"\" .',\n",
    "    ':du_2f_risk_05 prov:wasGeneratedBy :du_2f_risks_biases .',\n",
    "    \n",
    "    # Overall conclusion\n",
    "    ':du_2f_conclusion rdf:type prov:Entity .',\n",
    "    ':du_2f_conclusion sc:isPartOf :du_2f_risks_biases .',\n",
    "    f':du_2f_conclusion rdfs:label \"Risk Assessment Conclusion\" .',\n",
    "    f':du_2f_conclusion rdfs:comment \"\"\"{section_2f_conclusion}\"\"\" .',\n",
    "    ':du_2f_conclusion prov:wasGeneratedBy :du_2f_risks_biases .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(activity_2f_triples, prefixes=prefixes)\n",
    "    print(\"Section 2f complete - Risk and bias assessment with 5 findings and comprehensive evaluation recorded\")\n",
    "except:\n",
    "    print(\"Note: Could not insert to graph DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85bf9fd",
   "metadata": {},
   "source": [
    "### 2g) Required Data Preparation Actions\n",
    "\n",
    "This section outlines the necessary data preparation steps based on findings from sections 2a-2f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b845a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2g) Analysis of required data preparation actions \n",
    "start_time_2g = now()\n",
    "# Provenance: code writer\n",
    "data_understanding_2g_code_writer = student_a\n",
    "# 1. Feature scaling verification\n",
    "print(\" Feature Scaling Status\")\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "sensor_cols = numeric_cols.drop('subject', errors='ignore')\n",
    "if numeric_cols.size > 0:\n",
    "    min_val = train_df[sensor_cols].min().min()\n",
    "    max_val = train_df[sensor_cols].max().max()\n",
    "    print(f\"Overall feature value range: [{min_val:.4f}, {max_val:.4f}]\")\n",
    "    if -1.1 <= min_val <= 0 and 0 <= max_val <= 1.1:\n",
    "        print(\"Status: Features already normalized to approximately [-1, 1]\")\n",
    "        scaling_needed = False\n",
    "    else:\n",
    "        print(\"Status: Features may need scaling\")\n",
    "        scaling_needed = True\n",
    "else:\n",
    "    print(\"No numeric features found\")\n",
    "    scaling_needed = False\n",
    "\n",
    "# 2. Target variable encoding check\n",
    "print(f\"\\n Target Variable Status \")\n",
    "print(f\"Activity column type: {train_df['Activity'].dtype}\")\n",
    "print(f\"Unique activities: {train_df['Activity'].nunique()}\")\n",
    "print(f\"Sample activities: {train_df['Activity'].unique()[:3].tolist()}\")\n",
    "if train_df['Activity'].dtype == 'object':\n",
    "    print(\"Status: Target variable is categorical text - encoding required\")\n",
    "    target_encoding_needed = True\n",
    "else:\n",
    "    print(\"Status: Target variable already numeric\")\n",
    "    target_encoding_needed = False\n",
    "\n",
    "# 3. Train-test subject separation check\n",
    "print(f\"\\n Train-Test Subject Separation \")\n",
    "train_subjects = set(train_df['subject'].unique())\n",
    "test_subjects = set(test_df['subject'].unique())\n",
    "overlap_subjects = train_subjects.intersection(test_subjects)\n",
    "print(f\"Training subjects: {len(train_subjects)}\")\n",
    "print(f\"Test subjects: {len(test_subjects)}\")\n",
    "print(f\"Overlapping subjects: {len(overlap_subjects)}\")\n",
    "if len(overlap_subjects) == 0:\n",
    "    print(\"Status: Good - Train and test sets use completely different subjects\")\n",
    "    split_validation_needed = False\n",
    "else:\n",
    "    print(\"Status: Warning - Same subjects appear in both sets\")\n",
    "    split_validation_needed = True\n",
    "\n",
    "# 4. Missing values and data completeness\n",
    "print(f\"\\n Data Completeness\")\n",
    "print(f\"Total rows in training set: {len(train_df)}\")\n",
    "print(f\"Total rows in test set: {len(test_df)}\")\n",
    "print(f\"Missing values in train set: {train_df.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test set: {test_df.isnull().sum().sum()}\")\n",
    "print(\"Status: No missing values - no imputation needed\")\n",
    "\n",
    "# 5. High correlation feature summary\n",
    "print(f\"\\n Multicollinearity Assessment \")\n",
    "print(f\"High correlation feature pairs (> 0.95): {high_corr_count}\")\n",
    "if high_corr_count > 50:\n",
    "    print(\"Status: Significant multicollinearity present - feature reduction recommended\")\n",
    "    feature_reduction_recommended = True\n",
    "else:\n",
    "    print(\"Status: Moderate multicollinearity - optional to address\")\n",
    "    feature_reduction_recommended = True\n",
    "\n",
    "# 6. Class balance for stratification\n",
    "print(f\"\\n Class Balance Summary \")\n",
    "print(f\"Most common activity: {activity_distribution.index[0]} ({activity_distribution.iloc[0]} samples)\")\n",
    "print(f\"Least common activity: {activity_distribution.index[-1]} ({activity_distribution.iloc[-1]} samples)\")\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "if imbalance_ratio > 1.4:\n",
    "    print(\"Status: Moderate imbalance detected - stratified sampling essential\")\n",
    "    stratified_cv_required = True\n",
    "else:\n",
    "    print(\"Status: Manageable imbalance - standard sampling acceptable\")\n",
    "    stratified_cv_required = True  # Still recommended\n",
    "\n",
    "end_time_2g = now()\n",
    "print(\"\\nData preparation analysis completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4385e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2g) Record data preparation requirements\n",
    "\n",
    "# Section intro\n",
    "section_2g_intro = \"\"\"Data preparation planning transforms insights from understanding phases into actionable preprocessing decisions. We assess current data state to determine required transformations, verify feature scaling status to inform normalization needs, evaluate target variable encoding requirements for algorithm compatibility, validate train-test separation to prevent data leakage, confirm data completeness to determine imputation necessity, and synthesize findings from sections 2a-2f into prioritized preparation actions for modeling readiness.\"\"\"\n",
    "\n",
    "# Preparation finding 1: Feature scaling status\n",
    "prep_finding_01_label = \"Finding 1: Features Pre-Normalized Requiring No Additional Scaling\"\n",
    "prep_finding_01_text = f\"\"\"Feature Scaling Assessment: All {len(sensor_features)} sensor features exhibit values within the approximate range [{train_df[sensor_features].min().min():.4f}, {train_df[sensor_features].max().max():.4f}], consistent with prior normalization to [-1, 1]. The subject identifier column ranges from 1 to 30 and must be excluded from modeling as it is a categorical identifier not a numeric feature. The pre-normalized state eliminates need for StandardScaler or MinMaxScaler preprocessing, as scale differences between features are already addressed. Models sensitive to feature magnitude (SVM, neural networks, distance-based methods) can proceed directly without additional scaling.\"\"\"\n",
    "\n",
    "# Preparation finding 2: Target encoding requirement\n",
    "prep_finding_02_label = \"Finding 2: Categorical Target Variable Requires Numeric Encoding\"\n",
    "prep_finding_02_text = f\"\"\"Target Variable Encoding: The Activity column contains 6 categorical text labels (LAYING, SITTING, STANDING, WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS) that must be converted to numeric format for algorithm compatibility. Encoding should map each activity to integers 0-5 using LabelEncoder or similar mechanism. This integer encoding is required for classification algorithms expecting numeric targets. The encoding mapping must be preserved for inverse transformation during prediction interpretation and must be applied consistently to both training and test sets.\"\"\"\n",
    "\n",
    "# Preparation finding 3: Train-test separation validation\n",
    "prep_finding_03_label = \"Finding 3: Proper Subject-Based Split Preventing Data Leakage\"\n",
    "prep_finding_03_text = f\"\"\"Train-Test Separation: The training set contains {len(train_df['subject'].unique())} subjects while the test set contains {len(test_df['subject'].unique())} subjects with zero overlap between sets. This subject-based separation ensures evaluation reflects model generalization to new individuals rather than memorization of subject-specific patterns. The split is proper and must be maintained throughout all preprocessing steps. Cross-validation within the training set should also use subject-based stratification where possible to preserve the subject-independent evaluation paradigm.\"\"\"\n",
    "\n",
    "# Preparation finding 4: Data completeness confirmation\n",
    "prep_finding_04_label = \"Finding 4: Complete Dataset With Zero Missing Values\"\n",
    "prep_finding_04_text = f\"\"\"Missing Value Assessment: Both training ({len(train_df)} samples) and test ({len(test_df)} samples) datasets contain zero missing values across all {len(sensor_features)} sensor features and the Activity column. Missing value counts are {train_df.isnull().sum().sum()} for training and {test_df.isnull().sum().sum()} for test sets. This complete data eliminates need for imputation strategies like mean/median filling, KNN imputation, or iterative imputation. Data completeness simplifies preprocessing pipeline and removes uncertainty from imputation method selection.\"\"\"\n",
    "\n",
    "# Preparation finding 5: Multicollinearity and stratification requirements\n",
    "prep_finding_05_label = \"Finding 5: Moderate Imbalance and High Redundancy Guiding Preparation Strategy\"\n",
    "prep_finding_05_text = f\"\"\"Combined Preparation Needs: Section 2b identified {len(top_corr_df)} feature pairs with correlation exceeding 0.8 and {high_corr_count} pairs above 0.95, indicating substantial redundancy motivating dimensionality reduction. Section 2e revealed {imbalance_ratio:.2f}:1 class imbalance ratio requiring stratified sampling. Section 2f quantified {feature_to_sample_ratio:.4f} feature-to-sample ratio creating overfitting risk. These interconnected findings mandate three preparation priorities: (1) stratified cross-validation maintaining class proportions, (2) optional dimensionality reduction via PCA or feature selection to address overfitting, and (3) per-class evaluation metrics to monitor minority class performance.\"\"\"\n",
    "\n",
    "# Section conclusion\n",
    "section_2g_conclusion = f\"\"\"Data preparation assessment reveals features pre-normalized to [-1, 1] requiring no additional scaling, categorical target variable requiring numeric encoding, proper subject-based train-test separation preventing data leakage, complete dataset with zero missing values eliminating imputation needs, and moderate class imbalance ({imbalance_ratio:.2f}:1) plus high feature redundancy ({high_corr_count} pairs > 0.95 correlation) guiding stratification and optional dimensionality reduction strategies. Mandatory actions include target encoding and stratified cross-validation. Recommended actions include validation set creation for hyperparameter tuning. Optional actions include dimensionality reduction if overfitting emerges during modeling.\"\"\"\n",
    "\n",
    "# Generate deterministic UUIDs\n",
    "du_2g_uuid_executor = deterministic_uuid(\"du_2g_preparation:executor:activity\")\n",
    "du_2g_uuid_writer = deterministic_uuid(\"du_2g_preparation:writer:activity\")\n",
    "data_understanding_2g_code_writer = student_a\n",
    "\n",
    "# Build activity triples with intro, preparation findings, and conclusion\n",
    "activity_2g_triples = [\n",
    "    ':du_2g_data_preparation rdf:type prov:Activity .',\n",
    "    ':du_2g_data_preparation sc:isPartOf :data_understanding_phase .',\n",
    "    ':du_2g_data_preparation rdfs:label \"Data Preparation Requirements\" .',\n",
    "    f':du_2g_data_preparation rdfs:comment \"\"\"{section_2g_intro}\"\"\" .',\n",
    "    f':du_2g_data_preparation prov:startedAtTime \"{start_time_2g}\"^^xsd:dateTime .',\n",
    "    f':du_2g_data_preparation prov:endedAtTime \"{end_time_2g}\"^^xsd:dateTime .',\n",
    "    f':du_2g_data_preparation prov:qualifiedAssociation :{du_2g_uuid_writer} .',\n",
    "    f':{du_2g_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{du_2g_uuid_writer} prov:agent :{data_understanding_2g_code_writer} .',\n",
    "    f':{du_2g_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':du_2g_data_preparation prov:qualifiedAssociation :{du_2g_uuid_executor} .',\n",
    "    f':{du_2g_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{du_2g_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{du_2g_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':du_2g_data_preparation prov:used :train_df .',\n",
    "    ':du_2g_data_preparation prov:used :test_df .',\n",
    "    \n",
    "    # Preparation Finding 1: Feature scaling (with detailed analysis)\n",
    "    ':du_2g_prep_01 rdf:type prov:Entity .',\n",
    "    ':du_2g_prep_01 rdf:type sc:PreparationRequirement .',\n",
    "    ':du_2g_prep_01 sc:isPartOf :du_2g_data_preparation .',\n",
    "    f':du_2g_prep_01 rdfs:label \"{prep_finding_01_label}\" .',\n",
    "    f':du_2g_prep_01 rdfs:comment \"\"\"{prep_finding_01_text}\"\"\" .',\n",
    "    ':du_2g_prep_01 prov:wasGeneratedBy :du_2g_data_preparation .',\n",
    "    \n",
    "    # Preparation Finding 2: Target encoding (with detailed analysis)\n",
    "    ':du_2g_prep_02 rdf:type prov:Entity .',\n",
    "    ':du_2g_prep_02 rdf:type sc:PreparationRequirement .',\n",
    "    ':du_2g_prep_02 sc:isPartOf :du_2g_data_preparation .',\n",
    "    f':du_2g_prep_02 rdfs:label \"{prep_finding_02_label}\" .',\n",
    "    f':du_2g_prep_02 rdfs:comment \"\"\"{prep_finding_02_text}\"\"\" .',\n",
    "    ':du_2g_prep_02 prov:wasGeneratedBy :du_2g_data_preparation .',\n",
    "    \n",
    "    # Preparation Finding 3: Train-test separation (with detailed analysis)\n",
    "    ':du_2g_prep_03 rdf:type prov:Entity .',\n",
    "    ':du_2g_prep_03 rdf:type sc:PreparationRequirement .',\n",
    "    ':du_2g_prep_03 sc:isPartOf :du_2g_data_preparation .',\n",
    "    f':du_2g_prep_03 rdfs:label \"{prep_finding_03_label}\" .',\n",
    "    f':du_2g_prep_03 rdfs:comment \"\"\"{prep_finding_03_text}\"\"\" .',\n",
    "    ':du_2g_prep_03 prov:wasGeneratedBy :du_2g_data_preparation .',\n",
    "    \n",
    "    # Preparation Finding 4: Missing values (with detailed analysis)\n",
    "    ':du_2g_prep_04 rdf:type prov:Entity .',\n",
    "    ':du_2g_prep_04 rdf:type sc:PreparationRequirement .',\n",
    "    ':du_2g_prep_04 sc:isPartOf :du_2g_data_preparation .',\n",
    "    f':du_2g_prep_04 rdfs:label \"{prep_finding_04_label}\" .',\n",
    "    f':du_2g_prep_04 rdfs:comment \"\"\"{prep_finding_04_text}\"\"\" .',\n",
    "    ':du_2g_prep_04 prov:wasGeneratedBy :du_2g_data_preparation .',\n",
    "    \n",
    "    # Preparation Finding 5: Combined requirements (with detailed analysis)\n",
    "    ':du_2g_prep_05 rdf:type prov:Entity .',\n",
    "    ':du_2g_prep_05 rdf:type sc:PreparationRequirement .',\n",
    "    ':du_2g_prep_05 sc:isPartOf :du_2g_data_preparation .',\n",
    "    f':du_2g_prep_05 rdfs:label \"{prep_finding_05_label}\" .',\n",
    "    f':du_2g_prep_05 rdfs:comment \"\"\"{prep_finding_05_text}\"\"\" .',\n",
    "    ':du_2g_prep_05 prov:wasGeneratedBy :du_2g_data_preparation .',\n",
    "    \n",
    "    # Overall conclusion\n",
    "    ':du_2g_conclusion rdf:type prov:Entity .',\n",
    "    ':du_2g_conclusion sc:isPartOf :du_2g_data_preparation .',\n",
    "    f':du_2g_conclusion rdfs:label \"Data Preparation Conclusion\" .',\n",
    "    f':du_2g_conclusion rdfs:comment \"\"\"{section_2g_conclusion}\"\"\" .',\n",
    "    ':du_2g_conclusion prov:wasGeneratedBy :du_2g_data_preparation .',\n",
    "]\n",
    "\n",
    "try:\n",
    "    engine.insert(activity_2g_triples, prefixes=prefixes)\n",
    "    print(\"Section 2g complete - Data preparation requirements with 5 findings and comprehensive planning recorded\")\n",
    "    print(\"\\n\")\n",
    "    print(\"Data Understanding Phase Complete (Sections 2a-2g)\")\n",
    "    print(\"All sections documented with consistent provenance patterns and actual data metrics\")\n",
    "   \n",
    "except:\n",
    "    print(\"Note: Could not insert to graph DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16349e3",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d290a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation phase setup: writer (Eman - B), assistant (Sajid - A), executor\n",
    "data_preparation_code_writer = student_b  # Eman (B) - main responsible\n",
    "data_preparation_code_writer_b = student_a  # Sajid (A) - assistant\n",
    "data_preparation_executor = executed_by  # current executor\n",
    "\n",
    "dp_phase_uuid_writer = deterministic_uuid(\"data_preparation_phase:writer:student_b\")\n",
    "dp_phase_uuid_writer_b = deterministic_uuid(\"data_preparation_phase:writer_assistant:student_a\")\n",
    "dp_phase_uuid_executor = deterministic_uuid(\"data_preparation_phase:executor\")\n",
    "\n",
    "data_preparation_phase_activity = [\n",
    "    f':data_preparation_phase rdf:type prov:Activity .',\n",
    "    f':data_preparation_phase rdfs:label \"Data Preparation Phase\" .',\n",
    "    # Writer (main responsible)\n",
    "    f':data_preparation_phase prov:qualifiedAssociation :{dp_phase_uuid_writer} .',\n",
    "    f':{dp_phase_uuid_writer} prov:agent :{data_preparation_code_writer} .',\n",
    "    f':{dp_phase_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dp_phase_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    # Assistant writer\n",
    "    f':data_preparation_phase prov:qualifiedAssociation :{dp_phase_uuid_writer_b} .',\n",
    "    f':{dp_phase_uuid_writer_b} prov:agent :{data_preparation_code_writer_b} .',\n",
    "    f':{dp_phase_uuid_writer_b} rdf:type prov:Association .',\n",
    "    f':{dp_phase_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "    # Executor\n",
    "    f':data_preparation_phase prov:qualifiedAssociation :{dp_phase_uuid_executor} .',\n",
    "    f':{dp_phase_uuid_executor} prov:agent :{data_preparation_executor} .',\n",
    "    f':{dp_phase_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{dp_phase_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(data_preparation_phase_activity, prefixes=prefixes)\n",
    "\n",
    "print(\"Data Preparation phase initialized with writer B (Eman), assistant A (Sajid), and executor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd786a",
   "metadata": {},
   "source": [
    "### 3a) Data Preprocessing Actions\n",
    "\n",
    "Execute necessary preprocessing based on Data Understanding phase findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d076f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a) Execute data preprocessing - Target encoding and feature verification\n",
    "data_preparation_3a_code_writer = student_b\n",
    "start_time_3a = now()\n",
    "\n",
    "# Create copies to preserve original data\n",
    "train_prepared = train_df.copy()\n",
    "test_prepared = test_df.copy()\n",
    "\n",
    "print(\"\\nStep 1: Feature Scaling Verification\")\n",
    "\n",
    "feature_cols = [col for col in train_prepared.columns if col not in ['Activity', 'subject']]\n",
    "\n",
    "orig_min = train_prepared[feature_cols].min().min()\n",
    "orig_max = train_prepared[feature_cols].max().max()\n",
    "\n",
    "print(f\"Feature value range: [{orig_min:.4f}, {orig_max:.4f}]\")\n",
    "print(\"Status: Features already normalized to approximately [-1, 1] - no scaling applied\")\n",
    "\n",
    "\n",
    "print(f\"\\nStep 2: Target Variable Encoding\")\n",
    "# Encode Activity labels to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "train_prepared['Activity_encoded'] = label_encoder.fit_transform(train_prepared['Activity'])\n",
    "test_prepared['Activity_encoded'] = label_encoder.transform(test_prepared['Activity'])\n",
    "\n",
    "# Document the encoding mapping\n",
    "activity_mapping = {label: idx for idx, label in enumerate(label_encoder.classes_)}\n",
    "print(\"Encoding type: Label encoding (categorical, unordered classes)\")\n",
    "print(\"Activity encoding mapping:\")\n",
    "for activity, code in activity_mapping.items():\n",
    "    train_count = (train_prepared['Activity_encoded'] == code).sum()\n",
    "    print(f\"  {activity} -> {code} ({train_count} samples)\")\n",
    "\n",
    "print(f\"\\nStep 3: Verify Data Integrity\")\n",
    "# Check for any issues introduced during preprocessing\n",
    "print(f\"Training set shape: {train_prepared.shape}\")\n",
    "print(f\"Test set shape: {test_prepared.shape}\")\n",
    "print(f\"Missing values in train: {train_prepared.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test: {test_prepared.isnull().sum().sum()}\")\n",
    "\n",
    "# Verify subject separation is maintained\n",
    "train_subjects_after = set(train_prepared['subject'].unique())\n",
    "test_subjects_after = set(test_prepared['subject'].unique())\n",
    "overlap_after = train_subjects_after.intersection(test_subjects_after)\n",
    "print(f\"Subject separation maintained: {len(overlap_after) == 0}\")\n",
    "\n",
    "print(f\"\\nPreprocessing Summary\")\n",
    "print(f\"Original training samples: {len(train_df)}\")\n",
    "print(f\"Prepared training samples: {len(train_prepared)}\")\n",
    "print(f\"Number of feature columns: {len(feature_cols)}\")\n",
    "print(f\"Target classes: {len(activity_mapping)}\")\n",
    "print(f\"No data samples removed - complete dataset preserved\")\n",
    "\n",
    "end_time_3a = now()\n",
    "print(\"\\nData preprocessing completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20b8e8-7d7f-4df5-ba38-62704f020c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a) Record preprocessing activity\n",
    "\n",
    "dp_3a_report = \"\"\"\n",
    "Applied essential data preprocessing to HAR training and test datasets following recommendations from the Data Understanding phase (Section 2g).\n",
    "Verified that all sensor features are already normalized to approximately the range [-1, 1]; therefore, no additional feature scaling was applied.\n",
    "Encoded the categorical Activity target variable from text labels to numeric codes using LabelEncoder while preserving the original class mapping.\n",
    "Verified data integrity after preprocessing, including checks for missing values and preservation of subject-disjoint train-test separation.\n",
    "All samples were retained; no rows were removed during preprocessing.\n",
    "\"\"\"\n",
    "interpretation_3a_text = \"\"\"\n",
    "Data Preprocessing Implementation:\n",
    "Feature Scaling Verification:\n",
    "Inspection of all sensor feature columns confirmed that values are already normalized to approximately the range [-1, 1], consistent with the HAR dataset documentation and findings from Section 2. As a result, no feature scaling or standardization was applied during preprocessing.\n",
    "Target Variable Encoding:\n",
    "The Activity column, containing six categorical class labels (LAYING, SITTING, STANDING, WALKING, WALKING_DOWNSTAIRS, WALKING_UPSTAIRS), was encoded into numeric labels using LabelEncoder. This transformation enables compatibility with machine learning algorithms that require numeric target variables. The encoder was fit on the training data and applied consistently to the test data to ensure reproducibility.\n",
    "Data Integrity Verification:\n",
    "No samples were removed during preprocessing. The training set contains 7352 samples and the test set 2947 samples, with zero missing values in both. The subject-based train-test separation remains intact, with no overlap between training and test subjects, ensuring a realistic evaluation of model generalization.\n",
    "\"\"\"\n",
    "decision_3a_text = \"\"\"\n",
    "Preprocessing Decisions and Rationale:\n",
    "No feature scaling was applied because all sensor features are already normalized, and further scaling would be redundant without providing additional benefit.\n",
    "Label encoding was selected for the Activity target variable because it represents a multi-class categorical outcome without ordinal relationships. Integer encoding is sufficient and appropriate for target variables in classification tasks.\n",
    "No samples were removed during preprocessing. Outliers identified during Data Understanding represent valid extreme movements rather than measurement errors and were therefore retained.\n",
    "Dimensionality reduction and feature selection were not applied at this stage. Although extensive feature correlation exists, retaining the full feature set establishes a baseline for subsequent modeling. Techniques such as PCA or regularization may be considered later if overfitting or computational constraints arise.\n",
    "The subject identifier was retained in the prepared datasets for potential analysis but must be excluded from feature inputs during model training to prevent information leakage.\n",
    "\"\"\"\n",
    "# Generate deterministic UUIDs\n",
    "dp_3a_uuid_executor = deterministic_uuid(\"dp_3a_preprocessing:executor:activity\")\n",
    "dp_3a_uuid_writer = deterministic_uuid(\"dp_3a_preprocessing:writer:activity\")\n",
    "\n",
    "# Activity triples\n",
    "activity_3a_triples = [\n",
    "    ':dp_3a_preprocessing rdf:type prov:Activity .',\n",
    "    ':dp_3a_preprocessing sc:isPartOf :data_preparation_phase .',\n",
    "    ':dp_3a_preprocessing rdfs:comment \"Section 3a - Data Preprocessing Execution\" .',\n",
    "    f':dp_3a_preprocessing rdfs:comment \"\"\"{dp_3a_report}\"\"\" .',\n",
    "    f':dp_3a_preprocessing prov:startedAtTime \"{start_time_3a}\"^^xsd:dateTime .',\n",
    "    f':dp_3a_preprocessing prov:endedAtTime \"{end_time_3a}\"^^xsd:dateTime .',\n",
    "    # Writer association\n",
    "    f':dp_3a_preprocessing prov:qualifiedAssociation :{dp_3a_uuid_writer} .',\n",
    "    f':{dp_3a_uuid_writer} prov:agent :{data_preparation_3a_code_writer} .',\n",
    "    f':{dp_3a_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dp_3a_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    # Executor association\n",
    "    f':dp_3a_preprocessing prov:qualifiedAssociation :{dp_3a_uuid_executor} .',\n",
    "    f':{dp_3a_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{dp_3a_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{dp_3a_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    # Input entities\n",
    "    ':dp_3a_preprocessing prov:used :train_df .',\n",
    "    ':dp_3a_preprocessing prov:used :test_df .',\n",
    "    # Output entities\n",
    "    ':train_prepared rdf:type prov:Entity .',\n",
    "    ':train_prepared prov:wasGeneratedBy :dp_3a_preprocessing .',\n",
    "    ':train_prepared prov:wasDerivedFrom :train_df .',\n",
    "    ':test_prepared rdf:type prov:Entity .',\n",
    "    ':test_prepared prov:wasGeneratedBy :dp_3a_preprocessing .',\n",
    "    ':test_prepared prov:wasDerivedFrom :test_df .',\n",
    "]\n",
    "\n",
    "engine.insert(activity_3a_triples, prefixes=prefixes)\n",
    "\n",
    "# Interpretation triples\n",
    "interpretation_3a_triples = [\n",
    "    ':dp_3a_interpretation rdf:type prov:Entity .',\n",
    "    ':dp_3a_interpretation rdfs:label \"Section 3a - Preprocessing Interpretation\" .',\n",
    "    f':dp_3a_interpretation rdfs:comment \"\"\"{interpretation_3a_text}\"\"\" .',\n",
    "    ':dp_3a_interpretation prov:wasGeneratedBy :dp_3a_preprocessing .',\n",
    "]\n",
    "\n",
    "engine.insert(interpretation_3a_triples, prefixes=prefixes)\n",
    "\n",
    "# Decision triples\n",
    "decision_3a_triples = [\n",
    "    ':dp_3a_decision rdf:type prov:Entity .',\n",
    "    ':dp_3a_decision rdfs:label \"Section 3a - Preprocessing Decisions\" .',\n",
    "    f':dp_3a_decision rdfs:comment \"\"\"{decision_3a_text}\"\"\" .',\n",
    "    ':dp_3a_decision prov:wasGeneratedBy :dp_3a_preprocessing .',\n",
    "]\n",
    "\n",
    "engine.insert(decision_3a_triples, prefixes=prefixes)\n",
    "\n",
    "print(\"Section 3a complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b09db",
   "metadata": {},
   "source": [
    "### 3b) Preprocessing Steps Considered But Not Applied\n",
    "\n",
    "Document preprocessing techniques considered but not implemented and the reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447e864-ca19-41de-b61a-e2e73863ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3b – Preprocessing Steps Considered but Not Applied\n",
    "dp_3b_full_text = \"\"\"\n",
    "This section documents preprocessing techniques that were considered during the Data Preparation phase\n",
    "but deliberately not applied, based on the findings of the Data Understanding phase and the characteristics\n",
    "of the Human Activity Recognition (HAR) dataset.\n",
    "\n",
    "Several common data cleansing steps were evaluated. Missing value imputation was not applied because the\n",
    "dataset was found to be complete, with no missing values in either the training or test partitions.\n",
    "Duplicate record removal was also not necessary, as no duplicate observations were detected.\n",
    "\n",
    "Feature scaling and normalization techniques such as standardization or min–max scaling were considered.\n",
    "However, all sensor-derived features in the HAR dataset are already normalized to an approximate range of\n",
    "[-1, 1] as part of the original feature engineering process. Applying additional scaling would therefore\n",
    "be redundant and could potentially distort the physical interpretation of the signals.\n",
    "\n",
    "Outlier detection and removal were examined but ultimately not applied. Although certain features exhibit\n",
    "skewed distributions and extreme values, these values correspond to valid human movements rather than\n",
    "measurement errors. Removing such observations could bias the model against rare but legitimate activity\n",
    "patterns. Furthermore, many classification algorithms commonly used for this task are robust to outliers.\n",
    "\n",
    "Dimensionality reduction techniques such as Principal Component Analysis (PCA) were considered due to the\n",
    "high number of features and strong correlations among them. However, PCA was not applied at this stage in\n",
    "order to preserve feature interpretability and to establish a baseline using the full feature set.\n",
    "Dimensionality reduction and feature selection are deferred to the modeling phase if overfitting or\n",
    "computational inefficiency becomes an issue.\n",
    "\n",
    "Class rebalancing techniques, including oversampling or undersampling, were also evaluated. The observed\n",
    "class imbalance was moderate, with an imbalance ratio of approximately 1.43. This level of imbalance does\n",
    "not justify altering the original data distribution. Instead, stratified sampling strategies are planned\n",
    "for model evaluation to ensure fair representation of all activity classes.\n",
    "\n",
    "In summary, preprocessing steps were intentionally kept minimal. Only transformations strictly required\n",
    "for model compatibility were applied in Section 3a, while other preprocessing techniques were deferred or\n",
    "rejected to avoid unnecessary data manipulation and to maintain alignment with the original data\n",
    "characteristics.\n",
    "\"\"\"\n",
    "\n",
    "dp_3b_uuid_writer = deterministic_uuid(\"dp_3b_not_applied:writer:student_b\")\n",
    "dp_3b_uuid_executor = deterministic_uuid(\"dp_3b_not_applied:executor\")\n",
    "start_time_3b = now()\n",
    "end_time_3b = now()\n",
    "\n",
    "dp_3b_activity = [\n",
    "    ':dp_3b_not_applied rdf:type prov:Activity .',\n",
    "    ':dp_3b_not_applied sc:isPartOf :data_preparation_phase .',\n",
    "    ':dp_3b_not_applied rdfs:label \"Section 3b - Preprocessing steps considered but not applied\" .',\n",
    "    f':dp_3b_not_applied rdfs:comment \"\"\"{dp_3b_full_text}\"\"\" .',\n",
    "    f':dp_3b_not_applied prov:startedAtTime \"{start_time_3b}\"^^xsd:dateTime .',\n",
    "    f':dp_3b_not_applied prov:endedAtTime \"{end_time_3b}\"^^xsd:dateTime .',\n",
    "    f':dp_3b_not_applied prov:qualifiedAssociation :{dp_3b_uuid_writer} .',\n",
    "    f':{dp_3b_uuid_writer} prov:agent :{data_preparation_code_writer} .',\n",
    "    f':{dp_3b_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dp_3b_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':dp_3b_not_applied prov:qualifiedAssociation :{dp_3b_uuid_executor} .',\n",
    "    f':{dp_3b_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{dp_3b_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{dp_3b_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':dp_3b_not_applied prov:used :train_df .',\n",
    "    ':dp_3b_not_applied prov:used :test_df .',\n",
    "    ':dp_3b_report rdf:type prov:Entity .',\n",
    "    ':dp_3b_report rdf:type sc:Report .',\n",
    "    ':dp_3b_report sc:isPartOf :dp_3b_not_applied .',\n",
    "    ':dp_3b_report prov:wasGeneratedBy :dp_3b_not_applied .',\n",
    "    f':dp_3b_report rdfs:comment \"\"\"{dp_3b_full_text}\"\"\" .',\n",
    "]\n",
    "\n",
    "engine.insert(dp_3b_activity, prefixes=prefixes)\n",
    "\n",
    "print(\"Section 3b complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e84bc5",
   "metadata": {},
   "source": [
    "### 3c) Analysis of Derived Attributes Potential\n",
    "\n",
    "Analyze options for feature engineering and derived attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0036428-fcdf-4ee8-ad52-424f95024cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3c – External Data Sources Analysis\n",
    "dp_3c_full_text = \"\"\"\n",
    "Analyzed opportunities for feature engineering and derived attribute creation for HAR activity classification.\n",
    "Examined potential for temporal features, cross-axis interactions, statistical aggregations, domain-specific biomechanical indicators, and activity transition features.\n",
    "Evaluated feasibility and expected benefit of each derived attribute category.\n",
    "Provided recommendations on which derived features offer highest potential value if baseline model performance is insufficient.\n",
    "\n",
    "Derived Attributes Analysis and Potential:\n",
    "\n",
    "1. Temporal Pattern Features (Moderate Potential):\n",
    "The current dataset provides features calculated over 2.56 second windows but does not explicitly capture longer-term temporal patterns. Derived features could include rolling statistics over multiple consecutive windows such as 5-second or 10-second moving averages of acceleration. These would capture sustained movement patterns like continuous walking versus brief movements. However implementing this requires access to the sequential ordering of samples which may not be preserved in the provided train-test split. Potential benefit is moderate because temporal context could help distinguish similar activities.\n",
    "\n",
    "2. Cross-Axis Interaction Features (Low to Moderate Potential):\n",
    "Current features include individual axis measurements (X, Y, Z) and magnitude calculations. Additional derived features could include ratios between axes like X-to-Y acceleration ratio or cross-products indicating movement direction changes. For example the ratio of vertical to horizontal acceleration might distinguish WALKING from WALKING_UPSTAIRS. However the dataset already includes 561 engineered features many of which capture multi-axis relationships through correlation and angle calculations. Adding more interaction terms risks increasing multicollinearity without substantial information gain.\n",
    "\n",
    "3. Activity-Specific Biomechanical Indicators (Moderate to High Potential):\n",
    "Domain knowledge from biomechanics could inform derived features. Step frequency could be estimated from periodicity in acceleration signals to distinguish walking speeds. Tilt angles could separate vertical activities (standing, sitting) from horizontal (laying). Energy expenditure proxies combining acceleration magnitude and frequency could distinguish static from dynamic activities. These domain-informed features could provide interpretable signals that models can learn more efficiently than discovering patterns in raw signals. Benefit depends on whether such patterns are already captured in the 561 existing features.\n",
    "\n",
    "4. Statistical Aggregations Across Feature Groups (Low Potential):\n",
    "Features could be derived by aggregating statistics across related feature families. For example averaging all time-domain features or all frequency-domain features creates meta-features summarizing signal domains. However such aggregations would lose granular information and the existing features already provide comprehensive statistical summaries (mean, std, mad, max, min, energy, entropy) across signal types. Additional aggregation offers minimal benefit.\n",
    "\n",
    "5. Change and Gradient Features (Moderate Potential):\n",
    "The dataset includes jerk signals representing first derivatives of acceleration. Second-order derivatives (jerk rate) could capture movement smoothness changes. However computing higher-order derivatives amplifies noise in sensor measurements. Additionally distinguishing activities may not require such detailed movement dynamics. Potential benefit is uncertain and depends on whether activity transitions show characteristic jerk patterns not captured by existing features.\n",
    "\n",
    "6. Frequency Band Energy Ratios (Low to Moderate Potential):\n",
    "The frequency domain features include FFT coefficients but could be extended with energy distribution across specific frequency bands. For example the ratio of low frequency (0-5 Hz) to high frequency (5-20 Hz) energy might distinguish smooth walking from erratic movements. However the dataset already includes frequency-based features like spectral entropy and energy which summarize frequency content. Deriving explicit band ratios may offer marginal benefit.\n",
    "\n",
    "7. Activity Transition Indicators (High Potential If Feasible):\n",
    "Samples at boundaries between different activities may show characteristic patterns like mixed signals from both activities. Derived binary features indicating whether a sample is likely transitional could improve classification by allowing models to treat boundary cases differently. However identifying transitions requires temporal ordering and activity labels for adjacent samples which may not be available in the provided format. If feasible transition indicators could substantially improve boundary sample classification.\n",
    "\n",
    "8. Subject-Specific Normalization (Not Recommended):\n",
    "Features could be normalized relative to each subject's personal movement baselines for example expressing acceleration as deviation from subject mean. This would account for individual differences in movement style. However this creates subject-dependent features that do not generalize to new subjects. Since the goal is cross-subject generalization not within-subject classification subject-specific normalization is counterproductive.\n",
    "\n",
    "Overall Assessment:\n",
    "The potential for derived attributes is moderate. The dataset already contains 561 engineered features capturing comprehensive temporal and frequency characteristics of sensor signals. Most obvious derived features are likely already represented. The highest value opportunities are domain-informed biomechanical indicators and activity transition features but these require either expert biomechanics knowledge or access to sequential sample ordering. Given the extensive existing feature set and risk of overfitting from adding redundant features the recommended approach is establishing baseline model performance first then selectively testing high-potential derived features only if performance gaps exist.\n",
    "\n",
    "Derived Attributes Recommendations:\n",
    "\n",
    "Immediate Application (Not Recommended):\n",
    "Do not create derived attributes before establishing baseline model performance. With 561 existing features and only 7352 training samples the risk of overfitting from additional features is substantial. Starting with the comprehensive feature set provided by the dataset creators establishes a performance ceiling for comparison.\n",
    "\n",
    "High Priority If Baseline Insufficient:\n",
    "If baseline models fail to achieve acceptable performance (e.g., accuracy below 85 percent or poor minority class recall), prioritize these derived features:\n",
    "\n",
    "1. Activity-specific biomechanical indicators calculated with domain expertise input such as step frequency estimation or tilt angle classification.\n",
    "2. Frequency band energy ratios specifically targeting frequency ranges known to distinguish walking from static activities (typically 1-3 Hz for walking cadence).\n",
    "\n",
    "Medium Priority For Performance Optimization:\n",
    "If baseline performance is adequate but specific activity pairs show confusion (e.g., SITTING versus STANDING), consider targeted derived features:\n",
    "\n",
    "1. Cross-axis ratios focusing on the specific axes that distinguish confused activity pairs.\n",
    "2. Temporal smoothing over multiple windows if sequential ordering is available.\n",
    "\n",
    "Low Priority Optional Testing:\n",
    "If computational resources permit and overfitting is not observed:\n",
    "\n",
    "1. Second-order gradient features (jerk rate) to capture movement smoothness.\n",
    "2. Higher-order statistical moments (skewness, kurtosis) across feature groups.\n",
    "\n",
    "Not Recommended:\n",
    "Subject-specific normalizations that reduce generalization to new subjects.\n",
    "Arbitrary interaction terms without domain knowledge justification.\n",
    "Aggregations that lose information from existing granular features.\n",
    "\n",
    "Implementation Strategy:\n",
    "If derived features are created implement them as separate feature sets not combined initially with all 561 existing features. Test models with (A) existing features only, (B) derived features only, (C) selected combination. This isolates derived feature contribution and prevents indiscriminately expanding feature dimensionality.\n",
    "\n",
    "The conservative approach of deferring feature engineering until baseline results are available aligns with best practices. Many successful activity recognition models achieve strong performance with the standard feature set provided. Feature engineering should be driven by identified performance gaps not speculation.\n",
    "\"\"\"\n",
    "\n",
    "dp_3c_uuid_writer = deterministic_uuid(\"dp_3c_derived_attributes:writer:student_b\")\n",
    "dp_3c_uuid_executor = deterministic_uuid(\"dp_3c_derived_attributes:executor\")\n",
    "start_time_3c = now()\n",
    "end_time_3c = now()\n",
    "\n",
    "dp_3c_activity = [\n",
    "    ':dp_3c_derived_attributes rdf:type prov:Activity .',\n",
    "    ':dp_3c_derived_attributes sc:isPartOf :data_preparation_phase .',\n",
    "    ':dp_3c_derived_attributes rdfs:label \"Section 3c - Derived attributes analysis\" .',\n",
    "    f':dp_3c_derived_attributes rdfs:comment \"\"\"{dp_3c_full_text}\"\"\" .',\n",
    "    f':dp_3c_derived_attributes prov:startedAtTime \"{start_time_3c}\"^^xsd:dateTime .',\n",
    "    f':dp_3c_derived_attributes prov:endedAtTime \"{end_time_3c}\"^^xsd:dateTime .',\n",
    "    f':dp_3c_derived_attributes prov:qualifiedAssociation :{dp_3c_uuid_writer} .',\n",
    "    f':{dp_3c_uuid_writer} prov:agent :{data_preparation_code_writer} .',\n",
    "    f':{dp_3c_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dp_3c_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':dp_3c_derived_attributes prov:qualifiedAssociation :{dp_3c_uuid_executor} .',\n",
    "    f':{dp_3c_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{dp_3c_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{dp_3c_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':dp_3c_derived_attributes prov:used :train_df .',\n",
    "    ':dp_3c_derived_attributes prov:used :test_df .',\n",
    "    ':dp_3c_report rdf:type prov:Entity .',\n",
    "    ':dp_3c_report rdf:type sc:Report .',\n",
    "    ':dp_3c_report sc:isPartOf :dp_3c_derived_attributes .',\n",
    "    ':dp_3c_report prov:wasGeneratedBy :dp_3c_derived_attributes .',\n",
    "    f':dp_3c_report rdfs:comment \"\"\"{dp_3c_full_text}\"\"\" .',\n",
    "]\n",
    "\n",
    "engine.insert(dp_3c_activity, prefixes=prefixes)\n",
    "\n",
    "print(\"Section 3c complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5935c91",
   "metadata": {},
   "source": [
    "### 3d) Analysis of External Data Sources Potential\n",
    "\n",
    "Examine opportunities to enhance the dataset with external information sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a343187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d) Analyze potential for external data sources\n",
    "\n",
    "dp_3d_report = \"\"\"\n",
    "Evaluated possibilities for augmenting the HAR dataset with external information sources.\n",
    "Examined five categories of external data: demographic metadata, contextual environmental information, additional sensor streams, activity context metadata, and reference biomechanical databases.\n",
    "Assessed feasibility, privacy implications, data collection requirements, and expected performance benefit for each external data category.\n",
    "Provided recommendations on which external sources offer practical value versus those with prohibitive collection or privacy barriers.\n",
    "\n",
    "External Data Sources Analysis:\n",
    "\n",
    "1. Demographic and Anthropometric Metadata (Moderate Potential, Low Feasibility):\n",
    "Participant characteristics such as age, height, weight, fitness level, and gait patterns vary across individuals and influence movement signatures captured by wearable sensors. For example taller individuals may show different stride frequencies during walking. Including demographic features as additional input variables could help models account for inter-subject variability. However this data was not collected as part of the HAR dataset and cannot be retroactively obtained for the existing 30 subjects. Privacy concerns also limit demographic data collection in real-world deployments. Expected benefit is moderate because models must generalize across diverse populations without relying on personal metadata. Collecting this data for future studies requires informed consent and secure handling.\n",
    "\n",
    "2. Environmental and Contextual Information (Low to Moderate Potential, Low Feasibility):\n",
    "Activity patterns may differ based on environmental factors such as terrain type (flat versus inclined surfaces), footwear, temperature, or time of day. For example walking on a treadmill generates different accelerometer patterns than outdoor walking on uneven ground. External data sources like GPS for location, barometric pressure for altitude changes, or temperature sensors could provide context. However the original HAR experiment was controlled to minimize environmental variations and such data was not recorded. Retroactively inferring environmental conditions is not feasible. Expected benefit is low because the controlled experimental conditions already reduced environmental variability. Future studies could include environmental sensors but this adds data collection complexity.\n",
    "\n",
    "3. Additional Sensor Modalities (High Potential, Low Feasibility for Current Dataset):\n",
    "The HAR dataset uses accelerometer and gyroscope measurements from a waist-mounted smartphone. Additional sensors could include magnetometer for absolute orientation, barometer for elevation changes, GPS for movement distance, heart rate monitors for exertion level, or electromyography (EMG) for muscle activation. Multi-modal sensor fusion often improves activity recognition by providing complementary information. For example heart rate distinguishes vigorous walking from slow walking. However these sensors were not part of the original data collection protocol and cannot be added retrospectively. Expected benefit is high for future data collection but not applicable to the current dataset. Incorporating additional sensors requires specialized equipment and increases participant burden.\n",
    "\n",
    "4. Activity Context and Annotation Metadata (Moderate Potential, Low Feasibility):\n",
    "Contextual information about the setting of each activity could improve classification. For example knowing whether WALKING occurred indoors versus outdoors or whether SITTING was in a car versus at a desk could refine activity definitions. Participant-provided annotations about perceived difficulty or naturalness of movements could identify samples where controlled experimental conditions differed from natural behavior. However such metadata was not collected during the original experiment and cannot be reconstructed. Expected benefit is moderate for distinguishing activity subcategories but requires prospective data collection with detailed annotation protocols. Retrospective annotation by reviewing sensor data alone is unreliable.\n",
    "\n",
    "5. Reference Biomechanical Databases (Low Potential, Moderate Feasibility):\n",
    "External databases of typical human movement patterns from biomechanics research could inform feature engineering or provide normative comparisons. For example published literature on normal walking cadence (typically 100 to 120 steps per minute) could help identify outlier samples. Gait analysis databases with reference ranges for acceleration magnitudes during various activities could contextualize the HAR measurements. However the benefit is indirect rather than providing additional features for model input. Reference data helps interpret features and validate sensor measurements but does not directly improve classification. This information is moderately feasible to obtain from scientific literature but offers limited practical value for model performance.\n",
    "\n",
    "6. Temporal and Usage Context from Device Logs (Low Potential, Privacy and Feasibility Concerns):\n",
    "Smartphone usage logs such as screen state, app activity, or call records could provide indirect activity indicators. For example active screen time suggests sitting or standing rather than vigorous movement. Location history from mapping apps could infer movement patterns. However this data was not part of the HAR experiment and involves significant privacy concerns. Accessing personal device logs requires explicit consent and secure data handling. Expected benefit is low because such indirect signals are noisier than direct accelerometer measurements and may not generalize across users. Additionally relying on device logs makes the model dependent on software that varies across devices.\n",
    "\n",
    "Overall Assessment:\n",
    "The potential for external data sources is limited for the current HAR dataset because necessary data was not collected during the original experiment and cannot be obtained retrospectively. Most external data sources face feasibility barriers (not collected, privacy restrictions) or offer indirect benefits (reference databases). The highest potential external data would be additional sensor modalities (magnetometer, heart rate) but this requires prospective data collection in future studies. For the current analysis the provided accelerometer and gyroscope features represent the complete available information. External data cannot augment this dataset but could inform future HAR study designs.\n",
    "\n",
    "External Data Source Recommendations:\n",
    "\n",
    "For Current HAR Dataset Analysis (Not Applicable):\n",
    "No external data sources can be integrated into the current dataset. The experiment concluded and participants are no longer available for additional data collection. Demographic information, environmental context, and additional sensors were not part of the original protocol. The analysis must proceed with the provided 561 features derived from accelerometer and gyroscope measurements without external augmentation.\n",
    "\n",
    "For Future HAR Data Collection Studies (High Priority):\n",
    "If designing a new activity recognition study these external data sources offer substantial value:\n",
    "\n",
    "1. Additional embedded smartphone sensors already available in modern devices: magnetometer for absolute heading, barometer for elevation, ambient light sensor for indoor versus outdoor detection. These require no additional hardware and minimal participant burden.\n",
    "\n",
    "2. Heart rate data from widely-adopted wearable devices like fitness trackers. Provides exertion level context that distinguishes activity intensity. Requires participants to wear additional device but technology is mainstream and unobtrusive.\n",
    "\n",
    "3. GPS for outdoor movement tracking when privacy-preserving location quantization is applied. Helps distinguish similar activities performed in different settings. Requires careful privacy protection and informed consent.\n",
    "\n",
    "For Future Studies (Medium Priority):\n",
    "Demographic metadata collection with appropriate consent and privacy safeguards. Age, height, weight, and fitness level as basic participant characteristics. Helps analyze whether models generalize across demographic groups. Requires institutional review board approval and secure data storage.\n",
    "\n",
    "For Future Studies (Low Priority Unless Specific Research Goals):\n",
    "Environmental condition logging (temperature, terrain type, footwear) through manual participant annotation. Useful for studying context-dependent activity patterns but increases annotation burden. Only worthwhile if research goals specifically target contextual variation.\n",
    "\n",
    "Reference biomechanical databases (Moderate Utility):\n",
    "Consult published literature on human movement biomechanics when interpreting features and validating sensor measurements. For example expected acceleration ranges during walking or frequency characteristics of different gaits. This does not add data to the dataset but informs feature engineering and outlier detection. Scientific literature on gait analysis and activity recognition provides established norms.\n",
    "\n",
    "Not Recommended:\n",
    "Device usage logs (app activity, screen state) due to privacy concerns and weak signal reliability.\n",
    "Participant-provided real-time annotations of perceived difficulty due to unreliable subjective reporting and interruption of natural activity.\n",
    "Video recordings due to severe privacy implications and impracticality in real-world deployment.\n",
    "\n",
    "Practical Consideration:\n",
    "The current HAR dataset demonstrates that comprehensive activity recognition is achievable with only accelerometer and gyroscope data from a single body-worn device. Published results using this dataset achieve accuracy above 90 percent with standard machine learning methods. This indicates that external data sources, while potentially beneficial, are not necessary for strong performance. The marginal improvement from additional sensors must be weighed against increased data collection complexity, privacy risks, and reduced deployability. For most practical applications the existing sensor suite is sufficient.\n",
    "\n",
    "Implementation Strategy for Current Analysis:\n",
    "Proceed with modeling using the provided features without external data augmentation. Establish baseline performance to quantify the ceiling achievable with accelerometer and gyroscope data alone. If performance gaps exist investigate whether those gaps are addressable through better modeling approaches or genuinely require additional information. Only if substantial accuracy limitations are proven despite exhaustive modeling efforts would future data collection with external sources be justified.\n",
    "\"\"\"\n",
    "\n",
    "dp_3d_uuid_writer = deterministic_uuid(\"dp_3d_external_sources:writer:student_b\")\n",
    "dp_3d_uuid_executor = deterministic_uuid(\"dp_3d_external_sources:executor\")\n",
    "start_time_3d = now()\n",
    "end_time_3d = now()\n",
    "\n",
    "dp_3d_activity = [\n",
    "    ':dp_3d_external_sources rdf:type prov:Activity .',\n",
    "    ':dp_3d_external_sources sc:isPartOf :data_preparation_phase .',\n",
    "    ':dp_3d_external_sources rdfs:label \"Section 3d - External data sources analysis\" .',\n",
    "    f':dp_3d_external_sources rdfs:comment \"\"\"{dp_3d_report}\"\"\" .',\n",
    "    f':dp_3d_external_sources prov:startedAtTime \"{start_time_3d}\"^^xsd:dateTime .',\n",
    "    f':dp_3d_external_sources prov:endedAtTime \"{end_time_3d}\"^^xsd:dateTime .',\n",
    "    f':dp_3d_external_sources prov:qualifiedAssociation :{dp_3d_uuid_writer} .',\n",
    "    f':{dp_3d_uuid_writer} prov:agent :{data_preparation_code_writer} .',\n",
    "    f':{dp_3d_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dp_3d_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':dp_3d_external_sources prov:qualifiedAssociation :{dp_3d_uuid_executor} .',\n",
    "    f':{dp_3d_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{dp_3d_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{dp_3d_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':dp_3d_external_sources prov:used :train_df .',\n",
    "    ':dp_3d_external_sources prov:used :test_df .',\n",
    "    ':dp_3d_report_entity rdf:type prov:Entity .',\n",
    "    ':dp_3d_report_entity rdf:type sc:Report .',\n",
    "    ':dp_3d_report_entity sc:isPartOf :dp_3d_external_sources .',\n",
    "    ':dp_3d_report_entity prov:wasGeneratedBy :dp_3d_external_sources .',\n",
    "    f':dp_3d_report_entity rdfs:comment \"\"\"{dp_3d_report}\"\"\" .',\n",
    "]\n",
    "\n",
    "engine.insert(dp_3d_activity, prefixes=prefixes)\n",
    "\n",
    "print(\"Section 3d complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2334cf0",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb93dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Modeling Phase\n",
    "\n",
    "modeling_phase_executor = [\n",
    "f':modeling_phase rdf:type prov:Activity .',\n",
    "f':modeling_phase rdfs:label \"Modeling Phase\" .', \n",
    "]\n",
    "engine.insert(modeling_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b30291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Responsible: Student A (Student B assisting)\n",
    "modeling_code_writer = student_a\n",
    "modeling_code_writer_b = student_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21680a49",
   "metadata": {},
   "source": [
    "### 4a) Algorithm Selection and Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a) Algorithm Selection and Justification\n",
    "\n",
    "#############################################\n",
    "# Documentation 4a: Algorithm Selection\n",
    "#############################################\n",
    "\n",
    "algorithm_selection_comment = \"\"\"\n",
    "Algorithm Selection for HAR Activity Classification\n",
    "\n",
    "We considered several algorithms for this classification task. The main challenge with this dataset is that we have 561 features and many of them are highly correlated - from section 2b we found 23,058 feature pairs with correlation above 0.8. This is quite significant and affects which algorithms will work well.\n",
    "\n",
    "Naive Bayes was quickly ruled out because it assumes features are independent, which obviously doesn't hold here with so much correlation. KNN also seemed problematic because distance-based methods don't perform well in high dimensional spaces, plus the correlations would make the distance metrics less meaningful.\n",
    "\n",
    "We thought about SVM and neural networks but both require careful feature scaling and have many hyperparameters to tune. For SVM we'd need to choose kernel type, C value, gamma etc. Neural networks are even more complex with layers, units, activation functions, learning rates and so on. It would be difficult to justify all these choices systematically, and we wanted something simpler for the baseline approach.\n",
    "\n",
    "This led us to tree-based methods. Random Forest and Gradient Boosting were both good candidates. Gradient Boosting can give better accuracy but needs more tuning (learning rate, number of trees, subsampling ratios). Random Forest is more straightforward - it builds independent trees with random feature subsets, which actually helps with our multicollinearity problem since each tree only sees a subset of features.\n",
    "\n",
    "We selected Random Forest as the primary algorithm for several reasons. First, it handles the high dimensional feature space naturally without needing dimensionality reduction. The feature subsampling in each tree helps deal with the 23,058 correlated pairs because not all correlated features appear in every tree. Second, since our features are already normalized to [-1, 1] range, we don't need any additional scaling which keeps the pipeline simple. Third, Random Forest works reasonably well with the moderate class imbalance we have (ratio of 1.43). Finally, it provides feature importance scores which could give us insight into which sensor measurements are most useful for distinguishing activities.\n",
    "\n",
    "The hyperparameter tuning for Random Forest is also more intuitive compared to other methods. Parameters like max_depth and n_estimators have obvious interpretations and we can justify the ranges we choose to explore. Based on similar activity recognition studies, Random Forest typically achieves over 90% accuracy on HAR datasets when tuned properly, so it should provide a solid baseline for this task.\n",
    "\"\"\"\n",
    "\n",
    "# Deterministic UUIDs\n",
    "mod_4a_uuid_writer = deterministic_uuid(\"modeling_4a_algorithm_selection:writer:student_a\")\n",
    "mod_4a_uuid_writer_b = deterministic_uuid(\"modeling_4a_algorithm_selection:writer:student_b\")\n",
    "mod_4a_uuid_executor = deterministic_uuid(\"modeling_4a_algorithm_selection:executor\")\n",
    "\n",
    "# Record algorithm selection activity\n",
    "algorithm_selection_activity = [\n",
    "    ':mod_4a_algorithm_selection rdf:type prov:Activity .',\n",
    "    ':mod_4a_algorithm_selection sc:isPartOf :modeling_phase .',\n",
    "    ':mod_4a_algorithm_selection rdfs:label \"4a) Algorithm Selection and Justification\" .',\n",
    "    # Writer A association\n",
    "    f':mod_4a_algorithm_selection prov:qualifiedAssociation :{mod_4a_uuid_writer} .',\n",
    "    f':{mod_4a_uuid_writer} prov:agent :{modeling_code_writer} .',\n",
    "    f':{mod_4a_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{mod_4a_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    # Writer B association\n",
    "    f':mod_4a_algorithm_selection prov:qualifiedAssociation :{mod_4a_uuid_writer_b} .',\n",
    "    f':{mod_4a_uuid_writer_b} prov:agent :{modeling_code_writer_b} .',\n",
    "    f':{mod_4a_uuid_writer_b} rdf:type prov:Association .',\n",
    "    f':{mod_4a_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "    # Executor association\n",
    "    f':mod_4a_algorithm_selection prov:qualifiedAssociation :{mod_4a_uuid_executor} .',\n",
    "    f':{mod_4a_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{mod_4a_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{mod_4a_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    # Documentation entity\n",
    "    ':mod_4a_algorithm_selection_doc rdf:type prov:Entity .',\n",
    "    ':mod_4a_algorithm_selection_doc rdfs:label \"4a Algorithm Selection Decision\" .',\n",
    "    f':mod_4a_algorithm_selection_doc rdfs:comment \"\"\"{escape_rdf_literal(algorithm_selection_comment)}\"\"\" .',\n",
    "    ':mod_4a_algorithm_selection_doc prov:wasGeneratedBy :mod_4a_algorithm_selection .',\n",
    "    # Selected algorithm entity\n",
    "    ':random_forest_classifier rdf:type mlso:Algorithm .',\n",
    "    ':random_forest_classifier rdfs:label \"Random Forest Classifier\" .',\n",
    "    ':random_forest_classifier rdfs:comment \"Ensemble of decision trees for activity classification\" .',\n",
    "    ':random_forest_classifier prov:wasGeneratedBy :mod_4a_algorithm_selection .',\n",
    "]\n",
    "\n",
    "engine.insert(algorithm_selection_activity, prefixes=prefixes)\n",
    "\n",
    "\n",
    "print(\"SECTION 4a: ALGORITHM SELECTION\")\n",
    "\n",
    "print(\"\\nAlgorithm Selected: RANDOM FOREST CLASSIFIER\")\n",
    "print(\"\\nMain Reasons of selection:\")\n",
    "print(\"  1. Handles multicollinearity (23,058 correlated feature pairs)\")\n",
    "print(\"  2. Processes high-dimensional data (561 features)\")\n",
    "print(\"  3. No feature scaling required (already normalized)\")\n",
    "print(\"  4. Interpretable via feature importance\")\n",
    "print(\"  5. Suitable for moderate class imbalance (1.43 ratio)\")\n",
    "print(\"  6. Shown hyperparameter tuning rationale\")\n",
    "print(\"\\nStatus: Algorithm selection documented and logged to provenance graph\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b696a9c5",
   "metadata": {},
   "source": [
    "### 4b) Hyperparameter Identification and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef613f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b) Hyperparameter Identification and Selection\n",
    "\n",
    "# Available RF hyperparameters (core ones we consider for tuning)\n",
    "rf_hyperparams_available = {\n",
    "    \"n_estimators\": \"Number of trees (controls ensemble size and variance reduction)\",\n",
    "    \"max_depth\": \"Tree depth (controls model complexity and overfitting risk)\",\n",
    "    \"min_samples_split\": \"Minimum samples to split (regularization for deep trees)\",\n",
    "    \"min_samples_leaf\": \"Minimum samples per leaf (helps smooth noisy splits)\",\n",
    "    \"max_features\": \"Number of features per split (handles correlation via subsampling)\",\n",
    "    \"class_weight\": \"Balancing classes if imbalance worsens (ratio currently moderate ~1.43)\",\n",
    "}\n",
    "\n",
    "# Selected hyperparameter for tuning\n",
    "selected_hyperparam = \"max_depth\"\n",
    "selected_reason = (\n",
    "    \"Depth controls complexity and overfitting; with 561 correlated features, limiting depth helps generalization. \"\n",
    "    \"It is easy to tune reproducibly with a small discrete grid and has a obvious impact on bias-variance tradeoff.\"\n",
    ")\n",
    "# Tuning plan (discrete grid)\n",
    "max_depth_grid = [5, 10, 15, 20, 25, 30]\n",
    "step_width = 5  # step between grid points\n",
    "\n",
    "# Deterministic UUIDs for provenance\n",
    "mod_4b_uuid_writer = deterministic_uuid(\"modeling_4b_hyperparam_selection:writer:student_a\")\n",
    "mod_4b_uuid_writer_b = deterministic_uuid(\"modeling_4b_hyperparam_selection:writer:student_b\")\n",
    "mod_4b_uuid_executor = deterministic_uuid(\"modeling_4b_hyperparam_selection:executor\")\n",
    "\n",
    "# Compose documentation text (concise, data-grounded, no invented stats)\n",
    "hp_selection_comment = escape_rdf_literal(\n",
    "    \"Hyperparameter identification for Random Forest: considered core knobs (n_estimators, max_depth, \"\n",
    "    \"min_samples_split, min_samples_leaf, max_features, class_weight). Selected max_depth for tuning because it \"\n",
    "    \"directly controls tree complexity and overfitting risk in a 561-feature space with many correlated variables. \"\n",
    "    \"Chosen discrete grid 5-30 with step 5 for reproducible, low-compute search; impacts bias-variance obviously and \"\n",
    "    \"is straightforward to justify.\"\n",
    ")\n",
    "\n",
    "hyperparam_selection_activity = [\n",
    "    ':mod_4b_hyperparam_selection rdf:type prov:Activity .',\n",
    "    ':mod_4b_hyperparam_selection sc:isPartOf :modeling_phase .',\n",
    "    ':mod_4b_hyperparam_selection rdfs:label \"4b) Hyperparameter Identification and Selection\" .',\n",
    "    # Associations\n",
    "    f':mod_4b_hyperparam_selection prov:qualifiedAssociation :{mod_4b_uuid_writer} .',\n",
    "    f':{mod_4b_uuid_writer} prov:agent :{modeling_code_writer} .',\n",
    "    f':{mod_4b_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{mod_4b_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':mod_4b_hyperparam_selection prov:qualifiedAssociation :{mod_4b_uuid_writer_b} .',\n",
    "    f':{mod_4b_uuid_writer_b} prov:agent :{modeling_code_writer_b} .',\n",
    "    f':{mod_4b_uuid_writer_b} rdf:type prov:Association .',\n",
    "    f':{mod_4b_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "    f':mod_4b_hyperparam_selection prov:qualifiedAssociation :{mod_4b_uuid_executor} .',\n",
    "    f':{mod_4b_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{mod_4b_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{mod_4b_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    # Documentation entities\n",
    "    ':rf_hyperparams rdf:type prov:Entity .',\n",
    "    ':rf_hyperparams rdfs:label \"Random Forest hyperparameters (considered)\" .',\n",
    "    f':rf_hyperparams rdfs:comment \"\"\"{escape_rdf_literal(str(rf_hyperparams_available))}\"\"\" .',\n",
    "    ':rf_hyperparams prov:wasGeneratedBy :mod_4b_hyperparam_selection .',\n",
    "\n",
    "    ':rf_hp_max_depth rdf:type mls:HyperParameter .',\n",
    "    ':rf_hp_max_depth rdfs:label \"max_depth\" .',\n",
    "    ':rf_hp_max_depth rdfs:comment \"Tree depth controlling complexity and overfitting\" .',\n",
    "    ':rf_hp_max_depth prov:wasGeneratedBy :mod_4b_hyperparam_selection .',\n",
    "\n",
    "    ':rf_hp_max_depth_setting rdf:type mls:HyperParameterSetting .',\n",
    "    ':rf_hp_max_depth_setting mls:specifiedBy :rf_hp_max_depth .',\n",
    "    f':rf_hp_max_depth_setting rdfs:comment \"Selected grid: {max_depth_grid} (step {step_width})\" .',\n",
    "    ':rf_hp_max_depth_setting prov:wasGeneratedBy :mod_4b_hyperparam_selection .',\n",
    "\n",
    "    ':rf_hp_choice rdf:type prov:Entity .',\n",
    "    ':rf_hp_choice rdfs:label \"Selected hyperparameter for tuning\" .',\n",
    "    f':rf_hp_choice rdfs:comment \"\"\"{hp_selection_comment}\"\"\" .',\n",
    "    ':rf_hp_choice prov:wasGeneratedBy :mod_4b_hyperparam_selection .',\n",
    "]\n",
    "\n",
    "engine.insert(hyperparam_selection_activity, prefixes=prefixes)\n",
    "\n",
    "print(\"SECTION 4b: HYPERPARAMETER IDENTIFICATION\")\n",
    "\n",
    "print(\"Selected hyperparameter: max_depth\")\n",
    "print(f\"Grid: {max_depth_grid} (step {step_width})\")\n",
    "print(\"Rationale: controls complexity/overfitting in high-dimensional correlated feature space; easy, reproducible grid search\")\n",
    "print(\"Status: Hyperparameter selection documented and logged to provenance graph\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc6a299",
   "metadata": {},
   "source": [
    "### 4c) Train / Validation Split Definition (group-stratified by subject, keep provided test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995966b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4c) Train / Validation Split Definition (group-stratified, no leakage of subjects)\n",
    "# Stratified by activity labels and grouped by subject to avoid overlap; keep provided test set untouched.\n",
    "\n",
    "\n",
    "TARGET_COL = 'Activity'\n",
    "GROUP_COL = 'subject'\n",
    "ENCODED_TARGET = 'Activity_encoded'\n",
    "\n",
    "missing_cols = [col for col in [TARGET_COL, GROUP_COL, ENCODED_TARGET] if col not in train_prepared.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Expected columns missing for split: {missing_cols}\")\n",
    "\n",
    "feature_cols = [col for col in train_prepared.columns if col not in [TARGET_COL, ENCODED_TARGET, GROUP_COL]]\n",
    "X = train_prepared[feature_cols]\n",
    "y = train_prepared[ENCODED_TARGET]\n",
    "groups = train_prepared[GROUP_COL]\n",
    "\n",
    "total_train_rows = len(train_prepared)\n",
    "test_rows = len(test_prepared)\n",
    "train_subjects = set(train_prepared[GROUP_COL].unique())\n",
    "test_subjects = set(test_prepared[GROUP_COL].unique()) if GROUP_COL in test_prepared else set()\n",
    "subject_overlap = len(train_subjects.intersection(test_subjects))\n",
    "\n",
    "# Grouped stratified split ~80/20 using first fold of StratifiedGroupKFold\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_idx, val_idx = next(sgkf.split(X, y, groups))\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx].copy(), X.iloc[val_idx].copy()\n",
    "y_train, y_val = y.iloc[train_idx].copy(), y.iloc[val_idx].copy()\n",
    "\n",
    "train_split_size = len(X_train)\n",
    "val_split_size = len(X_val)\n",
    "train_pct = round(train_split_size / total_train_rows * 100, 2)\n",
    "val_pct = round(val_split_size / total_train_rows * 100, 2)\n",
    "test_pct = round(test_rows / (total_train_rows + test_rows) * 100, 2)\n",
    "\n",
    "mod_4c_uuid_writer = deterministic_uuid(\"modeling_4c_data_split:writer:student_a\")\n",
    "mod_4c_uuid_writer_b = deterministic_uuid(\"modeling_4c_data_split:writer:student_b\")\n",
    "mod_4c_uuid_executor = deterministic_uuid(\"modeling_4c_data_split:executor\")\n",
    "\n",
    "split_comment = escape_rdf_literal(\n",
    "    \"Grouped stratified split on HAR train set: preserves subject disjointness (groups=subject) and class balance \"\n",
    "    \"via StratifiedGroupKFold (n_splits=5, shuffle=True, random_state=42); uses first fold to obtain ~80% train and ~20% validation; \"\n",
    "    \"test set (2,947 rows) remains untouched; subject-level dependency (time-series per subject) handled by grouping to avoid leakage; \"\n",
    "    \"relative sizes: train ~81.8%, validation ~18.2% of original train; test ~28.6% of all provided data.\"\n",
    ")\n",
    "\n",
    "split_activity = [\n",
    "    ':mod_4c_data_split rdf:type prov:Activity .',\n",
    "    ':mod_4c_data_split sc:isPartOf :modeling_phase .',\n",
    "    ':mod_4c_data_split rdfs:label \"4c) Train/Validation/Test Split\" .',\n",
    "    f':mod_4c_data_split rdfs:comment \"\"\"{split_comment}\"\"\" .',\n",
    "    # Associations\n",
    "    f':mod_4c_data_split prov:qualifiedAssociation :{mod_4c_uuid_writer} .',\n",
    "    f':{mod_4c_uuid_writer} prov:agent :{modeling_code_writer} .',\n",
    "    f':{mod_4c_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{mod_4c_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':mod_4c_data_split prov:qualifiedAssociation :{mod_4c_uuid_writer_b} .',\n",
    "    f':{mod_4c_uuid_writer_b} prov:agent :{modeling_code_writer_b} .',\n",
    "    f':{mod_4c_uuid_writer_b} rdf:type prov:Association .',\n",
    "    f':{mod_4c_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "    f':mod_4c_data_split prov:qualifiedAssociation :{mod_4c_uuid_executor} .',\n",
    "    f':{mod_4c_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{mod_4c_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{mod_4c_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    # Inputs\n",
    "    ':mod_4c_data_split prov:used :prepared_data .',\n",
    "    # Outputs: train/validation sets\n",
    "    ':rf_train_set rdf:type sc:Dataset .',\n",
    "    ':rf_train_set rdfs:label \"Training Set (split, grouped)\" .',\n",
    "    f':rf_train_set rdfs:comment \"Size: {train_split_size}\" .',\n",
    "    ':rf_train_set prov:wasGeneratedBy :mod_4c_data_split .',\n",
    "    ':rf_train_set prov:wasDerivedFrom :prepared_data .',\n",
    "    ':rf_validation_set rdf:type sc:Dataset .',\n",
    "    ':rf_validation_set rdfs:label \"Validation Set (split, grouped)\" .',\n",
    "    f':rf_validation_set rdfs:comment \"Size: {val_split_size}\" .',\n",
    "    ':rf_validation_set prov:wasGeneratedBy :mod_4c_data_split .',\n",
    "    ':rf_validation_set prov:wasDerivedFrom :prepared_data .',\n",
    "]\n",
    "\n",
    "engine.insert(split_activity, prefixes=prefixes)\n",
    "\n",
    "\n",
    "print(\"SECTION 4c: DATA SPLIT (GROUP-STRATIFIED)\")\n",
    "\n",
    "print(f\"Train size: {train_split_size} ({train_pct}% of train)\")\n",
    "print(f\"Validation size: {val_split_size} ({val_pct}% of train)\")\n",
    "print(f\"Original train size: {total_train_rows}\")\n",
    "print(f\"Test size (provided, untouched): {test_rows} (~{test_pct}% of total data)\")\n",
    "print(f\"Subject overlap train vs test: {subject_overlap}\")\n",
    "print(\"Method: StratifiedGroupKFold (n_splits=5, shuffle=True, random_state=42); groups=subject; class-balanced\")\n",
    "print(\"Dependency handling: subject-level time-series grouped to avoid leakage\")\n",
    "print(\"Status: Split documented and logged to provenance graph\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48b806",
   "metadata": {},
   "source": [
    "### 4d) Model Training (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b5ed6-54d6-4c81-9adb-e295fbd5c364",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-978b274ef875c238",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 4d) Train Random Forest across max_depth grid; compare train vs validation; log all settings\n",
    "# Inputs: X_train, y_train, X_val, y_val from 4c; max_depth_grid from 4b\n",
    "\n",
    "mod_4d_uuid_writer = deterministic_uuid(\"modeling_4d_training:writer:student_a\")\n",
    "mod_4d_uuid_writer_b = deterministic_uuid(\"modeling_4d_training:writer:student_b\")\n",
    "mod_4d_uuid_executor = deterministic_uuid(\"modeling_4d_training:executor\")\n",
    "\n",
    "# Fixed hyperparameters (documented explicitly for reproducibility)\n",
    "fixed_params = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"max_features\": \"sqrt\",\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "results = []\n",
    "triples = [\n",
    "    ':mod_4d_training rdf:type prov:Activity .',\n",
    "    ':mod_4d_training sc:isPartOf :modeling_phase .',\n",
    "    ':mod_4d_training rdfs:label \"4d) Training and Comparison\" .',\n",
    "    # Associations\n",
    "    f':mod_4d_training prov:qualifiedAssociation :{mod_4d_uuid_writer} .',\n",
    "    f':{mod_4d_uuid_writer} prov:agent :{modeling_code_writer} .',\n",
    "    f':{mod_4d_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{mod_4d_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':mod_4d_training prov:qualifiedAssociation :{mod_4d_uuid_writer_b} .',\n",
    "    f':{mod_4d_uuid_writer_b} prov:agent :{modeling_code_writer_b} .',\n",
    "    f':{mod_4d_uuid_writer_b} rdf:type prov:Association .',\n",
    "    f':{mod_4d_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "    f':mod_4d_training prov:qualifiedAssociation :{mod_4d_uuid_executor} .',\n",
    "    f':{mod_4d_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{mod_4d_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{mod_4d_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    # Inputs\n",
    "    ':mod_4d_training prov:used :rf_train_set .',\n",
    "    ':mod_4d_training prov:used :rf_validation_set .',\n",
    "    ':mod_4d_training prov:used :rf_hp_max_depth_setting .',\n",
    "    ':mod_4d_training prov:used :rf_hp_fixed_params .',\n",
    "    # Fixed hyperparameters entity\n",
    "    ':rf_hp_fixed_params rdf:type mls:HyperParameterSetting .',\n",
    "    f':rf_hp_fixed_params rdfs:comment \"{escape_rdf_literal(str(fixed_params))}\" .',\n",
    "]\n",
    "\n",
    "for depth in max_depth_grid:\n",
    "    run_id = deterministic_uuid(f\"modeling_4d_run:max_depth={depth}\")\n",
    "    model_id = deterministic_uuid(f\"modeling_4d_model:max_depth={depth}\")\n",
    "    hp_setting_id = deterministic_uuid(f\"modeling_4d_hp_setting:max_depth={depth}\")\n",
    "    eval_train_id = deterministic_uuid(f\"modeling_4d_eval_train:max_depth={depth}\")\n",
    "    eval_val_id = deterministic_uuid(f\"modeling_4d_eval_val:max_depth={depth}\")\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=fixed_params[\"n_estimators\"],\n",
    "        max_depth=depth,\n",
    "        min_samples_split=fixed_params[\"min_samples_split\"],\n",
    "        min_samples_leaf=fixed_params[\"min_samples_leaf\"],\n",
    "        max_features=fixed_params[\"max_features\"],\n",
    "        random_state=fixed_params[\"random_state\"],\n",
    "        n_jobs=fixed_params[\"n_jobs\"],\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_pred = clf.predict(X_train)\n",
    "    val_pred = clf.predict(X_val)\n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"max_depth\": depth,\n",
    "        \"n_estimators\": fixed_params[\"n_estimators\"],\n",
    "        \"min_samples_split\": fixed_params[\"min_samples_split\"],\n",
    "        \"min_samples_leaf\": fixed_params[\"min_samples_leaf\"],\n",
    "        \"max_features\": fixed_params[\"max_features\"],\n",
    "        \"random_state\": fixed_params[\"random_state\"],\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_acc\": val_acc,\n",
    "    })\n",
    "\n",
    "    triples.extend([\n",
    "        f':run_{run_id} rdf:type mls:Run .',\n",
    "        f':run_{run_id} sc:isPartOf :mod_4d_training .',\n",
    "        f':run_{run_id} mls:realizes :random_forest_classifier .',\n",
    "        f':run_{run_id} mls:hasInput :rf_train_set .',\n",
    "        f':run_{run_id} mls:hasInput :rf_validation_set .',\n",
    "        f':run_{run_id} mls:hasInput :hp_setting_{hp_setting_id} .',\n",
    "        f':run_{run_id} mls:hasInput :rf_hp_fixed_params .',\n",
    "        f':hp_setting_{hp_setting_id} rdf:type mls:HyperParameterSetting .',\n",
    "        f':hp_setting_{hp_setting_id} mls:specifiedBy :rf_hp_max_depth .',\n",
    "        f':hp_setting_{hp_setting_id} mls:hasValue \"{depth}\"^^xsd:int .',\n",
    "        f':hp_setting_{hp_setting_id} prov:wasGeneratedBy :mod_4d_training .',\n",
    "        f':model_{model_id} rdf:type mls:Model .',\n",
    "        f':model_{model_id} prov:wasGeneratedBy :run_{run_id} .',\n",
    "        f':model_{model_id} mlso:trainedOn :rf_train_set .',\n",
    "        f':model_{model_id} mlso:hasAlgorithmType :random_forest_classifier .',\n",
    "        f':run_{run_id} mls:hasOutput :model_{model_id} .',\n",
    "        f':eval_train_{eval_train_id} rdf:type mls:ModelEvaluation .',\n",
    "        f':eval_train_{eval_train_id} mls:specifiedBy :accuracy_metric .',\n",
    "        f':eval_train_{eval_train_id} mls:hasValue \"{train_acc}\"^^xsd:double .',\n",
    "        f':eval_train_{eval_train_id} prov:used :rf_train_set .',\n",
    "        f':eval_train_{eval_train_id} prov:wasGeneratedBy :run_{run_id} .',\n",
    "        f':eval_val_{eval_val_id} rdf:type mls:ModelEvaluation .',\n",
    "        f':eval_val_{eval_val_id} mls:specifiedBy :accuracy_metric .',\n",
    "        f':eval_val_{eval_val_id} mls:hasValue \"{val_acc}\"^^xsd:double .',\n",
    "        f':eval_val_{eval_val_id} prov:used :rf_validation_set .',\n",
    "        f':eval_val_{eval_val_id} prov:wasGeneratedBy :run_{run_id} .',\n",
    "        f':run_{run_id} mls:hasOutput :eval_train_{eval_train_id} .',\n",
    "        f':run_{run_id} mls:hasOutput :eval_val_{eval_val_id} .',\n",
    "    ])\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"val_acc\", ascending=False)\n",
    "\n",
    "print(\"SECTION 4d: TRAIN & COMPARE (Random Forest, max_depth grid)\")\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "best_row = results_df.iloc[0]\n",
    "\n",
    "\n",
    "engine.insert(triples, prefixes=prefixes)\n",
    "\n",
    "print(\"\\nBest setting based on validation accuracy:\")\n",
    "print({\"max_depth\": int(best_row.max_depth), \"val_acc\": best_row.val_acc, \"train_acc\": best_row.train_acc})\n",
    "print(\"Fixed parameters for all runs:\")\n",
    "print(fixed_params)\n",
    "print(\"Status: Training runs logged to provenance graph\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a3d7e0",
   "metadata": {},
   "source": [
    "### 4e) Hyperparameter Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4e) Plot tuning results and log visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'results_df' not in globals():\n",
    "    raise RuntimeError(\"Run 4d cell first to create results_df\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(results_df['max_depth'], results_df['train_acc'], marker='o', label='Train accuracy')\n",
    "ax.plot(results_df['max_depth'], results_df['val_acc'], marker='s', label='Validation accuracy')\n",
    "ax.set_xlabel('max_depth')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Random Forest tuning: max_depth vs accuracy')\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Export figure to output/figures/ (matching Section 2 & 5a pattern)\n",
    "tuning_plot_path = os.path.join('output', 'figures', 'fig_4e_hyperparameter_tuning.png')\n",
    "os.makedirs(os.path.dirname(tuning_plot_path), exist_ok=True)\n",
    "fig.savefig(tuning_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(\"Saved: fig_4e_hyperparameter_tuning.png\")\n",
    "\n",
    "# Display before closing\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "mod_4e_uuid_writer = deterministic_uuid(\"modeling_4e_plot:writer:student_a\")\n",
    "mod_4e_uuid_writer_b = deterministic_uuid(\"modeling_4e_plot:writer:student_b\")\n",
    "mod_4e_uuid_executor = deterministic_uuid(\"modeling_4e_plot:executor\")\n",
    "\n",
    "plot_comment = escape_rdf_literal(\n",
    "    \"Visualization of train vs validation accuracy across max_depth grid (5,10,15,20,25,30) for Random Forest; \"\n",
    "    \"shows best validation at max_depth=10 and rising overfit for deeper trees.\"\n",
    ")\n",
    "\n",
    "plot_triples = [\n",
    "    ':mod_4e_plot rdf:type prov:Activity .',\n",
    "    ':mod_4e_plot sc:isPartOf :modeling_phase .',\n",
    "    ':mod_4e_plot rdfs:label \"4e) Hyperparameter Tuning Plot\" .',\n",
    "    f':mod_4e_plot rdfs:comment \"\"\"{plot_comment}\"\"\" .',\n",
    "    f':mod_4e_plot prov:qualifiedAssociation :{mod_4e_uuid_writer} .',\n",
    "    f':{mod_4e_uuid_writer} prov:agent :{modeling_code_writer} .',\n",
    "    f':{mod_4e_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{mod_4e_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':mod_4e_plot prov:qualifiedAssociation :{mod_4e_uuid_writer_b} .',\n",
    "    f':{mod_4e_uuid_writer_b} prov:agent :{modeling_code_writer_b} .',\n",
    "    f':{mod_4e_uuid_writer_b} rdf:type prov:Association .',\n",
    "    f':{mod_4e_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "    f':mod_4e_plot prov:qualifiedAssociation :{mod_4e_uuid_executor} .',\n",
    "    f':{mod_4e_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{mod_4e_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{mod_4e_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':mod_4e_plot prov:used :mod_4d_training .',\n",
    "    # Figure 4e: Hyperparameter tuning plot with separate name and interpretation\n",
    "    ':mod_4e_tuning_viz rdf:type prov:Entity .',\n",
    "    ':mod_4e_tuning_viz rdf:type sc:Figure .',\n",
    "    ':mod_4e_tuning_viz rdfs:label \"Random Forest max_depth Tuning Results\" .',\n",
    "    ':mod_4e_tuning_viz sc:contentUrl \"output/figures/fig_4e_hyperparameter_tuning.png\" .',\n",
    "    f':mod_4e_tuning_viz rdfs:comment \"{escape_rdf_literal(\"Line plot showing train and validation accuracy across max_depth values (5, 10, 15, 20, 25, 30). Validation accuracy peaks at max_depth=10 with highest score, while training accuracy continues to improve with deeper trees, indicating overfitting risk for max_depth>10. Train-validation gap widest at max_depth=30. The plot demonstrates why max_depth=10 was selected as the optimal hyperparameter balancing validation performance with generalization.\")}\" .',\n",
    "    ':mod_4e_tuning_viz prov:wasGeneratedBy :mod_4e_plot .',\n",
    "    ':rf_tuning_plot rdf:type prov:Entity .',\n",
    "    ':rf_tuning_plot rdfs:label \"RF max_depth tuning plot\" .',\n",
    "    ':rf_tuning_plot prov:wasGeneratedBy :mod_4e_plot .',\n",
    "]\n",
    "\n",
    "engine.insert(plot_triples, prefixes=prefixes)\n",
    "\n",
    "\n",
    "print(\"SECTION 4e: HYPERPARAMETER TUNING PLOT\")\n",
    "\n",
    "print(\"Status: Plot rendered and provenance logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99bd34",
   "metadata": {},
   "source": [
    "### 4f) Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02059271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4f) Select best model using validation accuracy and log decision\n",
    "if 'results_df' not in globals():\n",
    "    raise RuntimeError(\"Run 4d cell first to build results_df\")\n",
    "\n",
    "best_row = results_df.sort_values(by=\"val_acc\", ascending=False).iloc[0]\n",
    "best_depth = int(best_row.max_depth)\n",
    "best_val_acc = float(best_row.val_acc)\n",
    "best_train_acc = float(best_row.train_acc)\n",
    "\n",
    "best_run_id = deterministic_uuid(f\"modeling_4d_run:max_depth={best_depth}\")\n",
    "best_model_id = deterministic_uuid(f\"modeling_4d_model:max_depth={best_depth}\")\n",
    "best_hp_setting_id = deterministic_uuid(f\"modeling_4d_hp_setting:max_depth={best_depth}\")\n",
    "best_eval_val_id = deterministic_uuid(f\"modeling_4d_eval_val:max_depth={best_depth}\")\n",
    "\n",
    "mod_4f_uuid_writer = deterministic_uuid(\"modeling_4f_selection:writer:student_a\")\n",
    "mod_4f_uuid_writer_b = deterministic_uuid(\"modeling_4f_selection:writer:student_b\")\n",
    "mod_4f_uuid_executor = deterministic_uuid(\"modeling_4f_selection:executor\")\n",
    "\n",
    "start_time_4f = now()\n",
    "decision_4f_text = escape_rdf_literal(\n",
    "    f\"Selected Random Forest with max_depth={best_depth} as it reached validation accuracy {best_val_acc:.4f} \"\n",
    "    f\"and kept a small gap to train accuracy {best_train_acc:.4f}, giving best generalization in the grid.\"\n",
    " )\n",
    "end_time_4f = now()\n",
    "\n",
    "selection_triples = [\n",
    "    \":mod_4f_selection rdf:type prov:Activity .\",\n",
    "    \":mod_4f_selection sc:isPartOf :modeling_phase .\",\n",
    "    \":mod_4f_selection rdfs:label \\\"4f) Model Selection\\\" .\",\n",
    "    f\":mod_4f_selection prov:startedAtTime \\\"{start_time_4f}\\\"^^xsd:dateTime .\",\n",
    "    f\":mod_4f_selection prov:endedAtTime \\\"{end_time_4f}\\\"^^xsd:dateTime .\",\n",
    "    f\":mod_4f_selection prov:qualifiedAssociation :{mod_4f_uuid_writer} .\",\n",
    "    f\":{mod_4f_uuid_writer} prov:agent :{modeling_code_writer} .\",\n",
    "    f\":{mod_4f_uuid_writer} rdf:type prov:Association .\",\n",
    "    f\":{mod_4f_uuid_writer} prov:hadRole :{code_writer_role} .\",\n",
    "    f\":mod_4f_selection prov:qualifiedAssociation :{mod_4f_uuid_writer_b} .\",\n",
    "    f\":{mod_4f_uuid_writer_b} prov:agent :{modeling_code_writer_b} .\",\n",
    "    f\":{mod_4f_uuid_writer_b} rdf:type prov:Association .\",\n",
    "    f\":{mod_4f_uuid_writer_b} prov:hadRole :{code_writer_role} .\",\n",
    "    f\":mod_4f_selection prov:qualifiedAssociation :{mod_4f_uuid_executor} .\",\n",
    "    f\":{mod_4f_uuid_executor} prov:agent :{executed_by} .\",\n",
    "    f\":{mod_4f_uuid_executor} rdf:type prov:Association .\",\n",
    "    f\":{mod_4f_uuid_executor} prov:hadRole :{code_executor_role} .\",\n",
    "    \":mod_4f_selection prov:used :mod_4d_training .\",\n",
    "    f\":mod_4f_selection prov:used :run_{best_run_id} .\",\n",
    "    f\":mod_4f_selection prov:used :eval_val_{best_eval_val_id} .\",\n",
    "    f\":rf_selected_model rdf:type mls:Model .\",\n",
    "    f\":rf_selected_model prov:wasDerivedFrom :model_{best_model_id} .\",\n",
    "    f\":rf_selected_model prov:wasGeneratedBy :mod_4f_selection .\",\n",
    "    f\":rf_selected_model mlso:hasAlgorithmType :random_forest_classifier .\",\n",
    "    f\":rf_selected_model mls:hasHyperParameterSetting :hp_setting_{best_hp_setting_id} .\",\n",
    "    f\":rf_selected_model mls:achieves :eval_val_{best_eval_val_id} .\",\n",
    "    \":decision_4f_entity rdf:type prov:Entity .\",\n",
    "    f\":decision_4f_entity rdfs:comment \\\"\\\"\\\"{decision_4f_text}\\\"\\\"\\\" .\",\n",
    "    \":decision_4f_entity sc:isPartOf :mod_4f_selection .\",\n",
    "    \":decision_4f_entity prov:wasGeneratedBy :mod_4f_selection .\",\n",
    "    \":decision_4f_entity rdf:type sc:Decision .\",\n",
    "]\n",
    "engine.insert(selection_triples, prefixes=prefixes)\n",
    "\n",
    "\n",
    "print(\"SECTION 4f: MODEL SELECTION\")\n",
    "\n",
    "print(f\"Chosen model: Random Forest with max_depth={best_depth}\")\n",
    "print(f\"Validation accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Train accuracy: {best_train_acc:.4f}\")\n",
    "print(\"Status: Selection documented and logged to provenance graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15826224",
   "metadata": {},
   "source": [
    "### 4g) Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06583f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4g) Retrain final model on full train+validation split with selected hyperparameters\n",
    "if 'best_depth' not in globals() or 'results_df' not in globals():\n",
    "    raise RuntimeError(\"Run 4d to 4f first to select best_depth\")\n",
    "\n",
    "# Combine train and validation\n",
    "X_trainval = pd.concat([X_train, X_val], axis=0)\n",
    "y_trainval = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "final_run_id = deterministic_uuid(\"modeling_4g_run_final\")\n",
    "final_model_id = deterministic_uuid(\"modeling_4g_model_final\")\n",
    "final_eval_id = deterministic_uuid(\"modeling_4g_eval_trainval\")\n",
    "mod_4g_uuid_writer = deterministic_uuid(\"modeling_4g_final_train:writer:student_a\")\n",
    "mod_4g_uuid_writer_b = deterministic_uuid(\"modeling_4g_final_train:writer:student_b\")\n",
    "mod_4g_uuid_executor = deterministic_uuid(\"modeling_4g_final_train:executor\")\n",
    "\n",
    "rf_final = RandomForestClassifier(\n",
    "    n_estimators=fixed_params[\"n_estimators\"],\n",
    "    max_depth=int(best_depth),\n",
    "    min_samples_split=fixed_params[\"min_samples_split\"],\n",
    "    min_samples_leaf=fixed_params[\"min_samples_leaf\"],\n",
    "    max_features=fixed_params[\"max_features\"],\n",
    "    random_state=fixed_params[\"random_state\"],\n",
    "    n_jobs=fixed_params[\"n_jobs\"],\n",
    ")\n",
    "rf_final.fit(X_trainval, y_trainval)\n",
    "trainval_pred = rf_final.predict(X_trainval)\n",
    "trainval_acc = accuracy_score(y_trainval, trainval_pred)\n",
    "\n",
    "# Create descriptive text for LaTeX report\n",
    "mod_final_training = f\"\"\"After selecting the optimal max_depth of {int(best_depth)} through cross-validation, \n",
    "we retrained the Random Forest classifier on the combined training and validation sets. This final model \n",
    "uses {X_trainval.shape[0]} samples with {X_trainval.shape[1]} features and the previously determined \n",
    "hyperparameters (n_estimators={fixed_params[\"n_estimators\"]}, min_samples_split={fixed_params[\"min_samples_split\"]}, \n",
    "min_samples_leaf={fixed_params[\"min_samples_leaf\"]}). The combined train-validation accuracy reached {trainval_acc:.4f}, \n",
    "demonstrating consistent performance before final evaluation on the held-out test set. This model serves as our \n",
    "final classifier for deployment and subsequent bias assessment.\"\"\"\n",
    "\n",
    "final_triples = [\n",
    "    ':mod_4g_final_training rdf:type prov:Activity .',\n",
    "    ':mod_4g_final_training sc:isPartOf :modeling_phase .',\n",
    "    ':mod_4g_final_training rdfs:label \"4g) Final Training\" .',\n",
    "    f':mod_4g_final_training prov:qualifiedAssociation :{mod_4g_uuid_writer} .',\n",
    "    f':{mod_4g_uuid_writer} prov:agent :{modeling_code_writer} .',\n",
    "    f':{mod_4g_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{mod_4g_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':mod_4g_final_training prov:qualifiedAssociation :{mod_4g_uuid_writer_b} .',\n",
    "    f':{mod_4g_uuid_writer_b} prov:agent :{modeling_code_writer_b} .',\n",
    "    f':{mod_4g_uuid_writer_b} rdf:type prov:Association .',\n",
    "    f':{mod_4g_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "    f':mod_4g_final_training prov:qualifiedAssociation :{mod_4g_uuid_executor} .',\n",
    "    f':{mod_4g_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{mod_4g_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{mod_4g_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':mod_4g_final_training prov:used :rf_train_set .',\n",
    "    ':mod_4g_final_training prov:used :rf_validation_set .',\n",
    "    ':mod_4g_final_training prov:used :rf_hp_fixed_params .',\n",
    "    f':mod_4g_final_training prov:used :hp_setting_{best_hp_setting_id} .',\n",
    "    ':rf_trainval_set rdf:type mls:Dataset .',\n",
    "    ':rf_trainval_set rdfs:label \"Train and validation combined set\" .',\n",
    "    ':rf_trainval_set prov:wasDerivedFrom :rf_train_set .',\n",
    "    ':rf_trainval_set prov:wasDerivedFrom :rf_validation_set .',\n",
    "    ':rf_trainval_set prov:wasGeneratedBy :mod_4g_final_training .',\n",
    "    ':mod_4g_final_training prov:used :rf_trainval_set .',\n",
    "    f':run_{final_run_id} rdf:type mls:Run .',\n",
    "    f':run_{final_run_id} sc:isPartOf :mod_4g_final_training .',\n",
    "    f':run_{final_run_id} mls:realizes :random_forest_classifier .',\n",
    "    f':run_{final_run_id} mls:hasInput :rf_trainval_set .',\n",
    "    f':run_{final_run_id} mls:hasInput :hp_setting_{best_hp_setting_id} .',\n",
    "    f':run_{final_run_id} mls:hasInput :rf_hp_fixed_params .',\n",
    "    f':hp_setting_{best_hp_setting_id} mls:specifiedBy :rf_hp_max_depth .',\n",
    "    f':hp_setting_{best_hp_setting_id} mls:hasValue \"{int(best_depth)}\"^^xsd:int .',\n",
    "    f':model_{final_model_id} rdf:type mls:Model .',\n",
    "    f':model_{final_model_id} prov:wasGeneratedBy :run_{final_run_id} .',\n",
    "    f':model_{final_model_id} mlso:trainedOn :rf_trainval_set .',\n",
    "    f':model_{final_model_id} mlso:hasAlgorithmType :random_forest_classifier .',\n",
    "    f':model_{final_model_id} mls:hasHyperParameterSetting :hp_setting_{best_hp_setting_id} .',\n",
    "    f':run_{final_run_id} mls:hasOutput :model_{final_model_id} .',\n",
    "    f':eval_trainval_{final_eval_id} rdf:type mls:ModelEvaluation .',\n",
    "    f':eval_trainval_{final_eval_id} mls:specifiedBy :accuracy_metric .',\n",
    "    f':eval_trainval_{final_eval_id} mls:hasValue \"{trainval_acc}\"^^xsd:double .',\n",
    "    f':eval_trainval_{final_eval_id} prov:used :rf_trainval_set .',\n",
    "    f':eval_trainval_{final_eval_id} prov:wasGeneratedBy :run_{final_run_id} .',\n",
    "    f':run_{final_run_id} mls:hasOutput :eval_trainval_{final_eval_id} .',\n",
    "    ':rf_final_model rdf:type mls:Model .',\n",
    "    ':rf_final_model prov:wasGeneratedBy :mod_4g_final_training .',\n",
    "    f':rf_final_model prov:wasDerivedFrom :model_{final_model_id} .',\n",
    "    ':rf_final_model mlso:hasAlgorithmType :random_forest_classifier .',\n",
    "    ':rf_final_model mlso:trainedOn :rf_trainval_set .',\n",
    "    f':rf_final_model mls:achieves :eval_trainval_{final_eval_id} .',\n",
    "]\n",
    "engine.insert(final_triples, prefixes=prefixes)\n",
    "\n",
    "\n",
    "print(\"SECTION 4g: FINAL TRAINING\")\n",
    "\n",
    "print(f\"Final model: Random Forest max_depth={int(best_depth)}\")\n",
    "print(f\"Train+Val accuracy: {trainval_acc:.4f}\")\n",
    "print(\"Status: Final model trained on combined train and validation and logged to provenance graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88bf71f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46137067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Evaluation Phase\n",
    "\n",
    "evaluation_phase_executor = [\n",
    "f':evaluation_phase rdf:type prov:Activity .',\n",
    "f':evaluation_phase rdfs:label \"Evaluation Phase\" .', \n",
    "]\n",
    "engine.insert(evaluation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d80e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Responsible: Student B (Student A assisting)\n",
    "eval_code_writer = student_b\n",
    "eval_code_writer_b = student_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46231d",
   "metadata": {},
   "source": [
    "### 5a) Final model evaluation on held-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7816757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5a) Apply final model on held-out test set and log performance\n",
    "\n",
    "# Preconditions\n",
    "required_objs = ['rf_final', 'test_prepared', 'feature_cols', 'label_encoder', 'ENCODED_TARGET', 'student_a', 'student_b', 'executed_by']\n",
    "missing_objs = [obj for obj in required_objs if obj not in globals()]\n",
    "if missing_objs:\n",
    "    raise RuntimeError(f\"Missing required objects for evaluation: {missing_objs}\")\n",
    "if ENCODED_TARGET not in test_prepared.columns:\n",
    "    raise RuntimeError(f\"Column {ENCODED_TARGET} missing in test_prepared\")\n",
    "\n",
    "# Prepare data\n",
    "X_test = test_prepared[feature_cols]\n",
    "y_test = test_prepared[ENCODED_TARGET]\n",
    "\n",
    "# Predict\n",
    "test_pred = rf_final.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_test, test_pred)\n",
    "prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(y_test, test_pred, average='macro', zero_division=0)\n",
    "prec_micro, rec_micro, f1_micro, _ = precision_recall_fscore_support(y_test, test_pred, average='micro', zero_division=0)\n",
    "cls_report = classification_report(y_test, test_pred, target_names=label_encoder.classes_, output_dict=True, zero_division=0)\n",
    "conf_mat = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "# Save artifacts\n",
    "report_dir = os.path.join('output')\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "metrics_path = os.path.join(report_dir, 'evaluation_metrics_5a.csv')\n",
    "pd.DataFrame({\n",
    "    'metric': ['accuracy','precision_macro','recall_macro','f1_macro','precision_micro','recall_micro','f1_micro'],\n",
    "    'value': [acc, prec_macro, rec_macro, f1_macro, prec_micro, rec_micro, f1_micro],\n",
    "}).to_csv(metrics_path, index=False)\n",
    "\n",
    "cls_report_path = os.path.join(report_dir, 'evaluation_classification_report_5a.csv')\n",
    "pd.DataFrame(cls_report).to_csv(cls_report_path)\n",
    "\n",
    "cm_path = os.path.join(report_dir, 'evaluation_confusion_matrix_5a.csv')\n",
    "pd.DataFrame(conf_mat, index=label_encoder.classes_, columns=label_encoder.classes_).to_csv(cm_path)\n",
    "\n",
    "# Create confusion matrix visualization\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save BEFORE show\n",
    "cm_plot_path = os.path.join('output', 'figures', 'evaluation_confusion_matrix_5a.png')\n",
    "os.makedirs(os.path.dirname(cm_plot_path), exist_ok=True)\n",
    "plt.savefig(cm_plot_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "print(\"Saved confusion matrix visualization:\", cm_plot_path)\n",
    "\n",
    "# Create and display per-class metrics visualization\n",
    "metrics_data = {\n",
    "    'Activity': list(label_encoder.classes_),\n",
    "    'Precision': [cls_report[cls]['precision'] for cls in label_encoder.classes_],\n",
    "    'Recall': [cls_report[cls]['recall'] for cls in label_encoder.classes_],\n",
    "    'F1-Score': [cls_report[cls]['f1-score'] for cls in label_encoder.classes_]\n",
    "}\n",
    "metrics_plot_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = range(len(metrics_plot_df))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar([i - width for i in x], metrics_plot_df['Precision'], width, label='Precision', alpha=0.8)\n",
    "ax.bar(x, metrics_plot_df['Recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar([i + width for i in x], metrics_plot_df['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Activity Class', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Per-Class Performance Metrics on Test Set', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_plot_df['Activity'], rotation=45, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "perclass_plot_path = os.path.join('output', 'figures', 'evaluation_perclass_metrics_5a.png')\n",
    "os.makedirs(os.path.dirname(perclass_plot_path), exist_ok=True)\n",
    "fig.savefig(perclass_plot_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"Saved per-class metrics visualization:\", perclass_plot_path)\n",
    "\n",
    "\n",
    "print(\"SECTION 5a:  EVALUATION (metrics only)\")\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Macro P/R/F1: {prec_macro:.4f}, {rec_macro:.4f}, {f1_macro:.4f}\")\n",
    "print(f\"Micro P/R/F1: {prec_micro:.4f}, {rec_micro:.4f}, {f1_micro:.4f}\")\n",
    "print(f\"Artifacts saved: {metrics_path}, {cls_report_path}, {cm_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04358a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Provenance logging (writer: B, assistant: A, executor: current runner)\n",
    "# Preconditions\n",
    "required_objs = ['acc', 'prec_macro', 'rec_macro', 'f1_macro', 'prec_micro', 'rec_micro', 'f1_micro', \n",
    "                 'cm_path', 'metrics_path', 'cls_report_path', 'cm_plot_path', 'perclass_plot_path', \n",
    "                 'student_a', 'student_b', 'executed_by']\n",
    "missing_objs = [obj for obj in required_objs if obj not in globals()]\n",
    "if missing_objs:\n",
    "    raise RuntimeError(f\"Missing required objects for 5a provenance logging: {missing_objs}\")\n",
    "\n",
    "eval_5a_uuid_writer_b = deterministic_uuid(\"evaluation_5a:writer:student_b\")\n",
    "eval_5a_uuid_writer_a = deterministic_uuid(\"evaluation_5a:writer_assistant:student_a\")\n",
    "eval_5a_uuid_executor = deterministic_uuid(\"evaluation_5a:executor\")\n",
    "eval_5a_activity_id = 'eval_5a_test_evaluation'\n",
    "start_time_5a = now()\n",
    "\n",
    "summary_comment = escape_rdf_literal(\n",
    "    f\"Test accuracy={acc:.4f}; macro P/R/F1=({prec_macro:.4f}, {rec_macro:.4f}, {f1_macro:.4f}); \"\n",
    "    f\"micro P/R/F1=({prec_micro:.4f}, {rec_micro:.4f}, {f1_micro:.4f}); artifacts at output/.\"\n",
    " )\n",
    "end_time_5a = now()\n",
    "\n",
    "# Normalize paths for RDF (use forward slashes)\n",
    "cm_plot_path_rdf = cm_plot_path.replace('\\\\', '/')\n",
    "perclass_plot_path_rdf = perclass_plot_path.replace('\\\\', '/')\n",
    "\n",
    "# Figure interpretations based on actual visualization content\n",
    "fig_5a_confusion_name = \"Confusion Matrix - Test Set\"\n",
    "fig_5a_confusion_interp = escape_rdf_literal(\n",
    "    f\"The confusion matrix shows the distribution of predicted vs true activity labels across all {len(y_test)} test samples. \"\n",
    "    f\"Diagonal cells represent correct predictions, while off-diagonal cells show misclassifications. \"\n",
    "    f\"The overall accuracy of {acc:.4f} indicates that approximately {int(acc*100)}% of predictions match the true activity labels. \"\n",
    "    f\"The matrix reveals which activity classes are frequently confused with each other.\"\n",
    ")\n",
    "\n",
    "fig_5a_perclass_name = \"Per-Class Performance Metrics (Precision, Recall, F1-Score)\"\n",
    "fig_5a_perclass_interp = escape_rdf_literal(\n",
    "    f\"Bar chart comparing per-class metrics (Precision, Recall, F1-Score) for each activity. \"\n",
    "    f\"Precision measures correctness of positive predictions for each class; Recall (sensitivity) measures how many actual instances were correctly identified. \"\n",
    "    f\"F1-Score balances precision and recall. Macro-averaged values (P={prec_macro:.4f}, R={rec_macro:.4f}, F1={f1_macro:.4f}) indicate balanced performance across all classes. \"\n",
    "    f\"Classes with lower recall highlight activities that are harder to predict accurately.\"\n",
    ")\n",
    "\n",
    "metric_defs = [\n",
    "    ':precision_macro_metric rdf:type mls:EvaluationMeasure .',\n",
    "    ':precision_macro_metric rdfs:label \"Macro Precision\" .',\n",
    "    ':recall_macro_metric rdf:type mls:EvaluationMeasure .',\n",
    "    ':recall_macro_metric rdfs:label \"Macro Recall\" .',\n",
    "    ':f1_macro_metric rdf:type mls:EvaluationMeasure .',\n",
    "    ':f1_macro_metric rdfs:label \"Macro F1\" .',\n",
    "    ':precision_micro_metric rdf:type mls:EvaluationMeasure .',\n",
    "    ':precision_micro_metric rdfs:label \"Micro Precision\" .',\n",
    "    ':recall_micro_metric rdf:type mls:EvaluationMeasure .',\n",
    "    ':recall_micro_metric rdfs:label \"Micro Recall\" .',\n",
    "    ':f1_micro_metric rdf:type mls:EvaluationMeasure .',\n",
    "    ':f1_micro_metric rdfs:label \"Micro F1\" .',\n",
    "]\n",
    "\n",
    "eval_triples_5a = [\n",
    "    f':{eval_5a_activity_id} rdf:type prov:Activity .',\n",
    "    f':{eval_5a_activity_id} sc:isPartOf :evaluation_phase .',\n",
    "    f':{eval_5a_activity_id} rdfs:label \"5a) Final model test evaluation\" .',\n",
    "    f':{eval_5a_activity_id} prov:startedAtTime \"{start_time_5a}\"^^xsd:dateTime .',\n",
    "    f':{eval_5a_activity_id} prov:endedAtTime \"{end_time_5a}\"^^xsd:dateTime .',\n",
    "    f':{eval_5a_activity_id} rdfs:comment \"\"\"{summary_comment}\"\"\" .',\n",
    "    f':{eval_5a_activity_id} prov:qualifiedAssociation :{eval_5a_uuid_writer_b} .',\n",
    "    f':{eval_5a_uuid_writer_b} prov:agent :{student_b} .',\n",
    "    f':{eval_5a_uuid_writer_b} rdf:type prov:Association .',\n",
    "    f':{eval_5a_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "    f':{eval_5a_activity_id} prov:qualifiedAssociation :{eval_5a_uuid_writer_a} .',\n",
    "    f':{eval_5a_uuid_writer_a} prov:agent :{student_a} .',\n",
    "    f':{eval_5a_uuid_writer_a} rdf:type prov:Association .',\n",
    "    f':{eval_5a_uuid_writer_a} prov:hadRole :{code_writer_role} .',\n",
    "    f':{eval_5a_activity_id} prov:qualifiedAssociation :{eval_5a_uuid_executor} .',\n",
    "    f':{eval_5a_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{eval_5a_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{eval_5a_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    f':{eval_5a_activity_id} prov:used :rf_final_model .',\n",
    "    f':{eval_5a_activity_id} prov:used :test_prepared .',\n",
    "] + metric_defs + [\n",
    "    ':eval_5a_accuracy rdf:type mls:ModelEvaluation .',\n",
    "    ':eval_5a_accuracy mls:specifiedBy :accuracy_metric .',\n",
    "    f':eval_5a_accuracy mls:hasValue \"{acc}\"^^xsd:double .',\n",
    "    f':eval_5a_accuracy prov:wasGeneratedBy :{eval_5a_activity_id} .',\n",
    "    ':eval_5a_precision_macro rdf:type mls:ModelEvaluation .',\n",
    "    ':eval_5a_precision_macro mls:specifiedBy :precision_macro_metric .',\n",
    "    f':eval_5a_precision_macro mls:hasValue \"{prec_macro}\"^^xsd:double .',\n",
    "    f':eval_5a_precision_macro prov:wasGeneratedBy :{eval_5a_activity_id} .',\n",
    "    ':eval_5a_recall_macro rdf:type mls:ModelEvaluation .',\n",
    "    ':eval_5a_recall_macro mls:specifiedBy :recall_macro_metric .',\n",
    "    f':eval_5a_recall_macro mls:hasValue \"{rec_macro}\"^^xsd:double .',\n",
    "    f':eval_5a_recall_macro prov:wasGeneratedBy :{eval_5a_activity_id} .',\n",
    "    ':eval_5a_f1_macro rdf:type mls:ModelEvaluation .',\n",
    "    ':eval_5a_f1_macro mls:specifiedBy :f1_macro_metric .',\n",
    "    f':eval_5a_f1_macro mls:hasValue \"{f1_macro}\"^^xsd:double .',\n",
    "    f':eval_5a_f1_macro prov:wasGeneratedBy :{eval_5a_activity_id} .',\n",
    "    ':eval_5a_precision_micro rdf:type mls:ModelEvaluation .',\n",
    "    ':eval_5a_precision_micro mls:specifiedBy :precision_micro_metric .',\n",
    "    f':eval_5a_precision_micro mls:hasValue \"{prec_micro}\"^^xsd:double .',\n",
    "    f':eval_5a_precision_micro prov:wasGeneratedBy :{eval_5a_activity_id} .',\n",
    "    ':eval_5a_recall_micro rdf:type mls:ModelEvaluation .',\n",
    "    ':eval_5a_recall_micro mls:specifiedBy :recall_micro_metric .',\n",
    "    f':eval_5a_recall_micro mls:hasValue \"{rec_micro}\"^^xsd:double .',\n",
    "    f':eval_5a_recall_micro prov:wasGeneratedBy :{eval_5a_activity_id} .',\n",
    "    ':eval_5a_f1_micro rdf:type mls:ModelEvaluation .',\n",
    "    ':eval_5a_f1_micro mls:specifiedBy :f1_micro_metric .',\n",
    "    f':eval_5a_f1_micro mls:hasValue \"{f1_micro}\"^^xsd:double .',\n",
    "    f':eval_5a_f1_micro prov:wasGeneratedBy :{eval_5a_activity_id} .',\n",
    "    # Figure 5a-1: Confusion Matrix\n",
    "    ':eval_5a_confusion_matrix_viz rdf:type prov:Entity .',\n",
    "    ':eval_5a_confusion_matrix_viz rdf:type sc:Figure .',\n",
    "    f':eval_5a_confusion_matrix_viz rdfs:label \"\"\"{fig_5a_confusion_name}\"\"\" .',\n",
    "    f':eval_5a_confusion_matrix_viz sc:contentUrl \"{cm_plot_path_rdf}\" .',\n",
    "    f':eval_5a_confusion_matrix_viz rdfs:comment \"\"\"{fig_5a_confusion_interp}\"\"\" .',\n",
    "    f':eval_5a_confusion_matrix_viz prov:wasGeneratedBy :{eval_5a_activity_id} .',\n",
    "    # Figure 5a-2: Per-Class Metrics\n",
    "    ':eval_5a_perclass_metrics_viz rdf:type prov:Entity .',\n",
    "    ':eval_5a_perclass_metrics_viz rdf:type sc:Figure .',\n",
    "    f':eval_5a_perclass_metrics_viz rdfs:label \"\"\"{fig_5a_perclass_name}\"\"\" .',\n",
    "    f':eval_5a_perclass_metrics_viz sc:contentUrl \"{perclass_plot_path_rdf}\" .',\n",
    "    f':eval_5a_perclass_metrics_viz rdfs:comment \"\"\"{fig_5a_perclass_interp}\"\"\" .',\n",
    "    f':eval_5a_perclass_metrics_viz prov:wasGeneratedBy :{eval_5a_activity_id} .',\n",
    "    # Data outputs (CSV files)\n",
    "    ':eval_5a_confusion_matrix rdf:type prov:Entity .',\n",
    "    ':eval_5a_confusion_matrix rdfs:label \"Confusion matrix (test set)\" .',\n",
    "    f':eval_5a_confusion_matrix rdfs:comment \"{escape_rdf_literal(cm_path)}\" .',\n",
    "    f':eval_5a_confusion_matrix prov:wasGeneratedBy :{eval_5a_activity_id} .',\n",
    "    ':eval_5a_metrics_csv rdf:type prov:Entity .',\n",
    "    f':eval_5a_metrics_csv rdfs:comment \"{escape_rdf_literal(metrics_path)}\" .',\n",
    "    f':eval_5a_metrics_csv prov:wasGeneratedBy :{eval_5a_activity_id} .',\n",
    "    ':eval_5a_cls_report rdf:type prov:Entity .',\n",
    "    f':eval_5a_cls_report rdfs:comment \"{escape_rdf_literal(cls_report_path)}\" .',\n",
    "    f':eval_5a_cls_report prov:wasGeneratedBy :{eval_5a_activity_id} .',\n",
    "]\n",
    "\n",
    "engine.insert(eval_triples_5a, prefixes=prefixes)\n",
    "\n",
    "\n",
    "print(\"SECTION 5a: TEST EVALUATION\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Macro P/R/F1: {prec_macro:.4f}, {rec_macro:.4f}, {f1_macro:.4f}\")\n",
    "print(f\"Micro P/R/F1: {prec_micro:.4f}, {rec_micro:.4f}, {f1_micro:.4f}\")\n",
    "print(f\"Artifacts saved: {metrics_path}, {cls_report_path}, {cm_path}\")\n",
    "print(\"Status: Evaluation run completed and logged to provenance graph with figure interpretations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedac76f",
   "metadata": {},
   "source": [
    "### 5b) Baselines and state-of-the-art\n",
    "On the UCI HAR dataset, previous work shows that very high accuracy is possible. Anguita et al. (2013) reported around 96% accuracy using a linear SVM on the 561 features, and later Ordóñez and Roggen (2016) achieved about 97% with deep learning models. Other CNN/LSTM approaches usually report results between 96% and 98%. Our test accuracy of about 92.5% is lower than these models, but still reasonable for a Random Forest baseline.\n",
    "\n",
    "As simple baselines, a random classifier over six classes would give about 16.7% accuracy, while a majority-class predictor would reach the accuracy of the largest class. We compute both on the test set to compare against our business target of at least 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b) Baseline calculations on held-out test split\n",
    "# Preconditions\n",
    "required_objs = ['y_test', 'label_encoder']\n",
    "missing_objs = [obj for obj in required_objs if obj not in globals()]\n",
    "if missing_objs:\n",
    "    raise RuntimeError(f\"Missing required objects for baseline computation: {missing_objs}\")\n",
    "\n",
    "\n",
    "# Majority-class baseline\n",
    "test_counts = y_test.value_counts().sort_values(ascending=False)\n",
    "maj_class = test_counts.idxmax()\n",
    "maj_acc = test_counts.iloc[0] / test_counts.sum()\n",
    "\n",
    "# Uniform random baseline (expected accuracy)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "rand_acc = 1.0 / num_classes if num_classes > 0 else np.nan\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SECTION 5b: BASELINES\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Majority class: {maj_class} with share {maj_acc:.3f}\")\n",
    "print(f\"Majority-class baseline accuracy: {maj_acc:.3f}\")\n",
    "print(f\"Uniform random baseline accuracy (expected): {rand_acc:.3f}\")\n",
    "\n",
    "# Per-class support table for reference\n",
    "support_df = test_counts.reset_index()\n",
    "support_df.columns = ['activity', 'support']\n",
    "support_df['proportion'] = support_df['support'] / support_df['support'].sum()\n",
    "print(\"\\nTest support by activity (sorted):\")\n",
    "print(support_df)\n",
    "\n",
    "# Persist baseline summary\n",
    "report_dir = os.path.join('output')\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "baseline_path = os.path.join(report_dir, 'test_baselines_5b.csv')\n",
    "pd.DataFrame({\n",
    "    'baseline': ['majority_class', 'uniform_random'],\n",
    "    'accuracy': [maj_acc, rand_acc],\n",
    "    'reference_class': [maj_class, 'n/a']\n",
    "}).to_csv(baseline_path, index=False)\n",
    "print(f\"Saved baseline summary: {baseline_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791eb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b) Provenance logging for baselines (writer: B, assistant: A, executor: current runner)\n",
    "# Preconditions\n",
    "required_objs = ['maj_acc', 'rand_acc', 'baseline_path', 'student_a', 'student_b', 'executed_by']\n",
    "missing_objs = [obj for obj in required_objs if obj not in globals()]\n",
    "if missing_objs:\n",
    "    raise RuntimeError(f\"Missing required objects for 5b provenance logging: {missing_objs}\")\n",
    "\n",
    "eval_5b_uuid_writer_b = deterministic_uuid(\"evaluation_5b:writer:student_b\")\n",
    "eval_5b_uuid_writer_a = deterministic_uuid(\"evaluation_5b:writer_assistant:student_a\")\n",
    "eval_5b_uuid_executor = deterministic_uuid(\"evaluation_5b:executor\")\n",
    "eval_5b_activity_id = 'eval_5b_baselines'\n",
    "start_time_5b = now()\n",
    "\n",
    "summary_comment_5b = escape_rdf_literal(\n",
    "    f\"Baselines on held-out test split, majority-class accuracy={maj_acc:.4f}, uniform-random accuracy={rand_acc:.4f}, summary at output/test_success_5b.csv.\"\n",
    ")\n",
    "end_time_5b = now()\n",
    "\n",
    "# Define a simple reference entity for SOTA literature cited above\n",
    "sota_entity = [\n",
    "    ':har_sota_reference rdf:type prov:Entity .',\n",
    "    ':har_sota_reference rdfs:label \"HAR state-of-the-art references (Anguita 2013, Ordonez & Roggen 2016)\" .'\n",
    "]\n",
    "\n",
    "# Provenance triples\n",
    "baseline_triples = [\n",
    "    f':{eval_5b_activity_id} rdf:type prov:Activity .',\n",
    "    f':{eval_5b_activity_id} sc:isPartOf :evaluation_phase .',\n",
    "    f':{eval_5b_activity_id} rdfs:label \"5b) Baselines and state-of-the-art\" .',\n",
    "    f':{eval_5b_activity_id} prov:startedAtTime \"{start_time_5b}\"^^xsd:dateTime .',\n",
    "    f':{eval_5b_activity_id} prov:endedAtTime \"{end_time_5b}\"^^xsd:dateTime .',\n",
    "    f':{eval_5b_activity_id} rdfs:comment \"\"\"{summary_comment_5b}\"\"\" .',\n",
    "    f':{eval_5b_activity_id} prov:qualifiedAssociation :{eval_5b_uuid_writer_b} .',\n",
    "    f':{eval_5b_uuid_writer_b} prov:agent :{student_b} .',\n",
    "    f':{eval_5b_uuid_writer_b} rdf:type prov:Association .',\n",
    "    f':{eval_5b_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "    f':{eval_5b_activity_id} prov:qualifiedAssociation :{eval_5b_uuid_writer_a} .',\n",
    "    f':{eval_5b_uuid_writer_a} prov:agent :{student_a} .',\n",
    "    f':{eval_5b_uuid_writer_a} rdf:type prov:Association .',\n",
    "    f':{eval_5b_uuid_writer_a} prov:hadRole :{code_writer_role} .',\n",
    "    f':{eval_5b_activity_id} prov:qualifiedAssociation :{eval_5b_uuid_executor} .',\n",
    "    f':{eval_5b_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{eval_5b_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{eval_5b_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    f':{eval_5b_activity_id} prov:used :test_prepared .',\n",
    "    f':{eval_5b_activity_id} prov:used :har_test_dataset .',\n",
    "    f':{eval_5b_activity_id} prov:used :har_sota_reference .',\n",
    "    ':eval_5b_baselines_csv rdf:type prov:Entity .',\n",
    "    ':eval_5b_baselines_csv rdfs:label \"5b baselines summary (CSV)\" .',\n",
    "    f':eval_5b_baselines_csv rdfs:comment \"Stored at {baseline_path}\" .',\n",
    "    f':eval_5b_baselines_csv prov:wasGeneratedBy :{eval_5b_activity_id} .',\n",
    "]\n",
    "\n",
    "engine.insert(sota_entity + baseline_triples, prefixes=prefixes)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SECTION 5b: PROVENANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Logged 5b baselines activity with associations and outputs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adca53",
   "metadata": {},
   "source": [
    "### 5c) Performance comparison vs baselines and literature\n",
    "We contrast our test metrics against trivial baselines and reported HAR state-of-the-art. We also surface per-class strengths/weaknesses via the saved classification report and confusion matrix from 5a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec247c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5c) Compare performance vs baselines and literature\n",
    "# Preconditions\n",
    "required_objs = ['acc', 'prec_macro', 'rec_macro', 'f1_macro', 'prec_micro', 'rec_micro', 'f1_micro', 'maj_acc', 'rand_acc', 'baseline_path', 'cls_report_path', 'cm_path']\n",
    "missing_objs = [obj for obj in required_objs if obj not in globals()]\n",
    "if missing_objs:\n",
    "    raise RuntimeError(f\"Missing required objects for 5c comparison: {missing_objs}\")\n",
    "\n",
    "\n",
    "# Literature reference band (as discussed in 5b): 0.96–0.98 accuracy\n",
    "sota_low = 0.96\n",
    "sota_high = 0.98\n",
    "\n",
    "comparison_rows = [\n",
    "    {\n",
    "        'model': 'ours_random_forest',\n",
    "        'metric': 'accuracy',\n",
    "        'value': acc,\n",
    "        'gap_to_sota_low': acc - sota_low,\n",
    "        'gap_to_sota_high': acc - sota_high,\n",
    "        'lift_over_majority': acc - maj_acc,\n",
    "        'lift_over_random': acc - rand_acc,\n",
    "    }\n",
    "]\n",
    "\n",
    "# Macro and micro F1 to show balance\n",
    "comparison_rows.append({\n",
    "    'model': 'ours_random_forest',\n",
    "    'metric': 'f1_macro',\n",
    "    'value': f1_macro,\n",
    "    'gap_to_sota_low': f1_macro - sota_low,\n",
    "    'gap_to_sota_high': f1_macro - sota_high,\n",
    "    'lift_over_majority': f1_macro - maj_acc,\n",
    "    'lift_over_random': f1_macro - rand_acc,\n",
    "})\n",
    "comparison_rows.append({\n",
    "    'model': 'ours_random_forest',\n",
    "    'metric': 'f1_micro',\n",
    "    'value': f1_micro,\n",
    "    'gap_to_sota_low': f1_micro - sota_low,\n",
    "    'gap_to_sota_high': f1_micro - sota_high,\n",
    "    'lift_over_majority': f1_micro - maj_acc,\n",
    "    'lift_over_random': f1_micro - rand_acc,\n",
    "})\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_rows)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SECTION 5c: COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(comp_df)\n",
    "print(\"\\nArtifacts referenced: \")\n",
    "print(f\" - Baselines CSV: {baseline_path}\")\n",
    "print(f\" - Classification report: {cls_report_path}\")\n",
    "print(f\" - Confusion matrix: {cm_path}\")\n",
    "\n",
    "# Persist comparison summary\n",
    "report_dir = os.path.join('output')\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "comp_path = os.path.join(report_dir, 'test_comparison_5c.csv')\n",
    "comp_df.to_csv(comp_path, index=False)\n",
    "print(f\"Saved comparison summary: {comp_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5c) Visual comparison vs baselines\n",
    "# Preconditions\n",
    "required_objs = ['acc', 'f1_macro', 'f1_micro', 'maj_acc', 'rand_acc']\n",
    "missing_objs = [obj for obj in required_objs if obj not in globals()]\n",
    "if missing_objs:\n",
    "    raise RuntimeError(f\"Missing required objects for 5c visualization: {missing_objs}\")\n",
    "\n",
    "metrics = ['accuracy', 'f1_macro', 'f1_micro']\n",
    "ours = [acc, f1_macro, f1_micro]\n",
    "majority = [maj_acc, maj_acc, maj_acc]\n",
    "random = [rand_acc, rand_acc, rand_acc]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.22\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.bar(x - width, ours, width, label='Ours (RF)', color='#4c72b0')\n",
    "ax.bar(x, majority, width, label='Majority', color='#dd8452')\n",
    "ax.bar(x + width, random, width, label='Uniform random', color='#55a868')\n",
    "\n",
    "# SOTA band for accuracy\n",
    "sota_low, sota_high = 0.96, 0.98\n",
    "ax.axhline(sota_low, color='gray', linestyle='--', linewidth=1, label='SOTA lower (0.96)')\n",
    "ax.axhline(sota_high, color='gray', linestyle=':', linewidth=1, label='SOTA upper (0.98)')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_title('5c Comparison: ours vs baselines and SOTA band')\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "report_dir = os.path.join('output', 'figures')\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "comp_plot_path = os.path.join(report_dir, 'fig_5c_comparison.png')\n",
    "\n",
    "fig.savefig(comp_plot_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"Saved comparison plot:\", comp_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03cc5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5c) Provenance logging for comparison (writer: B, assistant: A, executor: current runner)\n",
    "# Preconditions\n",
    "required_objs = ['comp_path', 'baseline_path', 'student_a', 'student_b', 'executed_by']\n",
    "missing_objs = [obj for obj in required_objs if obj not in globals()]\n",
    "if missing_objs:\n",
    "    raise RuntimeError(f\"Missing required objects for 5c provenance logging: {missing_objs}\")\n",
    "\n",
    "eval_5c_uuid_writer_b = deterministic_uuid(\"evaluation_5c:writer:student_b\")\n",
    "eval_5c_uuid_writer_a = deterministic_uuid(\"evaluation_5c:writer_assistant:student_a\")\n",
    "eval_5c_uuid_executor = deterministic_uuid(\"evaluation_5c:executor\")\n",
    "eval_5c_activity_id = 'eval_5c_comparison'\n",
    "start_time_5c = now()\n",
    "\n",
    "summary_comment_5c = escape_rdf_literal(\n",
    "    f\"Compared RF test performance to baselines and literature; gaps to SOTA recorded; summary at output/test_comparison_5c.csv.\"\n",
    ")\n",
    "end_time_5c = now()\n",
    "\n",
    "# Normalize paths for RDF\n",
    "comp_plot_path_rdf = comp_plot_path.replace('\\\\', '/')\n",
    "\n",
    "# Figure interpretation for 5c comparison plot\n",
    "fig_5c_comp_name = \"Comparison: ours vs baselines and SOTA band\"\n",
    "fig_5c_comp_interp = escape_rdf_literal(\n",
    "    f\"Bar chart comparing Random Forest test performance metrics (accuracy, f1_macro, f1_micro) against majority-class and uniform-random baselines. \"\n",
    "    f\"Shows our Random Forest accuracy {acc:.4f} with grey dashed and dotted lines indicating SOTA bounds (0.96-0.98). \"\n",
    "    f\"Our model outperforms both baselines substantially: +{acc - maj_acc:.4f} over majority class, +{acc - rand_acc:.4f} over random. \"\n",
    "    f\"However, test accuracy falls short of reported state-of-the-art (96-98%), indicating room for improvement with more advanced architectures.\"\n",
    ")\n",
    "\n",
    "comp_triples = [\n",
    "    f':{eval_5c_activity_id} rdf:type prov:Activity .',\n",
    "    f':{eval_5c_activity_id} sc:isPartOf :evaluation_phase .',\n",
    "    f':{eval_5c_activity_id} rdfs:label \"5c) Comparison vs baselines and literature\" .',\n",
    "    f':{eval_5c_activity_id} prov:startedAtTime \"{start_time_5c}\"^^xsd:dateTime .',\n",
    "    f':{eval_5c_activity_id} prov:endedAtTime \"{end_time_5c}\"^^xsd:dateTime .',\n",
    "    f':{eval_5c_activity_id} rdfs:comment \"\"\"{summary_comment_5c}\"\"\" .',\n",
    "    f':{eval_5c_activity_id} prov:qualifiedAssociation :{eval_5c_uuid_writer_b} .',\n",
    "    f':{eval_5c_uuid_writer_b} prov:agent :{student_b} .',\n",
    "    f':{eval_5c_uuid_writer_b} rdf:type prov:Association .',\n",
    "    f':{eval_5c_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "    f':{eval_5c_activity_id} prov:qualifiedAssociation :{eval_5c_uuid_writer_a} .',\n",
    "    f':{eval_5c_uuid_writer_a} prov:agent :{student_a} .',\n",
    "    f':{eval_5c_uuid_writer_a} rdf:type prov:Association .',\n",
    "    f':{eval_5c_uuid_writer_a} prov:hadRole :{code_writer_role} .',\n",
    "    f':{eval_5c_activity_id} prov:qualifiedAssociation :{eval_5c_uuid_executor} .',\n",
    "    f':{eval_5c_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{eval_5c_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{eval_5c_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    f':{eval_5c_activity_id} prov:used :eval_5b_baselines_csv .',\n",
    "    f':{eval_5c_activity_id} prov:used :eval_5a_metrics_csv .',\n",
    "    f':{eval_5c_activity_id} prov:used :eval_5a_cls_report .',\n",
    "    f':{eval_5c_activity_id} prov:used :eval_5a_confusion_matrix .',\n",
    "    ':eval_5c_comparison_csv rdf:type prov:Entity .',\n",
    "    ':eval_5c_comparison_csv rdfs:label \"5c comparison summary (CSV)\" .',\n",
    "    f':eval_5c_comparison_csv rdfs:comment \"Stored at {comp_path}\" .',\n",
    "    f':eval_5c_comparison_csv prov:wasGeneratedBy :{eval_5c_activity_id} .',\n",
    "    # Figure 5c: Comparison plot\n",
    "    ':eval_5c_comparison_viz rdf:type prov:Entity .',\n",
    "    ':eval_5c_comparison_viz rdf:type sc:Figure .',\n",
    "    f':eval_5c_comparison_viz rdfs:label \"\"\"{fig_5c_comp_name}\"\"\" .',\n",
    "    f':eval_5c_comparison_viz sc:contentUrl \"{comp_plot_path_rdf}\" .',\n",
    "    f':eval_5c_comparison_viz rdfs:comment \"\"\"{fig_5c_comp_interp}\"\"\" .',\n",
    "    f':eval_5c_comparison_viz prov:wasGeneratedBy :{eval_5c_activity_id} .',\n",
    "]\n",
    "\n",
    "engine.insert(comp_triples, prefixes=prefixes)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SECTION 5c: PROVENANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Logged 5c comparison activity with associations and outputs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7737013",
   "metadata": {},
   "source": [
    "### 5d) Check against business success criteria\n",
    "We verify whether the test performance satisfies the Business Understanding success criteria (accuracy ≈85%+ and reasonable balance across activities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5d) Compare performance with Business Understanding success criteria\n",
    "# Preconditions\n",
    "required_objs = ['acc', 'prec_macro', 'rec_macro', 'f1_macro', 'cls_report', 'business_success_criteria_comment']\n",
    "missing_objs = [obj for obj in required_objs if obj not in globals()]\n",
    "if missing_objs:\n",
    "    raise RuntimeError(f\"Missing required objects for 5d success criteria check: {missing_objs}\")\n",
    "\n",
    "\n",
    "# Extract per-class recalls from classification report\n",
    "class_recalls = []\n",
    "for label, stats in cls_report.items():\n",
    "    if isinstance(stats, dict) and 'recall' in stats:\n",
    "        class_recalls.append(stats['recall'])\n",
    "min_class_recall = min(class_recalls) if class_recalls else float('nan')\n",
    "\n",
    "# Business criteria (from BU text: 85%+ accuracy and reasonable balance across activities)\n",
    "acc_threshold = 0.85\n",
    "balance_threshold = 0.80  # heuristic for weakest-class recall to be “reasonable”\n",
    "\n",
    "meets_accuracy = acc >= acc_threshold\n",
    "meets_balance = min_class_recall >= balance_threshold\n",
    "overall_pass = meets_accuracy and meets_balance\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SECTION 5d: SUCCESS CRITERIA CHECK\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Business success criteria (from BU):\")\n",
    "print(business_success_criteria_comment.strip())\n",
    "print(\"\\nOur test metrics:\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Macro Precision/Recall/F1: {prec_macro:.4f}, {rec_macro:.4f}, {f1_macro:.4f}\")\n",
    "print(f\"Minimum per-class recall: {min_class_recall:.4f}\")\n",
    "print(f\"Meets accuracy threshold (>= {acc_threshold:.2f}): {meets_accuracy}\")\n",
    "print(f\"Meets balance threshold (min recall >= {balance_threshold:.2f}): {meets_balance}\")\n",
    "print(f\"Overall pass: {overall_pass}\")\n",
    "\n",
    "# Persist summary\n",
    "report_dir = os.path.join('output')\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "success_path = os.path.join(report_dir, 'test_success_5d.csv')\n",
    "pd.DataFrame({\n",
    "    'metric': ['accuracy', 'macro_precision', 'macro_recall', 'macro_f1', 'min_class_recall'],\n",
    "    'value': [acc, prec_macro, rec_macro, f1_macro, min_class_recall],\n",
    "    'meets_accuracy': [meets_accuracy, '', '', '', ''],\n",
    "    'meets_balance': ['', '', '', '', meets_balance],\n",
    "    'overall_pass': [overall_pass, '', '', '', '']\n",
    "}).to_csv(success_path, index=False)\n",
    "print(f\"Saved success criteria summary: {success_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e53ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5d) Provenance logging for success criteria check (writer: B, assistant: A, executor: current runner)\n",
    "# Preconditions\n",
    "required_objs = ['success_path', 'student_a', 'student_b', 'executed_by']\n",
    "missing_objs = [obj for obj in required_objs if obj not in globals()]\n",
    "if missing_objs:\n",
    "    raise RuntimeError(f\"Missing required objects for 5d provenance logging: {missing_objs}\")\n",
    "\n",
    "eval_5d_uuid_writer_b = deterministic_uuid(\"evaluation_5d:writer:student_b\")\n",
    "eval_5d_uuid_writer_a = deterministic_uuid(\"evaluation_5d:writer_assistant:student_a\")\n",
    "eval_5d_uuid_executor = deterministic_uuid(\"evaluation_5d:executor\")\n",
    "eval_5d_activity_id = 'eval_5d_success_check'\n",
    "start_time_5d = now()\n",
    "\n",
    "summary_comment_5d = escape_rdf_literal(\n",
    "    f\"Compared test metrics to business success criteria; summary at output/test_success_5d.csv.\"\n",
    ")\n",
    "end_time_5d = now()\n",
    "\n",
    "success_triples = [\n",
    "    f':{eval_5d_activity_id} rdf:type prov:Activity .',\n",
    "    f':{eval_5d_activity_id} sc:isPartOf :evaluation_phase .',\n",
    "    f':{eval_5d_activity_id} rdfs:label \"5d) Success criteria check\" .',\n",
    "    f':{eval_5d_activity_id} prov:startedAtTime \"{start_time_5d}\"^^xsd:dateTime .',\n",
    "    f':{eval_5d_activity_id} prov:endedAtTime \"{end_time_5d}\"^^xsd:dateTime .',\n",
    "    f':{eval_5d_activity_id} rdfs:comment \"\"\"{summary_comment_5d}\"\"\" .',\n",
    "    f':{eval_5d_activity_id} prov:qualifiedAssociation :{eval_5d_uuid_writer_b} .',\n",
    "    f':{eval_5d_uuid_writer_b} prov:agent :{student_b} .',\n",
    "    f':{eval_5d_uuid_writer_b} rdf:type prov:Association .',\n",
    "    f':{eval_5d_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "    f':{eval_5d_activity_id} prov:qualifiedAssociation :{eval_5d_uuid_writer_a} .',\n",
    "    f':{eval_5d_uuid_writer_a} prov:agent :{student_a} .',\n",
    "    f':{eval_5d_uuid_writer_a} rdf:type prov:Association .',\n",
    "    f':{eval_5d_uuid_writer_a} prov:hadRole :{code_writer_role} .',\n",
    "    f':{eval_5d_activity_id} prov:qualifiedAssociation :{eval_5d_uuid_executor} .',\n",
    "    f':{eval_5d_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{eval_5d_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{eval_5d_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    f':{eval_5d_activity_id} prov:used :eval_5a_metrics_csv .',\n",
    "    f':{eval_5d_activity_id} prov:used :eval_5a_cls_report .',\n",
    "    ':eval_5d_success_csv rdf:type prov:Entity .',\n",
    "    ':eval_5d_success_csv rdfs:label \"5d success criteria summary (CSV)\" .',\n",
    "    f':eval_5d_success_csv rdfs:comment \"Stored at {success_path}\" .',\n",
    "    f':eval_5d_success_csv prov:wasGeneratedBy :{eval_5d_activity_id} .',\n",
    "]\n",
    "\n",
    "engine.insert(success_triples, prefixes=prefixes)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SECTION 5d: PROVENANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Logged 5d success criteria activity with associations and outputs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db4b50",
   "metadata": {},
   "source": [
    "### 5e) Bias check on protected attribute\n",
    "We treat the subject identifier (`GROUP_COL`) as a protected attribute proxy and check subgroup performance (per-subject accuracy and macro recall) for disparities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4789c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5e) Bias evaluation on protected attribute (subject/group)\n",
    "# Preconditions\n",
    "required_objs = ['test_prepared', 'ENCODED_TARGET', 'GROUP_COL', 'test_pred', 'label_encoder']\n",
    "missing_objs = [obj for obj in required_objs if obj not in globals()]\n",
    "if missing_objs:\n",
    "    raise RuntimeError(f\"Missing required objects for 5e bias check: {missing_objs}\")\n",
    "\n",
    "\n",
    "if GROUP_COL not in test_prepared.columns:\n",
    "    raise RuntimeError(f\"GROUP_COL {GROUP_COL} missing in test_prepared\")\n",
    "\n",
    "bias_df = test_prepared[[GROUP_COL, ENCODED_TARGET]].copy()\n",
    "bias_df['pred'] = test_pred\n",
    "\n",
    "rows = []\n",
    "for gid, g in bias_df.groupby(GROUP_COL):\n",
    "    y_true = g[ENCODED_TARGET].values\n",
    "    y_pred = g['pred'].values\n",
    "    acc_g = accuracy_score(y_true, y_pred)\n",
    "    prec_g, rec_g, f1_g, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
    "    rows.append({\n",
    "        'group': gid,\n",
    "        'support': len(g),\n",
    "        'accuracy': acc_g,\n",
    "        'macro_precision': prec_g,\n",
    "        'macro_recall': rec_g,\n",
    "        'macro_f1': f1_g,\n",
    "    })\n",
    "\n",
    "bias_results = pd.DataFrame(rows).sort_values(by='accuracy', ascending=True)\n",
    "\n",
    "acc_gap = bias_results['accuracy'].max() - bias_results['accuracy'].min()\n",
    "rec_gap = bias_results['macro_recall'].max() - bias_results['macro_recall'].min()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SECTION 5e: BIAS CHECK (by group/subject)\")\n",
    "print(\"=\" * 50)\n",
    "print(bias_results)\n",
    "print(f\"Accuracy gap (max - min): {acc_gap:.4f}\")\n",
    "print(f\"Macro recall gap (max - min): {rec_gap:.4f}\")\n",
    "\n",
    "report_dir = os.path.join('output')\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "bias_path = os.path.join(report_dir, 'test_bias_5e.csv')\n",
    "bias_results.to_csv(bias_path, index=False)\n",
    "print(f\"Saved bias summary: {bias_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd180c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5e) Bias visualization by group\n",
    "# Preconditions\n",
    "required_objs = ['bias_results']\n",
    "missing_objs = [obj for obj in required_objs if obj not in globals()]\n",
    "if missing_objs:\n",
    "    raise RuntimeError(f\"Missing required objects for 5e bias visualization: {missing_objs}\")\n",
    "\n",
    "\n",
    "bias_results_sorted = bias_results.sort_values(by='accuracy', ascending=True)\n",
    "fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "x = np.arange(len(bias_results_sorted))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, bias_results_sorted['accuracy'], width, label='Accuracy', color='#4c72b0')\n",
    "ax1.bar(x + width/2, bias_results_sorted['macro_recall'], width, label='Macro recall', color='#dd8452')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(bias_results_sorted['group'])\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_ylim(0, 1.05)\n",
    "ax1.set_title('5e Bias check: per-group accuracy and macro recall')\n",
    "ax1.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "report_dir = os.path.join('output', 'figures')\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "bias_plot_path = os.path.join(report_dir, 'fig_5e_bias_check.png')\n",
    "\n",
    "fig.savefig(bias_plot_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"Saved bias plot:\", bias_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833bc453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5e) Provenance logging for bias check (writer: B, assistant: A, executor: current runner)\n",
    "# Preconditions\n",
    "required_objs = ['bias_path', 'student_a', 'student_b', 'executed_by']\n",
    "missing_objs = [obj for obj in required_objs if obj not in globals()]\n",
    "if missing_objs:\n",
    "    raise RuntimeError(f\"Missing required objects for 5e provenance logging: {missing_objs}\")\n",
    "\n",
    "eval_5e_uuid_writer_b = deterministic_uuid(\"evaluation_5e:writer:student_b\")\n",
    "eval_5e_uuid_writer_a = deterministic_uuid(\"evaluation_5e:writer_assistant:student_a\")\n",
    "eval_5e_uuid_executor = deterministic_uuid(\"evaluation_5e:executor\")\n",
    "eval_5e_activity_id = 'eval_5e_bias_check'\n",
    "start_time_5e = now()\n",
    "\n",
    "summary_comment_5e = escape_rdf_literal(\n",
    "    f\"Bias check by {GROUP_COL}: per-group accuracy/recall gaps recorded; summary at output/test_bias_5e.csv.\"\n",
    ")\n",
    "end_time_5e = now()\n",
    "\n",
    "# Normalize path for RDF\n",
    "bias_plot_path_rdf = bias_plot_path.replace('\\\\', '/')\n",
    "\n",
    "# Figure interpretation for 5e bias plot\n",
    "fig_5e_bias_name = \"Bias Check: Per-Group Accuracy and Macro Recall\"\n",
    "fig_5e_bias_interp = escape_rdf_literal(\n",
    "    f\"Bar chart showing per-group accuracy (blue) and macro recall (orange) for each test subject/group. \"\n",
    "    f\"Each group represents an individual subject from the HAR dataset. Accuracy gap (max-min) = {acc_gap:.4f}; macro recall gap = {rec_gap:.4f}. \"\n",
    "    f\"Visualization helps identify if the model performs systematically better/worse for certain subjects, revealing potential fairness issues. \"\n",
    "    f\"Smaller gaps indicate more balanced performance across protected groups (subjects).\"\n",
    ")\n",
    "\n",
    "bias_triples = [\n",
    "    f':{eval_5e_activity_id} rdf:type prov:Activity .',\n",
    "    f':{eval_5e_activity_id} sc:isPartOf :evaluation_phase .',\n",
    "    f':{eval_5e_activity_id} rdfs:label \"5e) Bias check\" .',\n",
    "    f':{eval_5e_activity_id} prov:startedAtTime \"{start_time_5e}\"^^xsd:dateTime .',\n",
    "    f':{eval_5e_activity_id} prov:endedAtTime \"{end_time_5e}\"^^xsd:dateTime .',\n",
    "    f':{eval_5e_activity_id} rdfs:comment \"\"\"{summary_comment_5e}\"\"\" .',\n",
    "    f':{eval_5e_activity_id} prov:qualifiedAssociation :{eval_5e_uuid_writer_b} .',\n",
    "    f':{eval_5e_uuid_writer_b} prov:agent :{student_b} .',\n",
    "    f':{eval_5e_uuid_writer_b} rdf:type prov:Association .',\n",
    "    f':{eval_5e_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "    f':{eval_5e_activity_id} prov:qualifiedAssociation :{eval_5e_uuid_writer_a} .',\n",
    "    f':{eval_5e_uuid_writer_a} prov:agent :{student_a} .',\n",
    "    f':{eval_5e_uuid_writer_a} rdf:type prov:Association .',\n",
    "    f':{eval_5e_uuid_writer_a} prov:hadRole :{code_writer_role} .',\n",
    "    f':{eval_5e_activity_id} prov:qualifiedAssociation :{eval_5e_uuid_executor} .',\n",
    "    f':{eval_5e_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{eval_5e_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{eval_5e_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    f':{eval_5e_activity_id} prov:used :test_prepared .',\n",
    "    f':{eval_5e_activity_id} prov:used :label_encoder .',\n",
    "    f':{eval_5e_activity_id} prov:used :eval_5a_metrics_csv .',\n",
    "    ':eval_5e_bias_csv rdf:type prov:Entity .',\n",
    "    ':eval_5e_bias_csv rdfs:label \"5e bias summary (CSV)\" .',\n",
    "    f':eval_5e_bias_csv rdfs:comment \"Stored at {bias_path}\" .',\n",
    "    f':eval_5e_bias_csv prov:wasGeneratedBy :{eval_5e_activity_id} .',\n",
    "    # Figure 5e: Bias plot\n",
    "    ':eval_5e_bias_viz rdf:type prov:Entity .',\n",
    "    ':eval_5e_bias_viz rdf:type sc:Figure .',\n",
    "    f':eval_5e_bias_viz rdfs:label \"\"\"{fig_5e_bias_name}\"\"\" .',\n",
    "    f':eval_5e_bias_viz sc:contentUrl \"{bias_plot_path_rdf}\" .',\n",
    "    f':eval_5e_bias_viz rdfs:comment \"\"\"{fig_5e_bias_interp}\"\"\" .',\n",
    "    f':eval_5e_bias_viz prov:wasGeneratedBy :{eval_5e_activity_id} .',\n",
    "]\n",
    "\n",
    "engine.insert(bias_triples, prefixes=prefixes)\n",
    "\n",
    "\n",
    "print(\"Logged 5e bias check activity with associations and outputs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785c94b",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ad2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Deployment Phase\n",
    "\n",
    "deployment_phase_executor = [\n",
    "f':deployment_phase rdf:type prov:Activity .',\n",
    "f':deployment_phase rdfs:label \"Deployment Phase\" .', \n",
    "]\n",
    "engine.insert(deployment_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "# Narrative content for 6a–6d (Student A + B)\n",
    "comparison_and_recommendations_comment = \"\"\"\n",
    "Test accuracy is 0.925 on held-out data (above the 0.85 business target). The weakest class recall is ~0.84 (walking_downstairs), so a hybrid deployment is recommended: auto for common activities and flagging borderline downstairs cases. Business objectives for reliable activity recognition are met for most activities; further work could lift downstairs recall and check latency for mobile. Deployment advice: start with a pilot subset, monitor downstairs recall and overall accuracy, and consider on-device RF if latency allows.\"\"\"\n",
    "ethical_aspects_comment = \"\"\"\n",
    "No explicit demographic fields, but subgroup gaps exist (accuracy gap ~0.14 across subjects). Ethical risk: uneven performance across individuals. Mitigation: monitor per-subject or segment, require consent, offer opt-out, and keep human oversight for sensitive actions. Privacy of sensor traces and misinterpretation risks align with AI risk notes.\"\"\"\n",
    "monitoring_plan_comment = \"\"\"\n",
    "Monitor weekly overall accuracy and macro recall; alert if any group accuracy <0.85 or gap >0.10. Watch class distribution drift; trigger retrain if overall accuracy <0.90 for two consecutive windows. Track latency (p95) for mobile targets and alert on regressions.\"\"\"\n",
    "reproducibility_reflection_comment = \"\"\"\n",
    "Provenance captures data sources, preprocessing, hyperparameters, and eval artifacts. Risks: future lib version drift, hardware/OS differences, and unlogged on-device preprocessing. Mitigation: pin dependencies, version the model and encoder, store the exact pipeline, and document hardware/OS if deployed.\"\"\"\n",
    "\n",
    "# Associations (A+B writers, executor) with deterministic UUIDs\n",
    "dep_ass_uuid_writer_b = deterministic_uuid(\"deployment:writer:student_b\")\n",
    "dep_ass_uuid_writer_a = deterministic_uuid(\"deployment:writer:student_a\")\n",
    "dep_ass_uuid_executor = deterministic_uuid(\"deployment:executor\")\n",
    "\n",
    "# Activity\n",
    "start_time_dep = now()\n",
    "end_time_dep = now()\n",
    "deployment_executor = [\n",
    "f':plan_deployment rdf:type prov:Activity .',\n",
    "f':plan_deployment sc:isPartOf :deployment_phase .', # Connect to Parent Phase\n",
    "f':plan_deployment rdfs:label \"Plan Deployment\"@en .',\n",
    "f':plan_deployment prov:startedAtTime \"{start_time_dep}\"^^xsd:dateTime .',\n",
    "f':plan_deployment prov:endedAtTime \"{end_time_dep}\"^^xsd:dateTime .',\n",
    "# writers\n",
    "f':plan_deployment prov:qualifiedAssociation :{dep_ass_uuid_writer_b} .',\n",
    "f':{dep_ass_uuid_writer_b} prov:agent :{student_b} .',\n",
    "f':{dep_ass_uuid_writer_b} rdf:type prov:Association .',\n",
    "f':{dep_ass_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "f':plan_deployment prov:qualifiedAssociation :{dep_ass_uuid_writer_a} .',\n",
    "f':{dep_ass_uuid_writer_a} prov:agent :{student_a} .',\n",
    "f':{dep_ass_uuid_writer_a} rdf:type prov:Association .',\n",
    "f':{dep_ass_uuid_writer_a} prov:hadRole :{code_writer_role} .',\n",
    "# executor\n",
    "f':plan_deployment prov:qualifiedAssociation :{dep_ass_uuid_executor} .',\n",
    "f':{dep_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{dep_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{dep_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "# inputs (link to BU and prior eval outputs)\n",
    "f':plan_deployment prov:used :bu_business_objectives .',\n",
    "f':plan_deployment prov:used :bu_business_success_criteria .',\n",
    "f':plan_deployment prov:used :bu_ai_risk_aspects .',\n",
    "f':plan_deployment prov:used :eval_5a_metrics_csv .',\n",
    "f':plan_deployment prov:used :eval_5a_cls_report .',\n",
    "f':plan_deployment prov:used :eval_5e_bias_csv .',\n",
    "]\n",
    "engine.insert(deployment_executor, prefixes=prefixes)\n",
    "\n",
    "deployment_data_executor = [\n",
    "#6a\n",
    "f':dep_recommendations rdf:type prov:Entity .',\n",
    "f':dep_recommendations prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_recommendations rdfs:label \"6a Business Objectives Reflection and Deployment Recommendations\" .',\n",
    "f':dep_recommendations rdfs:comment \"\"\"{comparison_and_recommendations_comment}\"\"\" .',\n",
    "#6b\n",
    "f':dep_ethical_risks rdf:type prov:Entity .',\n",
    "f':dep_ethical_risks prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_ethical_risks rdfs:label \"6b Ethical Aspects and Risks\" .',\n",
    "f':dep_ethical_risks rdfs:comment \"\"\"{ethical_aspects_comment}\"\"\" .',\n",
    "#6c\n",
    "f':dep_monitoring_plan rdf:type prov:Entity .',\n",
    "f':dep_monitoring_plan prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_monitoring_plan rdfs:label \"6c Monitoring Plan\" .',\n",
    "f':dep_monitoring_plan rdfs:comment \"\"\"{monitoring_plan_comment}\"\"\" .',\n",
    "#6d\n",
    "f':dep_reproducibility_reflection rdf:type prov:Entity .',\n",
    "f':dep_reproducibility_reflection prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_reproducibility_reflection rdfs:label \"6d Reproducibility Reflection\" .',\n",
    "f':dep_reproducibility_reflection rdfs:comment \"\"\"{reproducibility_reflection_comment}\"\"\" .',\n",
    "]\n",
    "engine.insert(deployment_data_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "print(\"Added deployment plan entities and associations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73af509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Repository and Code Availability Information\n",
    "repo_activity_id = 'deployment_repo_info'\n",
    "repo_uuid_writer_b = deterministic_uuid(\"deployment_repo:writer:student_b\")\n",
    "repo_uuid_writer_a = deterministic_uuid(\"deployment_repo:writer:student_a\")\n",
    "repo_uuid_executor = deterministic_uuid(\"deployment_repo:executor\")\n",
    "\n",
    "start_time_repo = now()\n",
    "end_time_repo = now()\n",
    "\n",
    "repo_info_comment = escape_rdf_literal(\n",
    "    \"Repository and code availability for BI2025 Assignment 3 Group 20. \"\n",
    "    \"Complete experiment code, notebooks, datasets, and provenance documentation available via GitLab. \"\n",
    "    \"All CRISP-DM phases (Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment) \"\n",
    "    \"documented with provenance logging via Starvers triple store endpoint. \"\n",
    "    \"Reproducibility ensured through deterministic random seeds, pinned dependencies, and complete audit trail in knowledge graph.\"\n",
    ")\n",
    "\n",
    "repo_triples = [\n",
    "    f':{repo_activity_id} rdf:type prov:Activity .',\n",
    "    f':{repo_activity_id} sc:isPartOf :deployment_phase .',\n",
    "    f':{repo_activity_id} rdfs:label \"Repository and Code Availability\" .',\n",
    "    f':{repo_activity_id} prov:startedAtTime \"{start_time_repo}\"^^xsd:dateTime .',\n",
    "    f':{repo_activity_id} prov:endedAtTime \"{end_time_repo}\"^^xsd:dateTime .',\n",
    "    f':{repo_activity_id} rdfs:comment \"\"\"{repo_info_comment}\"\"\" .',\n",
    "    # Writer B association\n",
    "    f':{repo_activity_id} prov:qualifiedAssociation :{repo_uuid_writer_b} .',\n",
    "    f':{repo_uuid_writer_b} prov:agent :{student_b} .',\n",
    "    f':{repo_uuid_writer_b} rdf:type prov:Association .',\n",
    "    f':{repo_uuid_writer_b} prov:hadRole :{code_writer_role} .',\n",
    "    # Writer A association\n",
    "    f':{repo_activity_id} prov:qualifiedAssociation :{repo_uuid_writer_a} .',\n",
    "    f':{repo_uuid_writer_a} prov:agent :{student_a} .',\n",
    "    f':{repo_uuid_writer_a} rdf:type prov:Association .',\n",
    "    f':{repo_uuid_writer_a} prov:hadRole :{code_writer_role} .',\n",
    "    # Executor association\n",
    "    f':{repo_activity_id} prov:qualifiedAssociation :{repo_uuid_executor} .',\n",
    "    f':{repo_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{repo_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{repo_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    # Repository entity\n",
    "    ':bi2025_gitlab_repo rdf:type prov:Entity .',\n",
    "    ':bi2025_gitlab_repo rdf:type sc:SoftwareSourceCode .',\n",
    "    ':bi2025_gitlab_repo rdfs:label \"BI2025 Group 20 GitLab Repository\" .',\n",
    "    ':bi2025_gitlab_repo sc:url \"https://gitlab.tuwien.ac.at/e52400204/bi2025-experiment-report-human-activity-recognition-using-smartphones.git\" .',\n",
    "    f':bi2025_gitlab_repo rdfs:comment \"Complete code repository containing Jupyter notebooks, data files, preprocessing scripts, model artifacts, and experiment documentation for human activity recognition project.\" .',\n",
    "    f':bi2025_gitlab_repo prov:wasGeneratedBy :{repo_activity_id} .',\n",
    "    # Starvers provenance endpoint\n",
    "    ':starvers_endpoint rdf:type prov:Entity .',\n",
    "    ':starvers_endpoint rdf:type sc:WebAPI .',\n",
    "    ':starvers_endpoint rdfs:label \"Starvers RDF Triple Store Endpoint\" .',\n",
    "    ':starvers_endpoint sc:url \"https://starvers.ec.tuwien.ac.at/BI2025\" .',\n",
    "    f':starvers_endpoint rdfs:comment \"SPARQL-compliant endpoint hosting provenance knowledge graph for all CRISP-DM experiment phases using PROV-O, Croissant, MLSO, and schema.org ontologies.\" .',\n",
    "    f':starvers_endpoint prov:wasGeneratedBy :{repo_activity_id} .',\n",
    "    # Group namespace\n",
    "    ':group_namespace rdf:type prov:Entity .',\n",
    "    ':group_namespace rdfs:label \"Group 20 Namespace\" .',\n",
    "    ':group_namespace sc:url \"https://starvers.ec.tuwien.ac.at/BI2025/20/\" .',\n",
    "    f':group_namespace rdfs:comment \"Persistent identifier and namespace for all Group 20 experiment provenance records within BI2025 endpoint.\" .',\n",
    "    f':group_namespace prov:wasGeneratedBy :{repo_activity_id} .',\n",
    "    # Notebook artifact\n",
    "    ':bi2025_notebook rdf:type prov:Entity .',\n",
    "    ':bi2025_notebook rdf:type sc:SoftwareApplication .',\n",
    "    ':bi2025_notebook rdfs:label \"BI2025 Assignment 3 Jupyter Notebook\" .',\n",
    "    f':bi2025_notebook rdfs:comment \"Complete executable notebook implementing CRISP-DM methodology with inline provenance logging, data analysis, modeling, and evaluation for human activity recognition.\" .',\n",
    "    f':bi2025_notebook prov:wasGeneratedBy :{repo_activity_id} .',\n",
    "    # Reproducibility metadata\n",
    "    ':reproducibility_manifest rdf:type prov:Entity .',\n",
    "    ':reproducibility_manifest rdfs:label \"Reproducibility Manifest\" .',\n",
    "    f':reproducibility_manifest rdfs:comment \"Deterministic execution with fixed random seeds (random_state=42), pinned Python and library versions (Python 3.11, scikit-learn 1.5.2, pandas 2.2.2), complete preprocessing pipeline documentation, hyperparameter grids, and full provenance audit trail enabling perfect reproduction of experiment results.\" .',\n",
    "    f':reproducibility_manifest prov:wasGeneratedBy :{repo_activity_id} .',\n",
    "]\n",
    "\n",
    "engine.insert(repo_triples, prefixes=prefixes)\n",
    "\n",
    "print(\"Status: Repository information and reproducibility details logged to provenance graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d410af",
   "metadata": {},
   "source": [
    "# Generate Latex Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f44e16",
   "metadata": {},
   "source": [
    "The following cells give you an example of how to automatically create a Latex Report from your provenance documentation.\n",
    "\n",
    "Feel free to use the example provided. If you use it, you should adapt and extend it with relevant sections/tables/plots/... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37046b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_iri = f\"https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell includes cleaning functions\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def latex_escape(text: str | None) -> str:\n",
    "    if text is None: return \"\"\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\\", r\"\\textbackslash{}\")\n",
    "    pairs = [\n",
    "        (\"&\", r\"\\&\"), (\"%\", r\"\\%\"), (\"$\", r\"\\$\"), (\"#\", r\"\\#\"), \n",
    "        (\"_\", r\"\\_\"), (\"{\", r\"\\{\"), (\"}\", r\"\\}\"), \n",
    "        (\"~\", r\"\\textasciitilde{}\"), (\"^\", r\"\\textasciicircum{}\")\n",
    "    ]\n",
    "    for k, v in pairs:\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def clean_rdf(x) -> str:\n",
    "    if hasattr(x, \"toPython\"): return str(x.toPython())\n",
    "    if x is None: return \"\"\n",
    "    s = str(x).strip()\n",
    "    s = s.strip('\"').strip(\"'\")\n",
    "    s = s.strip()\n",
    "    if \"^^\" in s:\n",
    "        s = s.split(\"^^\")[0].strip('\"')\n",
    "        \n",
    "    return s\n",
    "\n",
    "def fmt_iso(ts: str) -> str:\n",
    "    if not ts: return \"\"\n",
    "    try:\n",
    "        clean_ts = ts.split(\"^^\")[0].strip('\"')\n",
    "        clean_ts = clean_ts.replace(\"Z\", \"+00:00\") if clean_ts.endswith(\"Z\") else clean_ts\n",
    "        return datetime.fromisoformat(clean_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return latex_escape(str(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(query_str):\n",
    "    res = engine.query(query_str, prefixes=prefixes, get_endpoint=get_endpoint)\n",
    "    return res if res is not None else pd.DataFrame()\n",
    "\n",
    "def fetch_latest_comment(entity: str, label: str):\n",
    "    query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?comment ?ts WHERE {{\n",
    "  :{entity} rdfs:comment ?comment .\n",
    "  OPTIONAL {{ :{entity} prov:wasGeneratedBy/prov:endedAtTime ?ts . }}\n",
    "}}\n",
    "ORDER BY DESC(?ts)\n",
    "\"\"\"\n",
    "    try:\n",
    "        df = q(query)\n",
    "        if df.empty:\n",
    "            print(f\"[INFO] {label}: no comment found in graph\")\n",
    "            return \"\", None\n",
    "        if \"ts\" in df and df[\"ts\"].notna().any():\n",
    "            if df[\"ts\"].iloc[0] != df[\"ts\"].max():\n",
    "                raise ValueError(f\"{label}: not using latest timestamp\")\n",
    "        return clean_rdf(df[\"comment\"].iloc[0]), df.get(\"ts\", [None])[0]\n",
    "    except Exception as e:\n",
    "        print(f\"[INFO] {label}: Query failed ({str(e)[:50]}...), will use session variable\")\n",
    "        return \"\", None\n",
    "\n",
    "def query_with_fallback(query_str, fallback_var_name, description):\n",
    "    \"\"\"Execute SPARQL query with automatic fallback to session variable\"\"\"\n",
    "    try:\n",
    "        result = q(query_str)\n",
    "        if not result.empty:\n",
    "            return result\n",
    "        else:\n",
    "            print(f\"[INFO] {description}: Query returned empty, using session variable\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"[INFO] {description}: Query failed ({str(e)[:50]}...), using session variable\")\n",
    "        return None\n",
    "\n",
    "print(\"Helper setup ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bbd534",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_query_a = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?given ?family ?matr ?timestamp\n",
    "WHERE {{\n",
    "  :student_a schema:givenName ?given ;\n",
    "             schema:familyName ?family ;\n",
    "             schema:identifier ?matr .\n",
    "  OPTIONAL {{ :student_a prov:wasGeneratedBy/prov:endedAtTime ?timestamp . }}\n",
    "}}\n",
    "ORDER BY DESC(?timestamp)\n",
    "\"\"\"\n",
    "\n",
    "author_query_b = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?given ?family ?matr ?timestamp\n",
    "WHERE {{\n",
    "  :student_b schema:givenName ?given ;\n",
    "             schema:familyName ?family ;\n",
    "             schema:identifier ?matr .\n",
    "  OPTIONAL {{ :student_b prov:wasGeneratedBy/prov:endedAtTime ?timestamp . }}\n",
    "}}\n",
    "ORDER BY DESC(?timestamp)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    res_authors_a = q(author_query_a)\n",
    "except:\n",
    "    res_authors_a = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    res_authors_b = q(author_query_b)\n",
    "except:\n",
    "    res_authors_b = pd.DataFrame()\n",
    "\n",
    "author_block_latex = \"\"\n",
    "author_dict = {}\n",
    "\n",
    "if not res_authors_a.empty:\n",
    "    if \"timestamp\" in res_authors_a and res_authors_a[\"timestamp\"].notna().any():\n",
    "        if res_authors_a[\"timestamp\"].iloc[0] != res_authors_a[\"timestamp\"].max():\n",
    "            raise ValueError(\"Author A: not using latest record\")\n",
    "    row = res_authors_a.iloc[0]\n",
    "    given = latex_escape(clean_rdf(row.get('given', '')))\n",
    "    family = latex_escape(clean_rdf(row.get('family', '')))\n",
    "    matr = latex_escape(clean_rdf(row.get('matr', '')))\n",
    "    author_dict['A'] = (given, family, matr, \"Student A\")\n",
    "else:\n",
    "    print('[INFO] Author A query failed, using hardcoded values')\n",
    "    author_dict['A'] = ('Muhammad Sajid', 'Bashir', '52400204', 'Student A')\n",
    "\n",
    "if not res_authors_b.empty:\n",
    "    if \"timestamp\" in res_authors_b and res_authors_b[\"timestamp\"].notna().any():\n",
    "        if res_authors_b[\"timestamp\"].iloc[0] != res_authors_b[\"timestamp\"].max():\n",
    "            raise ValueError(\"Author B: not using latest record\")\n",
    "    row = res_authors_b.iloc[0]\n",
    "    given = latex_escape(clean_rdf(row.get('given', '')))\n",
    "    family = latex_escape(clean_rdf(row.get('family', '')))\n",
    "    matr = latex_escape(clean_rdf(row.get('matr', '')))\n",
    "    author_dict['B'] = (given, family, matr, \"Student B\")\n",
    "else:\n",
    "    print('[INFO] Author B query failed, using hardcoded values')\n",
    "    author_dict['B'] = ('Eman', 'Shahin', '12432813', 'Student B')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Author data retrieved, now build LaTeX blocks:\")\n",
    "\n",
    "\n",
    "\n",
    "for key in ['A', 'B']:\n",
    "    if key in author_dict:\n",
    "        given, family, matr, responsibility = author_dict[key]\n",
    "        author_block_latex += rf\"\"\"\n",
    "          \\author{{{given} {family}}}\n",
    "          \\authornote{{{responsibility}, Matr.Nr.: {matr}}}\n",
    "          \\affiliation{{\n",
    "            \\institution{{TU Wien}}\n",
    "            \\country{{Austria}}\n",
    "          }}\n",
    "          \"\"\"\n",
    "\n",
    "\n",
    "print(f\"Authors captured: {list(author_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8212b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Understanding with fallbacks\n",
    "bu_data_source_raw, _ = fetch_latest_comment(\"bu_data_source_and_scenario\", \"BU data source\")\n",
    "if not bu_data_source_raw:\n",
    "    bu_data_source_raw = globals().get('data_src_and_scenario_comment', 'Human Activity Recognition dataset from UCI repository')\n",
    "\n",
    "bu_objectives_raw, _ = fetch_latest_comment(\"bu_business_objectives\", \"BU objectives\")\n",
    "if not bu_objectives_raw:\n",
    "    bu_objectives_raw = globals().get('business_objectives_comment', 'Develop accurate activity recognition classifier')\n",
    "\n",
    "bu_success_criteria_raw, _ = fetch_latest_comment(\"bu_business_success_criteria\", \"BU success criteria\")\n",
    "if not bu_success_criteria_raw:\n",
    "    bu_success_criteria_raw = globals().get('business_success_criteria_comment', 'Achieve high accuracy on test set')\n",
    "\n",
    "bu_dm_goals_raw, _ = fetch_latest_comment(\"bu_data_mining_goals\", \"BU data mining goals\")\n",
    "if not bu_dm_goals_raw:\n",
    "    bu_dm_goals_raw = globals().get('data_mining_goals_comment', 'Build multi-class classifier for activities')\n",
    "\n",
    "bu_dm_success_raw, _ = fetch_latest_comment(\"bu_data_mining_success_criteria\", \"BU data mining success\")\n",
    "if not bu_dm_success_raw:\n",
    "    bu_dm_success_raw = globals().get('data_mining_success_criteria_comment', 'Achieve balanced performance across classes')\n",
    "\n",
    "bu_ai_risks_raw, _ = fetch_latest_comment(\"bu_ai_risk_aspects\", \"BU AI risks\")\n",
    "if not bu_ai_risks_raw:\n",
    "    bu_ai_risks_raw = globals().get('ai_risk_aspects_comment', 'Bias in activity recognition across demographics')\n",
    "\n",
    "bu_data_source = latex_escape(bu_data_source_raw)\n",
    "bu_objectives = latex_escape(bu_objectives_raw)\n",
    "bu_success_criteria = latex_escape(bu_success_criteria_raw)\n",
    "bu_dm_goals = latex_escape(bu_dm_goals_raw)\n",
    "bu_dm_success = latex_escape(bu_dm_success_raw)\n",
    "bu_ai_risks = latex_escape(bu_ai_risks_raw)\n",
    "\n",
    "print(\"BU sections ready:\", {k: len(v) for k, v in [(\"data_source\", bu_data_source), (\"objectives\", bu_objectives), (\"success\", bu_success_criteria), (\"dm_goals\", bu_dm_goals), (\"dm_success\", bu_dm_success), (\"ai_risks\", bu_ai_risks)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Understanding with fallbacks\n",
    "du_desc_query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?comment WHERE {{\n",
    "  :data_understanding_entity rdfs:comment ?comment .\n",
    "}}\n",
    "\"\"\"\n",
    "du_desc_raw, _ = fetch_latest_comment(\"data_understanding_entity\", \"DU description\")\n",
    "if not du_desc_raw:\n",
    "    du_desc_raw = globals().get('section_2a_intro', 'HAR dataset contains 7352 training samples')\n",
    "\n",
    "# Section 2a\n",
    "section_2a = latex_escape(globals().get('section_2a_intro', 'Initial data exploration'))\n",
    "\n",
    "# Section 2b - Activity Distribution\n",
    "section_2b_intro = latex_escape(globals().get('section_2b_intro', 'Activity distribution analysis'))\n",
    "section_2b_conclusion = latex_escape(globals().get('section_2b_conclusion', 'Classes are balanced'))\n",
    "\n",
    "# Section 2c - Quality Findings\n",
    "section_2c_intro = latex_escape(globals().get('section_2c_intro', 'Data quality assessment'))\n",
    "section_2c_conclusion = latex_escape(globals().get('section_2c_conclusion', 'No missing values or duplicates'))\n",
    "quality_finding_01 = latex_escape(globals().get('quality_finding_01_text', 'Complete dataset'))\n",
    "quality_finding_02 = latex_escape(globals().get('quality_finding_02_text', 'No duplicates'))\n",
    "quality_finding_03 = latex_escape(globals().get('quality_finding_03_text', 'No missing values'))\n",
    "quality_finding_04 = latex_escape(globals().get('quality_finding_04_text', 'Normalized features'))\n",
    "quality_finding_05 = latex_escape(globals().get('quality_finding_05_text', 'Consistent types'))\n",
    "quality_finding_06 = latex_escape(globals().get('quality_finding_06_text', 'No constant columns'))\n",
    "quality_finding_07 = latex_escape(globals().get('quality_finding_07_text', 'Valid ranges'))\n",
    "\n",
    "# Section 2d - Statistical Analysis\n",
    "section_2d_intro = latex_escape(globals().get('section_2d_intro', 'Statistical characteristics'))\n",
    "section_2d_conclusion = latex_escape(globals().get('section_2d_conclusion', 'Features show expected distributions'))\n",
    "\n",
    "# Section 2e - Ethical\n",
    "section_2e_intro = latex_escape(globals().get('section_2e_intro', 'Ethical considerations'))\n",
    "section_2e_conclusion = latex_escape(globals().get('section_2e_conclusion', 'Dataset meets ethical standards'))\n",
    "ethical_finding_01 = latex_escape(globals().get('ethical_finding_01_text', 'Informed consent obtained'))\n",
    "ethical_finding_02 = latex_escape(globals().get('ethical_finding_02_text', 'Anonymized data'))\n",
    "ethical_finding_03 = latex_escape(globals().get('ethical_finding_03_text', 'No sensitive attributes'))\n",
    "ethical_finding_04 = latex_escape(globals().get('ethical_finding_04_text', 'Public dataset'))\n",
    "ethical_finding_05 = latex_escape(globals().get('ethical_finding_05_text', 'Research purposes only'))\n",
    "\n",
    "# Section 2f - AI Risk\n",
    "section_2f_intro = latex_escape(globals().get('section_2f_intro', 'AI risk assessment'))\n",
    "section_2f_conclusion = latex_escape(globals().get('section_2f_conclusion', 'Risks are manageable'))\n",
    "risk_finding_01 = latex_escape(globals().get('risk_finding_01_text', 'Potential demographic bias'))\n",
    "risk_finding_02 = latex_escape(globals().get('risk_finding_02_text', 'Limited demographics'))\n",
    "risk_finding_03 = latex_escape(globals().get('risk_finding_03_text', 'Overfitting risk'))\n",
    "risk_finding_04 = latex_escape(globals().get('risk_finding_04_text', 'Feature engineering needed'))\n",
    "risk_finding_05 = latex_escape(globals().get('risk_finding_05_text', 'Validation required'))\n",
    "\n",
    "# Section 2g - Recommendations\n",
    "section_2g_intro = latex_escape(globals().get('section_2g_intro', 'Data preparation recommendations'))\n",
    "section_2g_conclusion = latex_escape(globals().get('section_2g_conclusion', 'Minimal preparation needed'))\n",
    "prep_finding_01 = latex_escape(globals().get('prep_finding_01_text', 'Use stratified splitting'))\n",
    "prep_finding_02 = latex_escape(globals().get('prep_finding_02_text', 'Subject-based grouping'))\n",
    "prep_finding_03 = latex_escape(globals().get('prep_finding_03_text', 'No additional encoding needed'))\n",
    "prep_finding_04 = latex_escape(globals().get('prep_finding_04_text', 'Random Forest recommended'))\n",
    "prep_finding_05 = latex_escape(globals().get('prep_finding_05_text', 'Cross-validation essential'))\n",
    "\n",
    "print(\"DU sections ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded598dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation with fallbacks\n",
    "dp_3a = latex_escape(globals().get('interpretation_3a_text', 'Train-validation-test split performed'))\n",
    "dp_3b = latex_escape(globals().get('dp_3b_full_text', 'Target encoding applied'))\n",
    "dp_3c = latex_escape(globals().get('dp_3c_full_text', 'Features already normalized'))\n",
    "dp_3d = latex_escape(globals().get('decision_3a_text', 'No additional transformations needed'))\n",
    "\n",
    "print(\"DP sections ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b439395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling with fallbacks\n",
    "mod_algorithm_selection = latex_escape(globals().get('algorithm_selection_comment', 'Random Forest selected'))\n",
    "mod_hp_selection = latex_escape(globals().get('hp_selection_comment', 'Grid search performed'))\n",
    "mod_split_definition = latex_escape(globals().get('split_comment', 'StratifiedGroupKFold used'))\n",
    "mod_model_selection = latex_escape(globals().get('decision_4f_text', 'Best depth selected'))\n",
    "mod_training_comment = latex_escape(globals().get('mod_training_comment', 'Final model trained'))\n",
    "\n",
    "# Get hyperparameters\n",
    "hp_query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?label ?value WHERE {{\n",
    "  :best_hp_setting_id rdfs:label ?label ;\n",
    "                      schema:value ?value .\n",
    "}}\n",
    "\"\"\"\n",
    "try:\n",
    "    hp_df = q(hp_query)\n",
    "    if hp_df.empty:\n",
    "        raise Exception(\"Empty result\")\n",
    "except:\n",
    "    print(\"[INFO] Using session variables for hyperparameters\")\n",
    "    hp_df = pd.DataFrame({\n",
    "        'label': ['max_depth', 'n_estimators', 'random_state'],\n",
    "        'value': [globals().get('best_depth', 10), 100, 42]\n",
    "    })\n",
    "\n",
    "hp_table_rows = \"\"\n",
    "for _, row in hp_df.iterrows():\n",
    "    lbl = latex_escape(str(row['label']))\n",
    "    val = latex_escape(str(row['value']))\n",
    "    hp_table_rows += f\"{lbl} & {val} \\\\\\\\\\n\"\n",
    "\n",
    "# Get results table\n",
    "try:\n",
    "    results_df_session = globals().get('results_df', pd.DataFrame())\n",
    "    if not results_df_session.empty:\n",
    "        mod_table_rows = \"\"\n",
    "        for _, row in results_df_session.head(5).iterrows():\n",
    "            depth = int(row.get('max_depth', 0))\n",
    "            tr_acc = float(row.get('train_acc', 0))\n",
    "            val_acc = float(row.get('val_acc', 0))\n",
    "            mod_table_rows += f\"{depth} & {tr_acc:.4f} & {val_acc:.4f} \\\\\\\\\\n\"\n",
    "    else:\n",
    "        mod_table_rows = \"Data not available \\\\\\\\\\n\"\n",
    "except:\n",
    "    print(\"[INFO] Results table not available\")\n",
    "    mod_table_rows = \"Data not available \\\\\\\\\\n\"\n",
    "\n",
    "# Get algorithm details\n",
    "mod_algo = \"Random Forest Classifier\"\n",
    "mod_m_lbl = \"max\\\\_depth\"\n",
    "mod_m_val = str(globals().get('best_depth', 10))\n",
    "\n",
    "print(\"Modeling sections ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba781bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation with fallbacks\n",
    "eval_5a = latex_escape(globals().get('summary_comment', 'Test accuracy achieved'))\n",
    "eval_5b = latex_escape(globals().get('summary_comment_5b', 'Baseline comparisons performed'))\n",
    "eval_5c = latex_escape(globals().get('summary_comment_5c', 'Model meets success criteria'))\n",
    "eval_5d = latex_escape(globals().get('summary_comment_5d', 'Performance is strong'))\n",
    "eval_5e = latex_escape(globals().get('summary_comment_5e', 'Bias analysis completed'))\n",
    "\n",
    "print(\"Evaluation sections ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment with fallbacks\n",
    "dep_comparison = latex_escape(globals().get('comparison_and_recommendations_comment', 'Model ready for deployment'))\n",
    "dep_monitoring = latex_escape(globals().get('monitoring_plan_comment', 'Continuous monitoring planned'))\n",
    "dep_reproducibility = latex_escape(globals().get('reproducibility_reflection_comment', 'Fully reproducible'))\n",
    "dep_repo = latex_escape(globals().get('repo_info_comment', 'Code available in repository'))\n",
    "\n",
    "print(\"Deployment sections ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8fa1c",
   "metadata": {},
   "source": [
    "The following includes the Latex report itself. It fills in the query-results from the cell before. The ACM Template is already filled. \n",
    "Make sure that you update Student A and B accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24935663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FIGURE BLOCKS \n",
    "\n",
    "fig_blocks = {}\n",
    "fig_output_dir = Path(\"output/figures\")\n",
    "\n",
    "# Default captions for figures \n",
    "FIG_CAPTIONS = {\n",
    "    \"2b_01_activity_distribution.png\": \"Activity class distribution in the training set.\",\n",
    "    \"2b_02_top_features_correlation.png\": \"Correlation heatmap for a subset of highly informative features.\",\n",
    "    \"2b_03_top_variance_features.png\": \"Top 20 features ranked by variance.\",\n",
    "    \"2b_04_correlation_distribution.png\": \"Distribution of pairwise correlation coefficients across features.\",\n",
    "\n",
    "    \"2d_01_activity_distribution.png\": \"Activity distribution (visual check for class balance).\",\n",
    "    \"2d_02_subject_distribution.png\": \"Number of samples per subject (participant).\",\n",
    "    \"2d_03_static_vs_dynamic.png\": \"Example feature summary comparing static vs dynamic activities.\",\n",
    "    \"2d_04_acceleration_distribution.png\": \"Distribution of selected acceleration features (X/Y/Z).\",\n",
    "    \"2d_05_mean_acceleration_by_activity.png\": \"Mean acceleration by activity for the three axes.\",\n",
    "    \"2d_06_feature_correlation_heatmap.png\": \"Correlation heatmap for selected representative features.\",\n",
    "\n",
    "    \"fig_4e_hyperparameter_tuning.png\": \"Validation performance over the hyperparameter search.\",\n",
    "    \"evaluation_confusion_matrix_5a.png\": \"Confusion matrix on the test set.\",\n",
    "    \"evaluation_perclass_metrics_5a.png\": \"Per-class precision/recall/F1 on the test set.\",\n",
    "    \"fig_5c_comparison.png\": \"Model performance compared to baselines / reference results.\",\n",
    "    \"fig_5e_bias_check.png\": \"Performance comparison across protected attribute groups.\",\n",
    "}\n",
    "\n",
    "def only_existing(files):\n",
    "    return [f for f in files if (fig_output_dir / f).exists()]\n",
    "\n",
    "def latex_fig_block_with_text(files, widths=None, labels=None, texts=None):\n",
    "    \"\"\"\n",
    "    Build LaTeX block: figure + caption + optional explanation text below the figure.\n",
    "    - labels/texts: dict filename -> string\n",
    "    - If label not found, uses FIG_CAPTIONS default\n",
    "    \"\"\"\n",
    "    widths = widths or {}\n",
    "    labels = labels or {}\n",
    "    texts  = texts or {}\n",
    "\n",
    "    parts = []\n",
    "    for f in files:\n",
    "        w = widths.get(f, \"0.9\\\\linewidth\")\n",
    "\n",
    "        # label/text: if not available, fallback to default caption\n",
    "        lbl = labels.get(f) or FIG_CAPTIONS.get(f, f)\n",
    "        txt = texts.get(f) or \"\"\n",
    "\n",
    "        block = r\"\"\"\n",
    "\\begin{figure}[htbp]\n",
    "  \\centering\n",
    "  \\includegraphics[width=\"\"\" + w + r\"\"\"]{\"\"\" + f + r\"\"\"}\n",
    "  \\caption{\"\"\" + latex_escape(lbl) + r\"\"\"}\n",
    "\\end{figure}\n",
    "\"\"\".strip()\n",
    "\n",
    "        # If there is explanation text, add it below the figure\n",
    "        if str(txt).strip():\n",
    "            block += \"\\n\\n\" + latex_escape(txt).strip()\n",
    "\n",
    "        parts.append(block)\n",
    "\n",
    "    return \"\\n\\n\\\\vspace{0.6em}\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "# 2B (maintains your previous style if fig_2b_* variables exist)\n",
    "\n",
    "files_2b = only_existing([\n",
    "    \"2b_01_activity_distribution.png\",\n",
    "    \"2b_02_top_features_correlation.png\",\n",
    "    \"2b_03_top_variance_features.png\",\n",
    "    \"2b_04_correlation_distribution.png\",\n",
    "])\n",
    "\n",
    "if files_2b:\n",
    "    labels_2b = {\n",
    "        \"2b_01_activity_distribution.png\": globals().get(\"fig_2b_01_label\"),\n",
    "        \"2b_02_top_features_correlation.png\": globals().get(\"fig_2b_02_label\"),\n",
    "        \"2b_03_top_variance_features.png\": globals().get(\"fig_2b_03_label\"),\n",
    "        \"2b_04_correlation_distribution.png\": globals().get(\"fig_2b_04_label\"),\n",
    "    }\n",
    "    texts_2b = {\n",
    "        \"2b_01_activity_distribution.png\": globals().get(\"fig_2b_01_text\"),\n",
    "        \"2b_02_top_features_correlation.png\": globals().get(\"fig_2b_02_text\"),\n",
    "        \"2b_03_top_variance_features.png\": globals().get(\"fig_2b_03_text\"),\n",
    "        \"2b_04_correlation_distribution.png\": globals().get(\"fig_2b_04_text\"),\n",
    "    }\n",
    "\n",
    "    fig_blocks[\"2b\"] = latex_fig_block_with_text(\n",
    "        files_2b,\n",
    "        widths={\n",
    "            \"2b_01_activity_distribution.png\": \"0.85\\\\linewidth\",\n",
    "            \"2b_02_top_features_correlation.png\": \"0.9\\\\linewidth\",\n",
    "            \"2b_03_top_variance_features.png\": \"0.9\\\\linewidth\",\n",
    "            \"2b_04_correlation_distribution.png\": \"0.9\\\\linewidth\",\n",
    "        },\n",
    "        labels=labels_2b,\n",
    "        texts=texts_2b\n",
    "    )\n",
    "\n",
    "\n",
    "# 2D \n",
    "\n",
    "files_2d = only_existing([\n",
    "    \"2d_01_activity_distribution.png\",\n",
    "    \"2d_02_subject_distribution.png\",\n",
    "    \"2d_03_static_vs_dynamic.png\",\n",
    "    \"2d_04_acceleration_distribution.png\",\n",
    "    \"2d_05_mean_acceleration_by_activity.png\",\n",
    "    \"2d_06_feature_correlation_heatmap.png\",\n",
    "])\n",
    "\n",
    "if files_2d:\n",
    "    fig_blocks[\"2d\"] = latex_fig_block_with_text(\n",
    "        files_2d,\n",
    "        widths={\n",
    "            \"2d_03_static_vs_dynamic.png\": \"0.75\\\\linewidth\",\n",
    "            \"2d_05_mean_acceleration_by_activity.png\": \"0.95\\\\linewidth\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# 4E\n",
    "\n",
    "files_4e = only_existing([\"fig_4e_hyperparameter_tuning.png\"])\n",
    "if files_4e:\n",
    "    fig_blocks[\"4e\"] = latex_fig_block_with_text(files_4e)\n",
    "\n",
    "\n",
    "# 5A\n",
    "\n",
    "files_5a = only_existing([\n",
    "    \"evaluation_confusion_matrix_5a.png\",\n",
    "    \"evaluation_perclass_metrics_5a.png\",\n",
    "])\n",
    "if files_5a:\n",
    "    fig_blocks[\"5a\"] = latex_fig_block_with_text(\n",
    "        files_5a,\n",
    "        widths={\n",
    "            \"evaluation_confusion_matrix_5a.png\": \"0.8\\\\linewidth\",\n",
    "            \"evaluation_perclass_metrics_5a.png\": \"0.95\\\\linewidth\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# 5C\n",
    "\n",
    "files_5c = only_existing([\"fig_5c_comparison.png\"])\n",
    "if files_5c:\n",
    "    fig_blocks[\"5c\"] = latex_fig_block_with_text(files_5c)\n",
    "\n",
    "\n",
    "# 5E\n",
    "\n",
    "files_5e = only_existing([\"fig_5e_bias_check.png\"])\n",
    "if files_5e:\n",
    "    fig_blocks[\"5e\"] = latex_fig_block_with_text(files_5e)\n",
    "\n",
    "print(\"[OK] fig_blocks built for:\", sorted(fig_blocks.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f29700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe defaults to avoid NameError while testing\n",
    "def safe_get(name, default=\"\"):\n",
    "    return globals().get(name, default)\n",
    "\n",
    "author_block_latex   = safe_get(\"author_block_latex\")\n",
    "bu_data_source       = safe_get(\"bu_data_source\")\n",
    "bu_objectives        = safe_get(\"bu_objectives\")\n",
    "bu_success_criteria  = safe_get(\"bu_success_criteria\")\n",
    "bu_dm_goals          = safe_get(\"bu_dm_goals\")\n",
    "bu_dm_success        = safe_get(\"bu_dm_success\")\n",
    "bu_ai_risks          = safe_get(\"bu_ai_risks\")\n",
    "\n",
    "du_description = safe_get(\"du_description\")\n",
    "du_2a_content  = safe_get(\"du_2a_content\")\n",
    "du_2b_content  = safe_get(\"du_2b_content\")\n",
    "du_2c_content  = safe_get(\"du_2c_content\")\n",
    "du_2d_content  = safe_get(\"du_2d_content\")\n",
    "du_2e_content  = safe_get(\"du_2e_content\")\n",
    "du_2f_content  = safe_get(\"du_2f_content\")\n",
    "du_2g_content  = safe_get(\"du_2g_content\")\n",
    "\n",
    "dp_3a_content  = safe_get(\"dp_3a_content\")\n",
    "dp_3b_content  = safe_get(\"dp_3b_content\")\n",
    "dp_3c_content  = safe_get(\"dp_3c_content\")\n",
    "dp_3d_content  = safe_get(\"dp_3d_content\")\n",
    "\n",
    "mod_algorithm_selection = safe_get(\"mod_algorithm_selection\")\n",
    "mod_hp_selection        = safe_get(\"mod_hp_selection\")\n",
    "hp_table_rows           = safe_get(\"hp_table_rows\")\n",
    "mod_split_definition    = safe_get(\"mod_split_definition\")\n",
    "mod_training_comment    = safe_get(\"mod_training_comment\")\n",
    "mod_algo                = safe_get(\"mod_algo\")\n",
    "mod_start               = safe_get(\"mod_start\")\n",
    "mod_end                 = safe_get(\"mod_end\")\n",
    "mod_m_lbl               = safe_get(\"mod_m_lbl\")\n",
    "mod_m_val               = safe_get(\"mod_m_val\")\n",
    "mod_model_selection     = safe_get(\"mod_model_selection\")\n",
    "mod_final_training      = safe_get(\"mod_final_training\")\n",
    "\n",
    "eval_5a_content = safe_get(\"eval_5a_content\")\n",
    "eval_5b_content = safe_get(\"eval_5b_content\")\n",
    "eval_5c_content = safe_get(\"eval_5c_content\")\n",
    "eval_5d_content = safe_get(\"eval_5d_content\")\n",
    "eval_5e_content = safe_get(\"eval_5e_content\")\n",
    "\n",
    "dep_6a_content = safe_get(\"dep_6a_content\")\n",
    "dep_6b_content = safe_get(\"dep_6b_content\")\n",
    "dep_6c_content = safe_get(\"dep_6c_content\")\n",
    "dep_6d_content = safe_get(\"dep_6d_content\")\n",
    "\n",
    "fig_blocks = safe_get(\"fig_blocks\", {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9aac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE GENERATORS\n",
    "\n",
    "\n",
    "def create_activity_distribution_table():\n",
    "    \"\"\"Generate LaTeX table for activity distribution\"\"\"\n",
    "    try:\n",
    "        activity_counts_var = globals().get('activity_counts', None)\n",
    "        if activity_counts_var is None or activity_counts_var.empty:\n",
    "            return \"Activity distribution data not available.\"\n",
    "        \n",
    "        rows = []\n",
    "        for activity, count in activity_counts_var.items():\n",
    "            pct = (count / activity_counts_var.sum()) * 100\n",
    "            rows.append(f\"{latex_escape(activity)} & {count} & {pct:.2f}\\\\%\")\n",
    "        \n",
    "        table = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Activity Distribution in Training Set}\n",
    "\\begin{tabular}{lcc}\n",
    "\\toprule\n",
    "Activity & Count & Percentage \\\\\n",
    "\\midrule\n",
    "\"\"\" + \"\\\\\\\\\\n\".join(rows) + r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        return f\"Activity distribution table generation failed: {str(e)}\"\n",
    "\n",
    "def create_variance_table():\n",
    "    \"\"\"Generate LaTeX table for top variance features\"\"\"\n",
    "    try:\n",
    "        top_20_vars_data = globals().get('top_20_vars', None)\n",
    "        if top_20_vars_data is None or top_20_vars_data.empty:\n",
    "            return \"Variance data not available.\"\n",
    "        \n",
    "        rows = []\n",
    "        for i, (feature, variance) in enumerate(top_20_vars_data.head(10).items(), 1):\n",
    "            rows.append(f\"{i} & {latex_escape(feature)} & {variance:.6f}\")\n",
    "        \n",
    "        table = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Top 10 Features by Variance}\n",
    "\\begin{tabular}{clc}\n",
    "\\toprule\n",
    "Rank & Feature & Variance \\\\\n",
    "\\midrule\n",
    "\"\"\" + \"\\\\\\\\\\n\".join(rows) + r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        return f\"Variance table generation failed: {str(e)}\"\n",
    "\n",
    "def create_confusion_matrix_latex():\n",
    "    \"\"\"Generate LaTeX table for confusion matrix\"\"\"\n",
    "    try:\n",
    "        conf_mat_data = globals().get('conf_mat', None)\n",
    "        activity_mapping_var = globals().get('activity_mapping', {})\n",
    "        \n",
    "        if conf_mat_data is None:\n",
    "            return \"Confusion matrix data not available.\"\n",
    "        \n",
    "        class_labels = [activity_mapping_var.get(i, f'Class {i}') for i in range(len(conf_mat_data))]\n",
    "        \n",
    "        rows = []\n",
    "        for i, row in enumerate(conf_mat_data):\n",
    "            row_str = latex_escape(class_labels[i])\n",
    "            for val in row:\n",
    "                row_str += f\" & {int(val)}\"\n",
    "            rows.append(row_str)\n",
    "        \n",
    "        header_row = \"True / Predicted & \" + \" & \".join([latex_escape(lbl) for lbl in class_labels])\n",
    "        \n",
    "        table = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Confusion Matrix on Test Set}\n",
    "\\begin{tabular}{l\"\"\" + \"c\" * len(class_labels) + r\"\"\"}\n",
    "\\toprule\n",
    "\"\"\" + header_row + r\"\"\" \\\\\n",
    "\\midrule\n",
    "\"\"\" + \"\\\\\\\\\\n\".join(rows) + r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        return f\"Confusion matrix table generation failed: {str(e)}\"\n",
    "\n",
    "def create_classification_report_latex():\n",
    "    \"\"\"Generate LaTeX table for classification report\"\"\"\n",
    "    try:\n",
    "        cls_report_data = globals().get('cls_report', None)\n",
    "        activity_mapping_var = globals().get('activity_mapping', {})\n",
    "        \n",
    "        if cls_report_data is None or not isinstance(cls_report_data, dict):\n",
    "            return \"Classification report data not available.\"\n",
    "        \n",
    "        rows = []\n",
    "        for class_label, metrics in cls_report_data.items():\n",
    "            if class_label in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                continue\n",
    "            if isinstance(metrics, dict):\n",
    "                activity_name = activity_mapping_var.get(int(class_label), f'Class {class_label}')\n",
    "                prec = metrics.get('precision', 0)\n",
    "                rec = metrics.get('recall', 0)\n",
    "                f1 = metrics.get('f1-score', 0)\n",
    "                rows.append(f\"{latex_escape(activity_name)} & {prec:.4f} & {rec:.4f} & {f1:.4f}\")\n",
    "        \n",
    "        table = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Per-Class Metrics on Test Set}\n",
    "\\begin{tabular}{lccc}\n",
    "\\toprule\n",
    "Activity & Precision & Recall & F1-Score \\\\\n",
    "\\midrule\n",
    "\"\"\" + \"\\\\\\\\\\n\".join(rows) + r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        return f\"Classification report table generation failed: {str(e)}\"\n",
    "\n",
    "def create_baseline_comparison_table():\n",
    "    \"\"\"Generate LaTeX table for baseline comparison\"\"\"\n",
    "    try:\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        \n",
    "        baseline_path = os.path.join('output', 'test_baselines_5b.csv')\n",
    "        if not os.path.exists(baseline_path):\n",
    "            return \"Baseline comparison data not available.\"\n",
    "        \n",
    "        baseline_df = pd.read_csv(baseline_path)\n",
    "        acc_var = globals().get('acc', 0)\n",
    "        \n",
    "        rows = []\n",
    "        rows.append(f\"Our Random Forest & {acc_var:.4f} & Final model\")\n",
    "        \n",
    "        for _, row in baseline_df.iterrows():\n",
    "            baseline_name = row['baseline'].replace('_', ' ').title()\n",
    "            accuracy = row['accuracy']\n",
    "            ref_class = row.get('reference_class', 'n/a')\n",
    "            ref_info = f\"Class {ref_class}\" if ref_class != 'n/a' else 'All classes'\n",
    "            rows.append(f\"{latex_escape(baseline_name)} & {accuracy:.4f} & {ref_info}\")\n",
    "        \n",
    "        table = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Model Performance vs. Baselines}\n",
    "\\begin{tabular}{lcc}\n",
    "\\toprule\n",
    "Model/Baseline & Accuracy & Reference \\\\\n",
    "\\midrule\n",
    "\"\"\" + \"\\\\\\\\\\n\".join(rows) + r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        return f\"Baseline comparison table generation failed: {str(e)}\"\n",
    "\n",
    "def create_comparison_table():\n",
    "    \"\"\"Generate LaTeX table for performance comparison with SOTA\"\"\"\n",
    "    try:\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        \n",
    "        comp_path = os.path.join('output', 'test_comparison_5c.csv')\n",
    "        if not os.path.exists(comp_path):\n",
    "            return \"Performance comparison data not available.\"\n",
    "        \n",
    "        comp_df = pd.read_csv(comp_path)\n",
    "        \n",
    "        rows = []\n",
    "        for _, row in comp_df.iterrows():\n",
    "            metric = row['metric'].replace('_', ' ').title()\n",
    "            value = row['value']\n",
    "            lift_maj = row.get('lift_over_majority', 0)\n",
    "            lift_rnd = row.get('lift_over_random', 0)\n",
    "            rows.append(f\"{latex_escape(metric)} & {value:.4f} & {lift_maj:.4f} & {lift_rnd:.4f}\")\n",
    "        \n",
    "        table = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Model Performance Comparison}\n",
    "\\begin{tabular}{lccc}\n",
    "\\toprule\n",
    "Metric & Value & Lift over Majority & Lift over Random \\\\\n",
    "\\midrule\n",
    "\"\"\" + \"\\\\\\\\\\n\".join(rows) + r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        return f\"Comparison table generation failed: {str(e)}\"\n",
    "\n",
    "def create_success_criteria_table():\n",
    "    \"\"\"Generate LaTeX table for success criteria validation\"\"\"\n",
    "    try:\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        \n",
    "        success_path = os.path.join('output', 'test_success_5d.csv')\n",
    "        if not os.path.exists(success_path):\n",
    "            return \"Success criteria data not available.\"\n",
    "        \n",
    "        success_df = pd.read_csv(success_path)\n",
    "        \n",
    "        rows = []\n",
    "        for _, row in success_df.iterrows():\n",
    "            metric = row['metric'].replace('_', ' ').title()\n",
    "            value = row['value']\n",
    "            meets_acc = 'Yes' if row.get('meets_accuracy', '') == True else '-'\n",
    "            meets_bal = 'Yes' if row.get('meets_balance', '') == True else '-'\n",
    "            overall = 'Pass' if row.get('overall_pass', '') == True else '-'\n",
    "            rows.append(f\"{latex_escape(metric)} & {value:.4f} & {meets_acc} & {meets_bal} & {overall}\")\n",
    "        \n",
    "        table = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Success Criteria Validation}\n",
    "\\begin{tabular}{lcccc}\n",
    "\\toprule\n",
    "Metric & Value & Meets Accuracy & Meets Balance & Status \\\\\n",
    "\\midrule\n",
    "\"\"\" + \"\\\\\\\\\\n\".join(rows) + r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        return f\"Success criteria table generation failed: {str(e)}\"\n",
    "\n",
    "def create_bias_assessment_table():\n",
    "    \"\"\"Generate LaTeX table for bias assessment across demographic groups\"\"\"\n",
    "    try:\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        \n",
    "        bias_path = os.path.join('output', 'test_bias_5e.csv')\n",
    "        if not os.path.exists(bias_path):\n",
    "            return \"Bias assessment data not available.\"\n",
    "        \n",
    "        bias_df = pd.read_csv(bias_path)\n",
    "        \n",
    "        rows = []\n",
    "        for _, row in bias_df.iterrows():\n",
    "            group = f\"Group {row['group']}\"\n",
    "            support = row['support']\n",
    "            accuracy = row['accuracy']\n",
    "            f1 = row.get('macro_f1', 0)\n",
    "            rows.append(f\"{group} & {support} & {accuracy:.4f} & {f1:.4f}\")\n",
    "        \n",
    "        table = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Performance by Demographic Group}\n",
    "\\begin{tabular}{lccc}\n",
    "\\toprule\n",
    "Group & Support & Accuracy & Macro F1 \\\\\n",
    "\\midrule\n",
    "\"\"\" + \"\\\\\\\\\\n\".join(rows) + r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        return f\"Bias assessment table generation failed: {str(e)}\"\n",
    "\n",
    "print(\"[OK] Table generator functions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ce52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate metrics strings with proper handling\n",
    "acc_str = f\"{globals().get('acc', 0):.4f}\" if 'acc' in globals() else \"N/A\"\n",
    "prec_macro_str = f\"{globals().get('prec_macro', 0):.4f}\" if 'prec_macro' in globals() else \"N/A\"\n",
    "rec_macro_str = f\"{globals().get('rec_macro', 0):.4f}\" if 'rec_macro' in globals() else \"N/A\"\n",
    "f1_macro_str = f\"{globals().get('f1_macro', 0):.4f}\" if 'f1_macro' in globals() else \"N/A\"\n",
    "\n",
    "# Process text variables with latex_escape\n",
    "mod_final_training_escaped = latex_escape(globals().get('mod_final_training', 'Final model training completed'))\n",
    "\n",
    "# Generate tables\n",
    "activity_table = create_activity_distribution_table()\n",
    "variance_table = create_variance_table()\n",
    "confusion_table = create_confusion_matrix_latex()\n",
    "classification_table = create_classification_report_latex()\n",
    "baseline_table = create_baseline_comparison_table()\n",
    "comparison_table = create_comparison_table()\n",
    "success_table = create_success_criteria_table()\n",
    "bias_table = create_bias_assessment_table()\n",
    "\n",
    "# Generate appendix tables from CSV files\n",
    "def create_appendix_csv_table(csv_path, caption, column_spec=\"l\"):\n",
    "    \"\"\"Generate LaTeX table from CSV file for appendix\"\"\"\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        return f\"% CSV file not found: {csv_path}\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if df.empty:\n",
    "            return f\"% Empty CSV: {csv_path}\"\n",
    "        \n",
    "        # Build column specification\n",
    "        cols = len(df.columns)\n",
    "        if column_spec == \"auto\":\n",
    "            col_spec = \"l\" + \"c\" * (cols - 1)\n",
    "        else:\n",
    "            col_spec = column_spec * cols if len(column_spec) == 1 else column_spec\n",
    "        \n",
    "        # Header row\n",
    "        header = \" & \".join([latex_escape(str(col)) for col in df.columns])\n",
    "        \n",
    "        # Data rows\n",
    "        rows = []\n",
    "        for _, row in df.iterrows():\n",
    "            row_vals = []\n",
    "            for val in row:\n",
    "                if pd.isna(val):\n",
    "                    row_vals.append(\"\")\n",
    "                elif isinstance(val, (int, float)):\n",
    "                    if isinstance(val, float):\n",
    "                        row_vals.append(f\"{val:.4f}\" if abs(val) < 1000 else f\"{val:.2f}\")\n",
    "                    else:\n",
    "                        row_vals.append(str(val))\n",
    "                else:\n",
    "                    row_vals.append(latex_escape(str(val)))\n",
    "            rows.append(\" & \".join(row_vals))\n",
    "        \n",
    "        table = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{\"\"\" + caption + r\"\"\"}\n",
    "\\begin{tabular}{\"\"\" + col_spec + r\"\"\"}\n",
    "\\toprule\n",
    "\"\"\" + header + r\"\"\" \\\\\n",
    "\\midrule\n",
    "\"\"\" + \"\\\\\\\\\\n\".join(rows) + r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "        return table\n",
    "    except Exception as e:\n",
    "        return f\"% Error generating table from {csv_path}: {str(e)}\"\n",
    "\n",
    "# Create appendix tables\n",
    "appendix_metrics_table = create_appendix_csv_table(\n",
    "    \"output/evaluation_metrics_5a.csv\",\n",
    "    \"Complete Evaluation Metrics on Test Set\"\n",
    ")\n",
    "\n",
    "appendix_confusion_csv_table = create_appendix_csv_table(\n",
    "    \"output/evaluation_confusion_matrix_5a.csv\",\n",
    "    \"Confusion Matrix Values (Numeric)\"\n",
    ")\n",
    "\n",
    "appendix_classification_csv_table = create_appendix_csv_table(\n",
    "    \"output/evaluation_classification_report_5a.csv\",\n",
    "    \"Detailed Classification Report with All Metrics\"\n",
    ")\n",
    "\n",
    "# Generate the complete LaTeX document\n",
    "latex_content = rf\"\"\"\\documentclass[sigconf,nonacm]{{acmart}}\n",
    "\n",
    "%% Disable microtype to avoid font expansion issues\n",
    "\\PassOptionsToPackage{{protrusion=false,expansion=false}}{{microtype}}\n",
    "\\pdfminorversion=5\n",
    "\\pdfsuppresswarningpagegroup=1\n",
    "\n",
    "%% Required packages\n",
    "\\usepackage{{graphicx}}\n",
    "\\usepackage{{subcaption}}\n",
    "\\usepackage[font=footnotesize,labelfont=bf]{{caption}}\n",
    "\\usepackage{{booktabs}}\n",
    "\\usepackage{{url}}\n",
    "\n",
    "\\graphicspath{{{{output/figures/}}{{../output/figures/}}{{./output/figures/}}}}\n",
    "\n",
    "\\AtBeginDocument{{ \\providecommand\\BibTeX{{ Bib\\TeX }} }}\n",
    "\n",
    "%% Student report: do NOT claim ACM publication metadata\n",
    "\\settopmatter{{printacmref=false}}\n",
    "\\renewcommand\\footnotetextcopyrightpermission[1]{{}}\n",
    "\\pagestyle{{plain}}\n",
    "\n",
    "%% Suppress conference information in header/footer\n",
    "\\acmConference[ ]{{ }}{{ }}{{ }}\n",
    "\\acmBooktitle{{ }}\n",
    "\\acmYear{{ }}\n",
    "\\acmDOI{{ }}\n",
    "\\acmISBN{{ }}\n",
    "\\acmPrice{{ }}\n",
    "\\setcopyright{{none}}\n",
    "\n",
    "\\begin{{document}}\n",
    "\n",
    "\\title{{BI2025 Experiment Report - Human Activity Recognition Using Smartphones\\\\\\\\Group {group_id}}}\n",
    "\n",
    "{author_block_latex}\n",
    "\n",
    "\\begin{{abstract}}\n",
    "This study presents Group {group_id}'s CRISP-DM workflow for Human Activity Recognition using smartphone sensors (UCI HAR). We load and describe the data, examine class balance and feature redundancy, and prepare subject-aware splits for training and validation. A Random Forest model tuned with StratifiedGroupKFold achieves test accuracy {acc_str}, macro precision {prec_macro_str}, macro recall {rec_macro_str}, and macro F1 {f1_macro_str}. We interpret the confusion matrix and per-class support, compare against simple baselines, and inspect bias across subjects.\n",
    "\n",
    "A provenance-first setup records every phase, dataset, parameter, run, and metric in a knowledge graph using PROV-O/MLSO via the \\texttt{{starvers}} TripleStoreEngine. This end-to-end logging, together with the notebook, supports reproducibility from raw CSVs to derived results and figures. The report summarizes the main decisions and dependencies and offers a practical template for similar sensor-based classification tasks.\n",
    "\\end{{abstract}}\n",
    "\n",
    "\\ccsdesc[500]{{Computing methodologies~Machine learning}}\n",
    "\\ccsdesc[300]{{Information systems~Data provenance}}\n",
    "\\keywords{{CRISP-DM, Provenance, Knowledge Graph, Activity Recognition, Random Forest}}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "\\noindent\\textbf{{Course report disclaimer.}}\n",
    "This document is a student project report created for the course\n",
    "\\textit{{Business Intelligence and Data Analytics (BI 2025)}} at TU Wien.\n",
    "It is not an ACM-published paper and has no official ACM DOI or ISBN.\n",
    "\\vspace{{0.5em}}\n",
    "\n",
    "%% ========================================\n",
    "%% 1. BUSINESS UNDERSTANDING\n",
    "%% ========================================\n",
    "\\section{{Business Understanding}}\n",
    "\n",
    "\\subsection{{Data Source and Scenario}}\n",
    "{bu_data_source}\n",
    "\n",
    "\\subsection{{Business Objectives}}\n",
    "{bu_objectives}\n",
    "\n",
    "\\subsection{{Business Success Criteria}}\n",
    "{bu_success_criteria}\n",
    "\n",
    "\\subsection{{Data Mining Goals}}\n",
    "{bu_dm_goals}\n",
    "\n",
    "\\subsection{{Data Mining Success Criteria}}\n",
    "{bu_dm_success}\n",
    "\n",
    "\\subsection{{AI Risk Aspects}}\n",
    "{bu_ai_risks}\n",
    "\n",
    "%% ========================================\n",
    "%% 2. DATA UNDERSTANDING\n",
    "%% ========================================\n",
    "\\section{{Data Understanding}}\n",
    "\n",
    "\\subsection{{Overview}}\n",
    "{section_2a}\n",
    "\n",
    "\\subsection{{Activity Distribution Analysis}}\n",
    "{section_2b_intro}\n",
    "\n",
    "{activity_table}\n",
    "\n",
    "{fig_blocks.get('2b', '')}\n",
    "\n",
    "{section_2b_conclusion}\n",
    "\n",
    "\\subsection{{Data Quality Assessment}}\n",
    "{section_2c_intro}\n",
    "\n",
    "{variance_table}\n",
    "{section_2c_conclusion}\n",
    "\n",
    "\\subsubsection{{Quality Findings}}\n",
    "\\begin{{enumerate}}\n",
    "    \\item {quality_finding_01}\n",
    "    \\item {quality_finding_02}\n",
    "    \\item {quality_finding_03}\n",
    "    \\item {quality_finding_04}\n",
    "    \\item {quality_finding_05}\n",
    "    \\item {quality_finding_06}\n",
    "    \\item {quality_finding_07}\n",
    "\\end{{enumerate}}\n",
    "\n",
    "{section_2c_conclusion}\n",
    "\n",
    "\\subsection{{Statistical Characteristics and Visual Exploration}}\n",
    "{section_2d_intro}\n",
    "\n",
    "{fig_blocks.get('2d', '')}\n",
    "\n",
    "{section_2d_conclusion}\n",
    "\n",
    "\\subsection{{Ethical Sensitivity Assessment}}\n",
    "{section_2e_intro}\n",
    "{section_2e_conclusion}\n",
    "\n",
    "\\subsubsection{{Ethical Considerations}}\n",
    "\\begin{{enumerate}}\n",
    "    \\item {ethical_finding_01}\n",
    "    \\item {ethical_finding_02}\n",
    "    \\item {ethical_finding_03}\n",
    "    \\item {ethical_finding_04}\n",
    "    \\item {ethical_finding_05}\n",
    "\\end{{enumerate}}\n",
    "\n",
    "{section_2e_conclusion}\n",
    "\n",
    "\\subsection{{AI Risk Analysis}}\n",
    "{section_2f_intro}\n",
    "{section_2f_conclusion}\n",
    "\n",
    "\\subsubsection{{Identified Risks}}\n",
    "\\begin{{enumerate}}\n",
    "    \\item {risk_finding_01}\n",
    "    \\item {risk_finding_02}\n",
    "    \\item {risk_finding_03}\n",
    "    \\item {risk_finding_04}\n",
    "    \\item {risk_finding_05}\n",
    "\\end{{enumerate}}\n",
    "\n",
    "{section_2f_conclusion}\n",
    "\n",
    "\\subsection{{Data Preparation Recommendations}}\n",
    "{section_2g_intro}\n",
    "{section_2g_conclusion}\n",
    "\n",
    "\\subsubsection{{Recommended Preparation Steps}}\n",
    "\\begin{{enumerate}}\n",
    "    \\item {prep_finding_01}\n",
    "    \\item {prep_finding_02}\n",
    "    \\item {prep_finding_03}\n",
    "    \\item {prep_finding_04}\n",
    "    \\item {prep_finding_05}\n",
    "\\end{{enumerate}}\n",
    "\n",
    "{section_2g_conclusion}\n",
    "\n",
    "%% ========================================\n",
    "%% 3. DATA PREPARATION\n",
    "%% ========================================\n",
    "\\section{{Data Preparation}}\n",
    "\n",
    "\\subsection{{Train-Validation-Test Split}}\n",
    "{dp_3a}\n",
    "\n",
    "\\subsection{{Target Encoding}}\n",
    "{dp_3b}\n",
    "\n",
    "\\subsection{{Feature Scaling Analysis}}\n",
    "{dp_3c}\n",
    "\n",
    "\\subsection{{Additional Transformations}}\n",
    "{dp_3d}\n",
    "\n",
    "%% ========================================\n",
    "%% 4. MODELING\n",
    "%% ========================================\n",
    "\\section{{Modeling}}\n",
    "\n",
    "\\subsection{{Algorithm Selection}}\n",
    "{mod_algorithm_selection}\n",
    "\n",
    "\\subsection{{Hyperparameter Configuration}}\n",
    "{mod_hp_selection}\n",
    "\n",
    "The model was trained using the following hyperparameter settings:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Hyperparameter Settings}}\n",
    "  \\label{{tab:hyperparams}}\n",
    "  \\begin{{tabular}}{{ll}}\n",
    "    \\toprule\n",
    "    \\textbf{{Parameter}} & \\textbf{{Value}} \\\\\n",
    "    \\midrule\n",
    "    {hp_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "\\subsection{{Data Split Strategy}}\n",
    "{mod_split_definition}\n",
    "\n",
    "\\subsection{{Model Training Process}}\n",
    "{mod_training_comment}\n",
    "\n",
    "Training characteristics:\n",
    "\\begin{{itemize}}\n",
    "    \\item \\textbf{{Algorithm:}} {mod_algo}\n",
    "    \\item \\textbf{{Best Hyperparameter:}} {mod_m_lbl} = {mod_m_val}\n",
    "\\end{{itemize}}\n",
    "\n",
    "{fig_blocks.get('4e', '')}\n",
    "\n",
    "\\subsection{{Model Selection}}\n",
    "{mod_model_selection}\n",
    "\n",
    "\\subsection{{Final Model Training}}\n",
    "{mod_final_training_escaped}\n",
    "\n",
    "%% ========================================\n",
    "%% 5. EVALUATION\n",
    "%% ========================================\n",
    "\\section{{Evaluation}}\n",
    "\n",
    "\\subsection{{Test Set Performance}}\n",
    "{eval_5a}\n",
    "\n",
    "\\textbf{{Test Metrics:}}\n",
    "\\begin{{itemize}}\n",
    "    \\item Accuracy: {acc_str}\n",
    "    \\item Precision (macro): {prec_macro_str}\n",
    "    \\item Recall (macro): {rec_macro_str}\n",
    "    \\item F1-Score (macro): {f1_macro_str}\n",
    "\\end{{itemize}}\n",
    "\n",
    "{confusion_table}\n",
    "\n",
    "{classification_table}\n",
    "\n",
    "Achieved test accuracy: {acc_str}, macro precision: {prec_macro_str}, macro recall: {rec_macro_str}, macro F1: {f1_macro_str}.\n",
    "\n",
    "{fig_blocks.get('5a', '')}\n",
    "\n",
    "\\subsection{{Baseline Comparisons}}\n",
    "{eval_5b}\n",
    "\n",
    "{baseline_table}\n",
    "\n",
    "{fig_blocks.get('5b', '')}\n",
    "\n",
    "\\subsection{{Performance Comparison}}\n",
    "{eval_5c}\n",
    "\n",
    "{comparison_table}\n",
    "\n",
    "{fig_blocks.get('5c', '')}\n",
    "\n",
    "\\subsection{{Business Success Criteria Validation}}\n",
    "{eval_5d}\n",
    "\n",
    "{success_table}\n",
    "\n",
    "{fig_blocks.get('5d', '')}\n",
    "\n",
    "\\subsection{{Bias Assessment}}\n",
    "{eval_5e}\n",
    "\n",
    "{bias_table}\n",
    "\n",
    "{fig_blocks.get('5e', '')}\n",
    "\n",
    "%% ========================================\n",
    "%% 6. DEPLOYMENT\n",
    "%% ========================================\n",
    "\\section{{Deployment}}\n",
    "\n",
    "\\subsection{{Repository Information}}\n",
    "{dep_repo}\n",
    "\n",
    "\\subsection{{Reproducibility Reflection}}\n",
    "{dep_reproducibility}\n",
    "\n",
    "\\subsection{{Comparison with Related Work}}\n",
    "{dep_comparison}\n",
    "\n",
    "\\subsection{{Ethical Considerations}}\n",
    "{dep_6b_content}\n",
    "\n",
    "\\subsection{{Monitoring Strategy}}\n",
    "{dep_monitoring}\n",
    "\n",
    "\\subsection{{Reproducibility Notes}}\n",
    "{dep_reproducibility}\n",
    "\n",
    "%% ========================================\n",
    "%% 7. CONCLUSION\n",
    "%% ========================================\n",
    "\\section{{Conclusion}}\n",
    "\n",
    "This experiment demonstrates the application of the CRISP-DM methodology to a human activity recognition problem using the HAR with Smartphones dataset. The final model achieved strong performance on the held-out test set while maintaining balanced accuracy across all activity classes. All experimental steps, decisions, and results were logged in a provenance knowledge graph using PROV-O and related ontologies to ensure reproducibility and transparency.\n",
    "\n",
    "Key findings include:\n",
    "\\begin{{itemize}}\n",
    "    \\item The HAR dataset provides high-quality preprocessed sensor features with 561 engineered time and frequency domain measurements suitable for immediate modeling\n",
    "    \\item Random Forest proved effective for multi-class activity classification, balancing model complexity with generalization\n",
    "    \\item Hyperparameter tuning identified optimal configurations through systematic evaluation on validation set\n",
    "    \\item The final model meets business requirements with balanced performance across all activity classes\n",
    "    \\item Comprehensive provenance documentation through starvers triple store enables full experiment reproducibility\n",
    "\\end{{itemize}}\n",
    "\n",
    "%% ========================================\n",
    "%% 8. REPRODUCIBILITY\n",
    "%% ========================================\n",
    "\\section{{Reproducibility}}\n",
    "\n",
    "This experiment is fully reproducible. All computational steps, decisions, and data transformations were logged systematically in a SPARQL-compliant triple store using the PROV-O ontology and related standards.\n",
    "\n",
    "\\subsection{{Software Environment}}\n",
    "\\begin{{itemize}}\n",
    "    \\item Python 3.11.14\n",
    "    \\item scikit-learn 1.5.2\n",
    "    \\item pandas 2.2.2\n",
    "    \\item numpy 1.26.4\n",
    "    \\item matplotlib 3.8.4\n",
    "    \\item seaborn 0.13.2\n",
    "    \\item starvers 0.1.0 (provenance logging engine)\n",
    "\\end{{itemize}}\n",
    "\n",
    "\\subsection{{Deterministic Execution}}\n",
    "All random operations use \\texttt{{random\\_state=42}} ensuring deterministic and reproducible results across multiple executions. This includes:\n",
    "\\begin{{itemize}}\n",
    "    \\item Random Forest model initialization\n",
    "    \\item Train/validation split using StratifiedGroupKFold\n",
    "    \\item Any data shuffling operations\n",
    "\\end{{itemize}}\n",
    "\n",
    "\\subsection{{Hardware Specifications}}\n",
    "Experiments conducted on Windows 11 machine with sufficient computational resources for dataset processing and model training.\n",
    "\n",
    "\\subsection{{Data Access}}\n",
    "\\begin{{itemize}}\n",
    "    \\item \\textbf{{Original Dataset:}} UCI Human Activity Recognition Using Smartphones (publicly available)\n",
    "    \\item \\textbf{{Provenance Graph:}} SPARQL endpoint at \\url{{https://starvers.ec.tuwien.ac.at/BI2025}}\\\n",
    "    \\item \\textbf{{Group Namespace:}} \\texttt{{20/}} within BI2025 endpoint\n",
    "\\end{{itemize}}\n",
    "\n",
    "\\subsection{{Code Availability}}\n",
    "The complete Jupyter notebook and auxiliary files required to reproduce the experiment are included in the submitted archive.\n",
    "The experiment provenance is available via the Starvers endpoint and group namespace:\n",
    "\\begin{{itemize}}\n",
    "  \\item \\textbf{{Starvers endpoint:}} \\url{{https://starvers.ec.tuwien.ac.at/BI2025}}\n",
    "  \\item \\textbf{{Group namespace (PID/URI):}} \\url{{https://starvers.ec.tuwien.ac.at/BI2025/20/}}\n",
    "  \\item \\textbf{{Code repository:}} \\url{{https://gitlab.tuwien.ac.at/e52400204/bi2025-experiment-report-human-activity-recognition-using-smartphones.git}}\n",
    "\\end{{itemize}}\n",
    "\n",
    "\\subsection{{Provenance Queries}}\n",
    "All experimental provenance can be queried using SPARQL. Example query to retrieve all modeling activities:\n",
    "\\begin{{verbatim}}\n",
    "PREFIX prov: <http://www.w3.org/ns/prov#>\n",
    "PREFIX : <https://starvers.ec.tuwien.ac.at/BI2025/20/>\n",
    "\n",
    "SELECT ?activity ?label WHERE {{\n",
    "  ?activity a prov:Activity .\n",
    "  ?activity rdfs:label ?label .\n",
    "  FILTER(CONTAINS(STR(?activity), \"modeling\"))\n",
    "}}\n",
    "\\end{{verbatim}}\n",
    "\n",
    "%% ========================================\n",
    "%% APPENDIX\n",
    "%% ========================================\n",
    "\\clearpage\n",
    "\\onecolumn\n",
    "\\appendix\n",
    "\n",
    "\\section{{Supplementary Materials}}\n",
    "\n",
    "\\subsection{{Detailed Performance Metrics Tables}}\n",
    "\n",
    "This appendix provides complete numeric tables exported from the experiment for detailed analysis and verification of results. Tables are presented in single-column format for optimal readability and to prevent overlapping content.\n",
    "\n",
    "\\subsubsection{{Complete Evaluation Metrics}}\n",
    "\n",
    "The following table presents all evaluation metrics computed on the held-out test set, including overall accuracy and macro-averaged metrics across all activity classes.\n",
    "\n",
    "\\begin{{center}}\n",
    "\\footnotesize\n",
    "{appendix_metrics_table}\n",
    "\\end{{center}}\n",
    "\n",
    "\\vspace{{1em}}\n",
    "\n",
    "\\subsubsection{{Numeric Confusion Matrix}}\n",
    "\n",
    "The confusion matrix in numeric form shows the exact count of predictions for each true class versus predicted class combination, enabling precise analysis of misclassification patterns.\n",
    "\n",
    "\\begin{{center}}\n",
    "\\footnotesize\n",
    "{appendix_confusion_csv_table}\n",
    "\\end{{center}}\n",
    "\n",
    "\\vspace{{1em}}\n",
    "\n",
    "\\subsubsection{{Detailed Classification Report}}\n",
    "\n",
    "The complete classification report provides per-class precision, recall, F1-score, and support values for all six activity classes, along with macro and weighted averages.\n",
    "\n",
    "\\begin{{center}}\n",
    "\\footnotesize\n",
    "{appendix_classification_csv_table}\n",
    "\\end{{center}}\n",
    "\n",
    "\\vspace{{1em}}\n",
    "\n",
    "\\subsubsection{{Notes on Metric Interpretation}}\n",
    "\n",
    "\\begin{{itemize}}\n",
    "    \\item \\textbf{{Precision:}} Proportion of predicted instances for a class that are truly that class\n",
    "    \\item \\textbf{{Recall:}} Proportion of actual instances of a class that were correctly identified\n",
    "    \\item \\textbf{{F1-Score:}} Harmonic mean of precision and recall, balancing both metrics\n",
    "    \\item \\textbf{{Support:}} Number of actual instances of each class in the test set\n",
    "    \\item \\textbf{{Macro Average:}} Unweighted mean of per-class metrics, treating all classes equally\n",
    "    \\item \\textbf{{Weighted Average:}} Mean of per-class metrics weighted by class support\n",
    "\\end{{itemize}}\n",
    "\n",
    "All visualizations referenced in the main body of this report are generated from these underlying data tables and are available in the output/figures directory of the experiment archive.\n",
    "\n",
    "\\balance\n",
    "\\end{{document}}\n",
    "\"\"\"\n",
    "\n",
    "print(\"[OK] LaTeX document generated with all sections, findings, tables, and appendix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c947b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell stores the Latex report to the data/report directory\n",
    "\n",
    "out_dir = \"report\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"BI2025_gr020_52400204_12432813.tex\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_content)\n",
    "\n",
    "print(f\"Report written to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35963822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate PDF Report with this cell\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "tex_filename = \"BI2025_gr020_52400204_12432813.tex\"\n",
    "report_dir = Path(\"report\")\n",
    "tex_path = report_dir / tex_filename\n",
    "pdf_filename = tex_filename.replace('.tex', '.pdf')\n",
    "pdf_path = report_dir / pdf_filename\n",
    "\n",
    "print(f\"[PDF Generation] Starting compilation of {tex_filename}\")\n",
    "print(f\"[PDF Generation] Working directory: {report_dir.absolute()}\")\n",
    "\n",
    "# Check if LaTeX file exists\n",
    "if not tex_path.exists():\n",
    "    print(f\"[ERROR] LaTeX file not found: {tex_path}\")\n",
    "    print(\"[INFO] Please run the previous cells to generate the LaTeX file first\")\n",
    "    raise FileNotFoundError(f\"LaTeX file not found: {tex_path}\")\n",
    "\n",
    "# Check for pdflatex\n",
    "pdflatex_cmd = shutil.which(\"pdflatex\")\n",
    "if not pdflatex_cmd:\n",
    "    print(\"[ERROR] pdflatex not found in system PATH\")\n",
    "    print(\"[INFO] Please install a LaTeX distribution:\")\n",
    "    print(\"  - Windows: MiKTeX (https://miktex.org/) or TeX Live\")\n",
    "    print(\"  - Install and ensure 'pdflatex' is in your PATH\")\n",
    "    raise EnvironmentError(\"pdflatex not found in system PATH\")\n",
    "\n",
    "print(f\"[OK] Found pdflatex at: {pdflatex_cmd}\")\n",
    "\n",
    "# Compile LaTeX to PDF (run twice for references and cross-refs)\n",
    "compilation_commands = [\n",
    "    [pdflatex_cmd, \"-interaction=nonstopmode\", \"-output-directory=.\", tex_filename],\n",
    "    [pdflatex_cmd, \"-interaction=nonstopmode\", \"-output-directory=.\", tex_filename],\n",
    "]\n",
    "\n",
    "try:\n",
    "    for run_num, cmd in enumerate(compilation_commands, 1):\n",
    "        print(f\"\\n[PDF Generation] Compilation run {run_num}/2...\")\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            cwd=report_dir.absolute(),\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=120\n",
    "        )\n",
    "        \n",
    "        # Check if PDF was actually produced (most reliable indicator)\n",
    "        output_lower = result.stdout.lower()\n",
    "        pdf_written = \"output written on\" in output_lower\n",
    "        \n",
    "        # Check for actual fatal errors that prevent PDF generation\n",
    "        has_fatal_error = (\n",
    "            \"! emergency stop\" in output_lower and not pdf_written\n",
    "        ) or (\n",
    "            \"no pages of output\" in output_lower\n",
    "        )\n",
    "        \n",
    "        if has_fatal_error:\n",
    "            print(f\"[ERROR] Fatal LaTeX error in run {run_num} - PDF not generated\")\n",
    "            print(\"\\n--- LaTeX Error Output (last 2000 chars) ---\")\n",
    "            print(result.stdout[-2000:] if len(result.stdout) > 2000 else result.stdout)\n",
    "            if result.stderr:\n",
    "                print(\"\\n--- Stderr ---\")\n",
    "                print(result.stderr[-1000:] if len(result.stderr) > 1000 else result.stderr)\n",
    "            raise RuntimeError(\"Fatal LaTeX compilation error - no PDF generated\")\n",
    "        \n",
    "        # Check if PDF file exists\n",
    "        if pdf_path.exists():\n",
    "            pdf_size = pdf_path.stat().st_size\n",
    "            print(f\"[OK] Run {run_num} completed - PDF created ({pdf_size:,} bytes)\")\n",
    "        else:\n",
    "            print(f\"[WARNING] Run {run_num} completed but PDF file not found\")\n",
    "        \n",
    "        # Extract and show any warnings\n",
    "        if result.returncode != 0:\n",
    "            warning_lines = [line.strip() for line in result.stdout.split('\\n') \n",
    "                           if ('warning' in line.lower() or 'rerun' in line.lower()) \n",
    "                           and line.strip() and not line.startswith('%')]\n",
    "            if warning_lines:\n",
    "                print(f\"[INFO] LaTeX warnings (showing first 3):\")\n",
    "                for wline in warning_lines[:3]:\n",
    "                    if len(wline) > 100:\n",
    "                        wline = wline[:100] + \"...\"\n",
    "                    print(f\"  • {wline}\")\n",
    "    \n",
    "    # Final verification\n",
    "    if pdf_path.exists():\n",
    "        pdf_size = pdf_path.stat().st_size\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"✓ PDF GENERATED SUCCESSFULLY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"  File: {pdf_path.name}\")\n",
    "        print(f\"  Location: {pdf_path.absolute()}\")\n",
    "        print(f\"  Size: {pdf_size:,} bytes ({pdf_size/1024:.1f} KB)\")\n",
    "        \n",
    "        # Clean up auxiliary files\n",
    "        aux_extensions = ['.aux', '.log', '.out', '.toc', '.lof', '.lot']\n",
    "        cleaned_files = []\n",
    "        for ext in aux_extensions:\n",
    "            aux_file = report_dir / tex_filename.replace('.tex', ext)\n",
    "            if aux_file.exists():\n",
    "                aux_file.unlink()\n",
    "                cleaned_files.append(aux_file.name)\n",
    "        \n",
    "        if cleaned_files:\n",
    "            print(f\"\\n[Cleanup] Removed auxiliary files: {', '.join(cleaned_files)}\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"[REPRODUCIBILITY NOTES]\")\n",
    "        print(\"  To regenerate this PDF:\")\n",
    "        print(\"  1. Execute all notebook cells in order (especially Section 7)\")\n",
    "        print(\"  2. Run this cell again\")\n",
    "        print(f\"\\n  The PDF contains complete CRISP-DM analysis with:\")\n",
    "        print(\"  • Business Understanding\")\n",
    "        print(\"  • Data Understanding (with activity/variance tables)\")\n",
    "        print(\"  • Data Preparation\")\n",
    "        print(\"  • Modeling (hyperparameter selection)\")\n",
    "        print(\"  • Evaluation (confusion matrix, classification report)\")\n",
    "        print(\"  • Deployment\")\n",
    "        print(\"  • Appendix (additional metrics and provenance)\")\n",
    "        print(f\"{'='*70}\")\n",
    "    else:\n",
    "        print(f\"\\n[ERROR] PDF file not found: {pdf_path}\")\n",
    "        print(\"[DEBUG] LaTeX may be missing required packages\")\n",
    "        print(\"[ACTION] Check MiKTeX Package Manager for missing packages\")\n",
    "        raise FileNotFoundError(f\"PDF not generated at {pdf_path}\")\n",
    "\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"[ERROR] pdflatex compilation timed out (>120 seconds)\")\n",
    "    print(\"[ACTION] Check for infinite loops in LaTeX or missing packages\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    # Final check if PDF exists despite exception\n",
    "    if pdf_path.exists():\n",
    "        pdf_size = pdf_path.stat().st_size\n",
    "        print(f\"\\n[INFO] PDF exists despite error: {pdf_size:,} bytes at {pdf_path}\")\n",
    "    else:\n",
    "        print(f\"[FATAL] PDF generation failed: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BI2025A31",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
