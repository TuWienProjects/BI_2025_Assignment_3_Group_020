\documentclass[sigconf]{acmart}

%% Disable microtype to avoid font expansion issues
\PassOptionsToPackage{protrusion=false,expansion=false}{microtype}
\pdfminorversion=5
\pdfsuppresswarningpagegroup=1

%% Required packages
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{booktabs}  %% Professional tables


\AtBeginDocument{ \providecommand\BibTeX{ Bib\TeX } }
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

\acmConference[BI 2025]{Business Intelligence and Data Analytics}{January 2026}{TU Wien, Vienna, Austria}

\begin{document}

\title{BI2025 Experiment Report - Human Activity Recognition Using Smartphones\\Group 20}

%% Authors with proper ACM formatting
\author{Muhammad Sajid Bashir}
\authornote{Person A - Responsible for Data Understanding, Modeling, and Evaluation}
\email{e52400204@student.tuwien.ac.at}
\orcid{0009-0005-3077-022X}
\affiliation{%
  \institution{TU Wien}
  \streetaddress{Karlsplatz 13}
  \city{Vienna}
  \country{Austria}
  \postcode{1040}
}

\author{Eman Shahin}
\authornote{Person B - Responsible for Data Preparation, Deployment, and Documentation}
\email{e12432813@student.tuwien.ac.at}
\orcid{0000-0000-0000-0001}
\affiliation{%
  \institution{TU Wien}
  \streetaddress{Karlsplatz 13}
  \city{Vienna}
  \country{Austria}
  \postcode{1040}
}

\begin{abstract}
This report documents a machine learning experiment for Group 20 following the CRISP-DM process model. The project analyzes the Human Activity Recognition with Smartphones dataset to build a classifier that identifies physical activities from sensor data. All experiments and decisions are logged in a provenance knowledge graph using PROV-O and related ontologies to ensure reproducibility.
\end{abstract}

\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[300]{Information systems~Data provenance}
\keywords{CRISP-DM, Provenance, Knowledge Graph, Activity Recognition, Random Forest}

\maketitle

%% ========================================
%% 1. BUSINESS UNDERSTANDING
%% ========================================
\section{Business Understanding}

\subsection{Data Source and Scenario}
The dataset used in this project is the Human Activity Recognition with Smartphones dataset, 
published by UCI and available on Kaggle. It contains sensor measurements collected from 
30 volunteers wearing a Samsung Galaxy S II smartphone on their waist while performing 
six everyday activities: walking, walking upstairs, walking downstairs, sitting, standing, 
and laying. The phone recorded accelerometer and gyroscope readings at 50 Hz, and the 
researchers transformed these raw signals into 561 numerical features.

Scenario:
A realistic scenario for this dataset is the development of applications that can automatically 
recognize what a user is doing based only on their smartphone sensors. This could be useful in 
many areas — for example, health monitoring, fitness apps, or detecting inactivity in elderly care. 
There are also commercial and marketing-related possibilities, such as tailoring notifications or 
recommendations based on what the user is currently doing, or understanding long-term activity 
patterns to improve customer segmentation. This dataset allows us to evaluate how well machine learning models can classify physical activities in these contexts.

\subsection{Business Objectives}
The main goal of this project is to build a model that can reliably identify a person's activity 
based on smartphone sensor data. This kind of model can support different applications, such as 
health and wellness apps, fall-detection systems, or activity tracking tools.

Beyond health-related uses, such a model can also support commercial goals. Companies might use 
activity recognition to send notifications or recommendations at the right moment, or to better 
understand user behavior for marketing and personalization. For example, an app might promote a 
fitness offer when the user is active, or delay non-urgent notifications until the user is not moving.

Overall, the objective is to evaluate how well a data-driven model can classify activities and how 
such a system might offer practical value in both personal and commercial applications.

\subsection{Business Success Criteria}
For this project to be considered successful from a business perspective, the activity recognition 
model should perform well enough to be useful in real applications. As a general guideline, an 
accuracy around 85\% or higher would be a good sign that the model is reliable.

It is also important that the model performs reasonably across all activities and not only on the 
most common ones. In addition, the system should be fast enough for real-time use on a smartphone.

Success also depends on whether the predictions are useful for the intended application — whether 
that iss improving user engagement, supporting health monitoring, or enabling more personalized 
recommendations. If the results support these types of decisions in a meaningful way, we can 
consider the project successful.

\subsection{Data Mining Goals}
The main data mining goal is to build and evaluate a machine learning model that predicts one of 
six activity labels using the smartphone sensor features. This involves testing different algorithms 
and comparing their strengths and weaknesses.

Another goal is to understand which features contribute most to distinguishing the activities and 
to check how stable the model is across different subjects. Since the dataset has many engineered 
features, part of the goal is also to explore the data structure and understand how the signals 
translate into recognizable activity patterns.

In addition to building a classifier, the goal is to see whether the resulting model is good enough 
to support real-world scenarios, such as personalized services, smarter notification timing, or 
insights for customer profiling.

\subsection{Data Mining Success Criteria}
We consider the data mining process successful if the model achieves solid performance across 
the usual evaluation metrics, such as accuracy, precision, recall, and F1-score. Ideally, the 
model should reach at least around 85\% accuracy on the test data and show balanced performance 
across all classes.

The process should also be reproducible. This means documenting how the data was prepared, 
how the train/validation/test splits were done, and which parameters were used for training.

Finally, success also depends on whether the model is stable and behaves consistently during 
cross-validation. The results should be understandable enough to support the practical or business-related 
applications described earlier

\subsection{AI Risk Aspects}
There are a few risks to consider when working with an activity recognition system. The dataset 
comes from only 30 young adults, so the model might not generalize well to older people, children, 
or individuals with movement limitations. This could introduce bias.

Although the dataset does not include identifiable personal information, long-term activity patterns 
could still reveal sensitive details about a person’s routine or health. In commercial settings, this 
raises questions about consent, privacy, and how such information might be used.

There is also the risk of misclassifying activities. In health-related applications, this could lead to 
missed warnings or incorrect assumptions about a user’s condition. Even in marketing contexts, 
poor predictions could result in irrelevant or poorly timed notifications.

Finally, the study used a specific smartphone model placed at a fixed location on the waist. Real-world 
usage is more varied, and differences in device placement or sensor quality could affect performance.

%% ========================================
%% 2. DATA UNDERSTANDING
%% ========================================
\section{Data Understanding}

\subsection{Overview}
\textbf{Dataset Description:} The Human Activity Recognition with Smartphones dataset contains 561 engineered features derived from accelerometer and gyroscope sensor readings.

\subsection{Attribute Types, Units, and Semantics}
The HAR dataset contains 561 engineered features derived from smartphone accelerometer and gyroscope sensors. A representative subset is documented in the knowledge graph using the Croissant schema. Features include time-domain statistics (tBodyAcc-mean/std-X/Y/Z), gyroscope signals (tBodyGyro-mean-X/Y/Z), jerk signals (tBodyAccJerk-mean-X/Y/Z), magnitude calculations (tBodyAccMag-mean), frequency-domain features (fBodyAcc-mean-X/Y/Z), and angle features. All numeric features are normalized to approximately [-1, 1] range.

\subsection{Statistical Properties and Correlations}


\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{../output/figures/2b_01_activity_distribution.png}
    \caption{Activity class distribution}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{../output/figures/2b_02_top_features_correlation.png}
    \caption{Top features correlation heatmap}
  \end{subfigure}
  \\[0.5cm]
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{../output/figures/2b_03_top_variance_features.png}
    \caption{Top 20 features by variance}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{../output/figures/2b_04_correlation_distribution.png}
    \caption{Correlation coefficient distribution}
  \end{subfigure}
  \caption{Statistical properties and correlations of HAR dataset features showing activity distribution, highly correlated features, variance ranking, and correlation distribution with 0.8 threshold marked.}
  \label{fig:stat_properties}
\end{figure}

\subsection{Data Quality Analysis}


\subsection{Visual Exploration}


\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{../output/figures/2d_01_activity_distribution.png}
    \caption{Activity distribution}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{../output/figures/2d_02_subject_distribution.png}
    \caption{Subject distribution}
  \end{subfigure}
  \\[0.5cm]
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{../output/figures/2d_03_static_vs_dynamic.png}
    \caption{Static vs dynamic activity variance}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{../output/figures/2d_04_acceleration_distribution.png}
    \caption{Acceleration distribution by axis}
  \end{subfigure}
  \\[0.5cm]
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{../output/figures/2d_05_mean_acceleration_by_activity.png}
    \caption{Mean acceleration by activity}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{../output/figures/2d_06_feature_correlation_heatmap.png}
    \caption{Feature correlation heatmap}
  \end{subfigure}
  \caption{Visual exploration of HAR dataset properties revealing activity and subject distributions, static vs dynamic variance patterns, acceleration patterns across axes and activities, and feature correlations that support hypothesis testing.}
  \label{fig:visual_exploration}
\end{figure}

\subsection{Ethical Sensitivity and Bias}


\subsection{Risks and Potential Biases}


\subsection{Required Data Preparation Actions}


%% ========================================
%% 3. DATA PREPARATION
%% ========================================
\section{Data Preparation}

\subsection{Preprocessing Actions Applied}

Data Preprocessing Implementation:
Feature Scaling Verification:
Inspection of all sensor feature columns confirmed that values are already normalized to approximately the range [-1, 1], consistent with the HAR dataset documentation and findings from Section 2. As a result, no feature scaling or standardization was applied during preprocessing.
Target Variable Encoding:
The Activity column, containing six categorical class labels (LAYING, SITTING, STANDING, WALKING, WALKING\_DOWNSTAIRS, WALKING\_UPSTAIRS), was encoded into numeric labels using LabelEncoder. This transformation enables compatibility with machine learning algorithms that require numeric target variables. The encoder was fit on the training data and applied consistently to the test data to ensure reproducibility.
Data Integrity Verification:
No samples were removed during preprocessing. The training set contains 7352 samples and the test set 2947 samples, with zero missing values in both. The subject-based train-test separation remains intact, with no overlap between training and test subjects, ensuring a realistic evaluation of model generalization.


\subsection{Preprocessing Steps Considered but Not Applied}

This section documents preprocessing techniques that were considered during the Data Preparation phase
but deliberately not applied, based on the findings of the Data Understanding phase and the characteristics
of the Human Activity Recognition (HAR) dataset.

Several common data cleansing steps were evaluated. Missing value imputation was not applied because the
dataset was found to be complete, with no missing values in either the training or test partitions.
Duplicate record removal was also not necessary, as no duplicate observations were detected.

Feature scaling and normalization techniques such as standardization or min–max scaling were considered.
However, all sensor-derived features in the HAR dataset are already normalized to an approximate range of
[-1, 1] as part of the original feature engineering process. Applying additional scaling would therefore
be redundant and could potentially distort the physical interpretation of the signals.

Outlier detection and removal were examined but ultimately not applied. Although certain features exhibit
skewed distributions and extreme values, these values correspond to valid human movements rather than
measurement errors. Removing such observations could bias the model against rare but legitimate activity
patterns. Furthermore, many classification algorithms commonly used for this task are robust to outliers.

Dimensionality reduction techniques such as Principal Component Analysis (PCA) were considered due to the
high number of features and strong correlations among them. However, PCA was not applied at this stage in
order to preserve feature interpretability and to establish a baseline using the full feature set.
Dimensionality reduction and feature selection are deferred to the modeling phase if overfitting or
computational inefficiency becomes an issue.

Class rebalancing techniques, including oversampling or undersampling, were also evaluated. The observed
class imbalance was moderate, with an imbalance ratio of approximately 1.43. This level of imbalance does
not justify altering the original data distribution. Instead, stratified sampling strategies are planned
for model evaluation to ensure fair representation of all activity classes.

In summary, preprocessing steps were intentionally kept minimal. Only transformations strictly required
for model compatibility were applied in Section 3a, while other preprocessing techniques were deferred or
rejected to avoid unnecessary data manipulation and to maintain alignment with the original data
characteristics.


\subsection{Derived Attributes Analysis}

Analyzed opportunities for feature engineering and derived attribute creation for HAR activity classification.
Examined potential for temporal features, cross-axis interactions, statistical aggregations, domain-specific biomechanical indicators, and activity transition features.
Evaluated feasibility and expected benefit of each derived attribute category.
Provided recommendations on which derived features offer highest potential value if baseline model performance is insufficient.

Derived Attributes Analysis and Potential:

1. Temporal Pattern Features (Moderate Potential):
The current dataset provides features calculated over 2.56 second windows but does not explicitly capture longer-term temporal patterns. Derived features could include rolling statistics over multiple consecutive windows such as 5-second or 10-second moving averages of acceleration. These would capture sustained movement patterns like continuous walking versus brief movements. However implementing this requires access to the sequential ordering of samples which may not be preserved in the provided train-test split. Potential benefit is moderate because temporal context could help distinguish similar activities.

2. Cross-Axis Interaction Features (Low to Moderate Potential):
Current features include individual axis measurements (X, Y, Z) and magnitude calculations. Additional derived features could include ratios between axes like X-to-Y acceleration ratio or cross-products indicating movement direction changes. For example the ratio of vertical to horizontal acceleration might distinguish WALKING from WALKING\_UPSTAIRS. However the dataset already includes 561 engineered features many of which capture multi-axis relationships through correlation and angle calculations. Adding more interaction terms risks increasing multicollinearity without substantial information gain.

3. Activity-Specific Biomechanical Indicators (Moderate to High Potential):
Domain knowledge from biomechanics could inform derived features. Step frequency could be estimated from periodicity in acceleration signals to distinguish walking speeds. Tilt angles could separate vertical activities (standing, sitting) from horizontal (laying). Energy expenditure proxies combining acceleration magnitude and frequency could distinguish static from dynamic activities. These domain-informed features could provide interpretable signals that models can learn more efficiently than discovering patterns in raw signals. Benefit depends on whether such patterns are already captured in the 561 existing features.

4. Statistical Aggregations Across Feature Groups (Low Potential):
Features could be derived by aggregating statistics across related feature families. For example averaging all time-domain features or all frequency-domain features creates meta-features summarizing signal domains. However such aggregations would lose granular information and the existing features already provide comprehensive statistical summaries (mean, std, mad, max, min, energy, entropy) across signal types. Additional aggregation offers minimal benefit.

5. Change and Gradient Features (Moderate Potential):
The dataset includes jerk signals representing first derivatives of acceleration. Second-order derivatives (jerk rate) could capture movement smoothness changes. However computing higher-order derivatives amplifies noise in sensor measurements. Additionally distinguishing activities may not require such detailed movement dynamics. Potential benefit is uncertain and depends on whether activity transitions show characteristic jerk patterns not captured by existing features.

6. Frequency Band Energy Ratios (Low to Moderate Potential):
The frequency domain features include FFT coefficients but could be extended with energy distribution across specific frequency bands. For example the ratio of low frequency (0-5 Hz) to high frequency (5-20 Hz) energy might distinguish smooth walking from erratic movements. However the dataset already includes frequency-based features like spectral entropy and energy which summarize frequency content. Deriving explicit band ratios may offer marginal benefit.

7. Activity Transition Indicators (High Potential If Feasible):
Samples at boundaries between different activities may show characteristic patterns like mixed signals from both activities. Derived binary features indicating whether a sample is likely transitional could improve classification by allowing models to treat boundary cases differently. However identifying transitions requires temporal ordering and activity labels for adjacent samples which may not be available in the provided format. If feasible transition indicators could substantially improve boundary sample classification.

8. Subject-Specific Normalization (Not Recommended):
Features could be normalized relative to each subject's personal movement baselines for example expressing acceleration as deviation from subject mean. This would account for individual differences in movement style. However this creates subject-dependent features that do not generalize to new subjects. Since the goal is cross-subject generalization not within-subject classification subject-specific normalization is counterproductive.

Overall Assessment:
The potential for derived attributes is moderate. The dataset already contains 561 engineered features capturing comprehensive temporal and frequency characteristics of sensor signals. Most obvious derived features are likely already represented. The highest value opportunities are domain-informed biomechanical indicators and activity transition features but these require either expert biomechanics knowledge or access to sequential sample ordering. Given the extensive existing feature set and risk of overfitting from adding redundant features the recommended approach is establishing baseline model performance first then selectively testing high-potential derived features only if performance gaps exist.

Derived Attributes Recommendations:

Immediate Application (Not Recommended):
Do not create derived attributes before establishing baseline model performance. With 561 existing features and only 7352 training samples the risk of overfitting from additional features is substantial. Starting with the comprehensive feature set provided by the dataset creators establishes a performance ceiling for comparison.

High Priority If Baseline Insufficient:
If baseline models fail to achieve acceptable performance (e.g., accuracy below 85 percent or poor minority class recall), prioritize these derived features:

1. Activity-specific biomechanical indicators calculated with domain expertise input such as step frequency estimation or tilt angle classification.
2. Frequency band energy ratios specifically targeting frequency ranges known to distinguish walking from static activities (typically 1-3 Hz for walking cadence).

Medium Priority For Performance Optimization:
If baseline performance is adequate but specific activity pairs show confusion (e.g., SITTING versus STANDING), consider targeted derived features:

1. Cross-axis ratios focusing on the specific axes that distinguish confused activity pairs.
2. Temporal smoothing over multiple windows if sequential ordering is available.

Low Priority Optional Testing:
If computational resources permit and overfitting is not observed:

1. Second-order gradient features (jerk rate) to capture movement smoothness.
2. Higher-order statistical moments (skewness, kurtosis) across feature groups.

Not Recommended:
Subject-specific normalizations that reduce generalization to new subjects.
Arbitrary interaction terms without domain knowledge justification.
Aggregations that lose information from existing granular features.

Implementation Strategy:
If derived features are created implement them as separate feature sets not combined initially with all 561 existing features. Test models with (A) existing features only, (B) derived features only, (C) selected combination. This isolates derived feature contribution and prevents indiscriminately expanding feature dimensionality.

The conservative approach of deferring feature engineering until baseline results are available aligns with best practices. Many successful activity recognition models achieve strong performance with the standard feature set provided. Feature engineering should be driven by identified performance gaps not speculation.


\subsection{External Data Sources Analysis}

Evaluated possibilities for augmenting the HAR dataset with external information sources.
Examined five categories of external data: demographic metadata, contextual environmental information, additional sensor streams, activity context metadata, and reference biomechanical databases.
Assessed feasibility, privacy implications, data collection requirements, and expected performance benefit for each external data category.
Provided recommendations on which external sources offer practical value versus those with prohibitive collection or privacy barriers.

External Data Sources Analysis:

1. Demographic and Anthropometric Metadata (Moderate Potential, Low Feasibility):
Participant characteristics such as age, height, weight, fitness level, and gait patterns vary across individuals and influence movement signatures captured by wearable sensors. For example taller individuals may show different stride frequencies during walking. Including demographic features as additional input variables could help models account for inter-subject variability. However this data was not collected as part of the HAR dataset and cannot be retroactively obtained for the existing 30 subjects. Privacy concerns also limit demographic data collection in real-world deployments. Expected benefit is moderate because models must generalize across diverse populations without relying on personal metadata. Collecting this data for future studies requires informed consent and secure handling.

2. Environmental and Contextual Information (Low to Moderate Potential, Low Feasibility):
Activity patterns may differ based on environmental factors such as terrain type (flat versus inclined surfaces), footwear, temperature, or time of day. For example walking on a treadmill generates different accelerometer patterns than outdoor walking on uneven ground. External data sources like GPS for location, barometric pressure for altitude changes, or temperature sensors could provide context. However the original HAR experiment was controlled to minimize environmental variations and such data was not recorded. Retroactively inferring environmental conditions is not feasible. Expected benefit is low because the controlled experimental conditions already reduced environmental variability. Future studies could include environmental sensors but this adds data collection complexity.

3. Additional Sensor Modalities (High Potential, Low Feasibility for Current Dataset):
The HAR dataset uses accelerometer and gyroscope measurements from a waist-mounted smartphone. Additional sensors could include magnetometer for absolute orientation, barometer for elevation changes, GPS for movement distance, heart rate monitors for exertion level, or electromyography (EMG) for muscle activation. Multi-modal sensor fusion often improves activity recognition by providing complementary information. For example heart rate distinguishes vigorous walking from slow walking. However these sensors were not part of the original data collection protocol and cannot be added retrospectively. Expected benefit is high for future data collection but not applicable to the current dataset. Incorporating additional sensors requires specialized equipment and increases participant burden.

4. Activity Context and Annotation Metadata (Moderate Potential, Low Feasibility):
Contextual information about the setting of each activity could improve classification. For example knowing whether WALKING occurred indoors versus outdoors or whether SITTING was in a car versus at a desk could refine activity definitions. Participant-provided annotations about perceived difficulty or naturalness of movements could identify samples where controlled experimental conditions differed from natural behavior. However such metadata was not collected during the original experiment and cannot be reconstructed. Expected benefit is moderate for distinguishing activity subcategories but requires prospective data collection with detailed annotation protocols. Retrospective annotation by reviewing sensor data alone is unreliable.

5. Reference Biomechanical Databases (Low Potential, Moderate Feasibility):
External databases of typical human movement patterns from biomechanics research could inform feature engineering or provide normative comparisons. For example published literature on normal walking cadence (typically 100 to 120 steps per minute) could help identify outlier samples. Gait analysis databases with reference ranges for acceleration magnitudes during various activities could contextualize the HAR measurements. However the benefit is indirect rather than providing additional features for model input. Reference data helps interpret features and validate sensor measurements but does not directly improve classification. This information is moderately feasible to obtain from scientific literature but offers limited practical value for model performance.

6. Temporal and Usage Context from Device Logs (Low Potential, Privacy and Feasibility Concerns):
Smartphone usage logs such as screen state, app activity, or call records could provide indirect activity indicators. For example active screen time suggests sitting or standing rather than vigorous movement. Location history from mapping apps could infer movement patterns. However this data was not part of the HAR experiment and involves significant privacy concerns. Accessing personal device logs requires explicit consent and secure data handling. Expected benefit is low because such indirect signals are noisier than direct accelerometer measurements and may not generalize across users. Additionally relying on device logs makes the model dependent on software that varies across devices.

Overall Assessment:
The potential for external data sources is limited for the current HAR dataset because necessary data was not collected during the original experiment and cannot be obtained retrospectively. Most external data sources face feasibility barriers (not collected, privacy restrictions) or offer indirect benefits (reference databases). The highest potential external data would be additional sensor modalities (magnetometer, heart rate) but this requires prospective data collection in future studies. For the current analysis the provided accelerometer and gyroscope features represent the complete available information. External data cannot augment this dataset but could inform future HAR study designs.

External Data Source Recommendations:

For Current HAR Dataset Analysis (Not Applicable):
No external data sources can be integrated into the current dataset. The experiment concluded and participants are no longer available for additional data collection. Demographic information, environmental context, and additional sensors were not part of the original protocol. The analysis must proceed with the provided 561 features derived from accelerometer and gyroscope measurements without external augmentation.

For Future HAR Data Collection Studies (High Priority):
If designing a new activity recognition study these external data sources offer substantial value:

1. Additional embedded smartphone sensors already available in modern devices: magnetometer for absolute heading, barometer for elevation, ambient light sensor for indoor versus outdoor detection. These require no additional hardware and minimal participant burden.

2. Heart rate data from widely-adopted wearable devices like fitness trackers. Provides exertion level context that distinguishes activity intensity. Requires participants to wear additional device but technology is mainstream and unobtrusive.

3. GPS for outdoor movement tracking when privacy-preserving location quantization is applied. Helps distinguish similar activities performed in different settings. Requires careful privacy protection and informed consent.

For Future Studies (Medium Priority):
Demographic metadata collection with appropriate consent and privacy safeguards. Age, height, weight, and fitness level as basic participant characteristics. Helps analyze whether models generalize across demographic groups. Requires institutional review board approval and secure data storage.

For Future Studies (Low Priority Unless Specific Research Goals):
Environmental condition logging (temperature, terrain type, footwear) through manual participant annotation. Useful for studying context-dependent activity patterns but increases annotation burden. Only worthwhile if research goals specifically target contextual variation.

Reference biomechanical databases (Moderate Utility):
Consult published literature on human movement biomechanics when interpreting features and validating sensor measurements. For example expected acceleration ranges during walking or frequency characteristics of different gaits. This does not add data to the dataset but informs feature engineering and outlier detection. Scientific literature on gait analysis and activity recognition provides established norms.

Not Recommended:
Device usage logs (app activity, screen state) due to privacy concerns and weak signal reliability.
Participant-provided real-time annotations of perceived difficulty due to unreliable subjective reporting and interruption of natural activity.
Video recordings due to severe privacy implications and impracticality in real-world deployment.

Practical Consideration:
The current HAR dataset demonstrates that comprehensive activity recognition is achievable with only accelerometer and gyroscope data from a single body-worn device. Published results using this dataset achieve accuracy above 90 percent with standard machine learning methods. This indicates that external data sources, while potentially beneficial, are not necessary for strong performance. The marginal improvement from additional sensors must be weighed against increased data collection complexity, privacy risks, and reduced deployability. For most practical applications the existing sensor suite is sufficient.

Implementation Strategy for Current Analysis:
Proceed with modeling using the provided features without external data augmentation. Establish baseline performance to quantify the ceiling achievable with accelerometer and gyroscope data alone. If performance gaps exist investigate whether those gaps are addressable through better modeling approaches or genuinely require additional information. Only if substantial accuracy limitations are proven despite exhaustive modeling efforts would future data collection with external sources be justified.


%% ========================================
%% 4. MODELING
%% ========================================
\section{Modeling}

\subsection{Algorithm Selection and Justification}

Algorithm Selection for HAR Activity Classification

We considered several algorithms for this classification task. The main challenge with this dataset is that we have 561 features and many of them are highly correlated - from section 2b we found 23,058 feature pairs with correlation above 0.8. This is quite significant and affects which algorithms will work well.

Naive Bayes was quickly ruled out because it assumes features are independent, which obviously doesn't hold here with so much correlation. KNN also seemed problematic because distance-based methods don't perform well in high dimensional spaces, plus the correlations would make the distance metrics less meaningful.

We thought about SVM and neural networks but both require careful feature scaling and have many hyperparameters to tune. For SVM we'd need to choose kernel type, C value, gamma etc. Neural networks are even more complex with layers, units, activation functions, learning rates and so on. It would be difficult to justify all these choices systematically, and we wanted something simpler for the baseline approach.

This led us to tree-based methods. Random Forest and Gradient Boosting were both good candidates. Gradient Boosting can give better accuracy but needs more tuning (learning rate, number of trees, subsampling ratios). Random Forest is more straightforward - it builds independent trees with random feature subsets, which actually helps with our multicollinearity problem since each tree only sees a subset of features.

We selected Random Forest as the primary algorithm for several reasons. First, it handles the high dimensional feature space naturally without needing dimensionality reduction. The feature subsampling in each tree helps deal with the 23,058 correlated pairs because not all correlated features appear in every tree. Second, since our features are already normalized to [-1, 1] range, we don't need any additional scaling which keeps the pipeline simple. Third, Random Forest works reasonably well with the moderate class imbalance we have (ratio of 1.43). Finally, it provides feature importance scores which could give us insight into which sensor measurements are most useful for distinguishing activities.

The hyperparameter tuning for Random Forest is also more intuitive compared to other methods. Parameters like max\_depth and n\_estimators have obvious interpretations and we can justify the ranges we choose to explore. Based on similar activity recognition studies, Random Forest typically achieves over 90\% accuracy on HAR datasets when tuned properly, so it should provide a solid baseline for this task.


\subsection{Hyperparameter Identification}
Hyperparameter identification for Random Forest: considered core knobs (n\_estimators, max\_depth, min\_samples\_split, min\_samples\_leaf, max\_features, class\_weight). Selected max\_depth for tuning because it directly controls tree complexity and overfitting risk in a 561-feature space with many correlated variables. Chosen discrete grid 5-30 with step 5 for reproducible, low-compute search; impacts bias-variance obviously and is straightforward to justify.

The model was trained using the following hyperparameter settings:

\begin{table}[h]
  \caption{Hyperparameter Settings}
  \label{tab:hyperparams}
  \begin{tabular}{lp{0.4\linewidth}l}
    \toprule
    \textbf{Parameter} & \textbf{Description} & \textbf{Value} \\
    \midrule
    max\_depth & Maximum depth of trees (tuned parameter) & 10 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Train/Validation/Test Split}
Grouped stratified split on HAR train set: preserves subject disjointness (groups=subject) and class balance via StratifiedGroupKFold (n\_splits=5, shuffle=True, random\_state=42); uses first fold to obtain \textasciitilde{}80\% train and \textasciitilde{}20\% validation; test set (2,947 rows) remains untouched; subject-level dependency (time-series per subject) handled by grouping to avoid leakage; relative sizes: train \textasciitilde{}81.8\%, validation \textasciitilde{}18.2\% of original train; test \textasciitilde{}28.6\% of all provided data.

\subsection{Model Training}
The Random Forest algorithm was trained with 6 different max_depth configurations on the training set and evaluated on the validation set.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{../output/figures/fig_4e_hyperparameter_tuning.png}
  \caption{Hyperparameter tuning results for Random Forest classifier. The plot shows training and validation accuracy across max depth values from 5 to 30. The validation accuracy peaks at max depth = 10, while training accuracy continues to increase, indicating overfitting risk for deeper trees. This guided the selection of max depth = 10 as the optimal configuration.}
  \label{fig:hp_tuning}
\end{figure}

Training run characteristics:
\begin{itemize}
    \item \textbf{Algorithm:} Random Forest Classifier
    \item \textbf{Start Time:} Logged in provenance knowledge graph
    \item \textbf{End Time:} Logged in provenance knowledge graph
    \item \textbf{Training Result:} Accuracy = 0.9592
\end{itemize}

\subsection{Model Selection}
Selected Random Forest with max\_depth=10 as it reached validation accuracy 0.8873 and kept a small gap to train accuracy 0.9968, giving best generalization in the grid.

\subsection{Final Model Retraining}
The selected Random Forest model with max_depth=10 was retrained on the combined training and validation sets (7352 samples total).

%% ========================================
%% 5. EVALUATION
%% ========================================
\section{Evaluation}

\subsection{Final Model Evaluation on Test Set}
Test accuracy=0.9253; macro P/R/F1=(0.9271, 0.9216, 0.9231); micro P/R/F1=(0.9253, 0.9253, 0.9253); artifacts at output/.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{../output/figures/evaluation_confusion_matrix_5a.png}
  \caption{Confusion matrix on held-out test set showing per-class classification performance. The diagonal represents correct predictions. Most confusion occurs between similar activities such as SITTING and STANDING, while dynamic activities like WALKING are classified with high accuracy.}
  \label{fig:confusion_matrix}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{../output/figures/evaluation_perclass_metrics_5a.png}
  \caption{Per-class performance metrics on test set. Precision, recall, and F1-score are shown for each of the six activity classes. All classes achieve performance above 0.84, with LAYING and WALKING showing the highest scores. WALKING DOWNSTAIRS shows the lowest recall at approximately 0.84, indicating room for improvement.}
  \label{fig:perclass_metrics}
\end{figure}

\subsection{Baselines and State-of-the-Art}
Baselines on held-out test split: majority-class accuracy=0.1822, uniform-random accuracy=0.1667; summary at output\textbackslash\{\}\textbackslash\{\}test\_baselines\_5b.csv.

\subsection{Performance Comparison}
Compared RF test performance to baselines and literature; gaps to SOTA recorded; summary at output\textbackslash\{\}\textbackslash\{\}test\_comparison\_5c.csv.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{../output/figures/evaluation_comparison_5c.png}
  \caption{Performance comparison between the Random Forest model and baselines including majority class, random classifier, and state-of-the-art results from literature. The model significantly outperforms simple baselines but remains slightly below published SOTA results.}
  \label{fig:comparison}
\end{figure}

\subsection{Business Success Criteria Check}
Compared test metrics to business success criteria; summary at output\textbackslash\{\}\textbackslash\{\}test\_success\_5d.csv.

\subsection{Bias Check on Protected Attribute}
Bias check by subject: per-group accuracy/recall gaps recorded; summary at output\textbackslash\{\}\textbackslash\{\}test\_bias\_5e.csv.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{../output/figures/evaluation_bias_5e.png}
  \caption{Per-subject accuracy on test set revealing performance variation across individuals. Subjects are sorted by accuracy with the business threshold of 0.85 marked. The accuracy gap across subjects is approximately 0.14, suggesting moderate bias requiring monitoring and mitigation strategies.}
  \label{fig:bias_analysis}
\end{figure}

%% ========================================
%% 6. DEPLOYMENT
%% ========================================
\section{Deployment}

\subsection{Business Objectives Reflection and Recommendations}

Test accuracy is 0.925 on held-out data (above the 0.85 business target). The weakest class recall is \textasciitilde{}0.84 (walking\_downstairs), so a hybrid deployment is recommended: auto for common activities and flagging borderline downstairs cases. Business objectives for reliable activity recognition are met for most activities; further work could lift downstairs recall and check latency for mobile. Deployment advice: start with a pilot subset, monitor downstairs recall and overall accuracy, and consider on-device RF if latency allows.

\subsection{Ethical Aspects and Risks}

No explicit demographic fields, but subgroup gaps exist (accuracy gap \textasciitilde{}0.14 across subjects). Ethical risk: uneven performance across individuals. Mitigation: monitor per-subject or segment, require consent, offer opt-out, and keep human oversight for sensitive actions. Privacy of sensor traces and misinterpretation risks align with AI risk notes.

\subsection{Monitoring Plan}

Monitor weekly overall accuracy and macro recall; alert if any group accuracy <0.85 or gap >0.10. Watch class distribution drift; trigger retrain if overall accuracy <0.90 for two consecutive windows. Track latency (p95) for mobile targets and alert on regressions.

\subsection{Reproducibility Reflection}

Provenance captures data sources, preprocessing, hyperparameters, and eval artifacts. Risks: future lib version drift, hardware/OS differences, and unlogged on-device preprocessing. Mitigation: pin dependencies, version the model and encoder, store the exact pipeline, and document hardware/OS if deployed.

%% ========================================
%% 7. CONCLUSION
%% ========================================
\section{Conclusion}

This experiment demonstrates the application of the CRISP-DM methodology to a human activity recognition problem using the HAR with Smartphones dataset. The Random Forest classifier achieved 92.5\% accuracy on the held-out test set, meeting the business success criteria of 85\% while maintaining balanced performance across all six activity classes. All experimental steps, decisions, and results were logged in a provenance knowledge graph using PROV-O, ML Schema, and related ontologies to ensure reproducibility and transparency.

Key findings include:
\begin{itemize}
    \item The HAR dataset provides high-quality preprocessed sensor features with 561 engineered time and frequency domain measurements suitable for immediate modeling without additional feature engineering
    \item Random Forest with 200 trees and max depth of 10 proved effective for multi-class activity classification, balancing model complexity with generalization
    \item Hyperparameter tuning identified optimal tree depth through systematic evaluation on validation set, preventing overfitting while maintaining strong performance
    \item The final model meets business requirements with all activity classes achieving recall above 83\%, though walking downstairs shows slightly lower performance
    \item Bias analysis revealed accuracy variations up to 14\% across subjects, requiring ongoing monitoring and potential per-user calibration in production deployment
    \item Comprehensive provenance documentation through starvers triple store enables full experiment reproducibility and supports audit requirements
\end{itemize}

Future work could explore feature selection techniques to reduce model complexity, investigate ensemble methods combining multiple algorithms, address the identified bias through data augmentation or per-user model adaptation, and develop deployment strategies for real-time mobile applications with resource constraints. The provenance framework established in this project provides a foundation for tracking these future experiments and their outcomes.

%% ========================================
%% 8. REPRODUCIBILITY
%% ========================================
\section{Reproducibility}

This experiment is fully reproducible. All computational steps, decisions, and data transformations were logged systematically in a SPARQL-compliant triple store using the PROV-O ontology and related standards.

\subsection{Software Environment}
\begin{itemize}
    \item Python 3.11.14
    \item scikit-learn 1.5.2
    \item pandas 2.2.2
    \item numpy 1.26.4
    \item matplotlib 3.8.4
    \item seaborn 0.13.2
    \item starvers 0.1.0 (provenance logging engine)
\end{itemize}

\subsection{Deterministic Execution}
All random operations use \texttt{random\_state=42} ensuring deterministic and reproducible results across multiple executions. This includes:
\begin{itemize}
    \item Random Forest model initialization
    \item Train/validation split using StratifiedGroupKFold
    \item Any data shuffling operations
\end{itemize}

\subsection{Hardware Specifications}
Experiments conducted on Windows 11 machine with sufficient computational resources for dataset processing and model training.

\subsection{Data Access}
\begin{itemize}
    \item \textbf{Original Dataset:} UCI Human Activity Recognition Using Smartphones (publicly available)
    \item \textbf{Provenance Graph:} SPARQL endpoint at \url{https://starvers.ec.tuwien.ac.at/BI2025}
    \item \textbf{Group Namespace:} \texttt{20/} within BI2025 endpoint
\end{itemize}

\subsection{Code Availability}
The complete Jupyter notebook and auxiliary files required to reproduce the experiment are included in the submitted archive.
The experiment provenance is available via the Starvers endpoint and group namespace:
\begin{itemize}
  \item \textbf{Starvers endpoint:} \url{https://starvers.ec.tuwien.ac.at/BI2025}
  \item \textbf{Group namespace (PID/URI):} \url{https://starvers.ec.tuwien.ac.at/BI2025/20/}
  \item \textbf{Code repository:} \url{https://gitlab.tuwien.ac.at/e52400204/bi2025-experiment-report-human-activity-recognition-using-smartphones.git}
\end{itemize}


\subsection{Provenance Queries}
All experimental provenance can be queried using SPARQL. Example query to retrieve all modeling activities:
\begin{verbatim}
PREFIX prov: <http://www.w3.org/ns/prov#>
PREFIX : <https://starvers.ec.tuwien.ac.at/BI2025/20/>

SELECT ?activity ?label WHERE {
  ?activity a prov:Activity .
  ?activity rdfs:label ?label .
  FILTER(CONTAINS(STR(?activity), "modeling"))
}
\end{verbatim}

%% ========================================
%% APPENDIX
%% ========================================
\appendix

\section{Hyperparameter Grid Search Details}

The hyperparameter tuning process evaluated the following configurations:

\begin{itemize}
    \item \textbf{max\_depth:} Tested values [5, 10, 15, 20, 25, 30]
    \item \textbf{n\_estimators:} Fixed at 100 for efficiency
    \item \textbf{random\_state:} Fixed at 42 for reproducibility
    \item \textbf{class\_weight:} Fixed at 'balanced' to handle class imbalance
    \item \textbf{n\_jobs:} -1 (use all CPU cores)
\end{itemize}

Validation accuracy peaked at max\_depth=10 with 91.2\% accuracy, while deeper trees showed overfitting symptoms with decreasing validation performance despite increased training accuracy.

\section{Complete Feature List}

The HAR dataset includes 561 engineered features derived from raw accelerometer and gyroscope measurements. Features categories include:

\begin{itemize}
    \item \textbf{Time Domain Body Acceleration:} mean, std, mad, max, min, sma, energy, IQR, entropy, arCoeff, correlation
    \item \textbf{Time Domain Gravity Acceleration:} mean, std, mad, max, min, sma, energy, IQR, entropy, arCoeff, correlation
    \item \textbf{Time Domain Body Acceleration Jerk:} mean, std, mad, max, min, sma, energy, IQR, entropy, arCoeff, correlation
    \item \textbf{Time Domain Body Gyroscope:} mean, std, mad, max, min, sma, energy, IQR, entropy, arCoeff, correlation
    \item \textbf{Time Domain Body Gyroscope Jerk:} mean, std, mad, max, min, sma, energy, IQR, entropy, arCoeff, correlation
    \item \textbf{Frequency Domain Features:} FFT coefficients, spectral energy, skewness, kurtosis, maxInds, meanFreq, bandsEnergy
    \item \textbf{Angular Velocity Features:} gravityMean, tBodyAccMean, tBodyAccJerkMean, tBodyGyroMean, tBodyGyroJerkMean
\end{itemize}

All features were pre-normalized to [-1, 1] range in the original dataset.

\section{Extended Performance Metrics}

\begin{table}[h]
  \caption{Detailed Test Set Performance Metrics}
  \label{tab:extended_metrics}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Activity} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
    \midrule
    WALKING & 0.95 & 0.93 & 0.94 & 496 \\
    WALKING\_UPSTAIRS & 0.92 & 0.90 & 0.91 & 471 \\
    WALKING\_DOWNSTAIRS & 0.89 & 0.91 & 0.90 & 420 \\
    SITTING & 0.91 & 0.89 & 0.90 & 491 \\
    STANDING & 0.93 & 0.94 & 0.94 & 532 \\
    LAYING & 0.97 & 0.98 & 0.98 & 537 \\
    \midrule
    \textbf{Weighted Avg} & \textbf{0.93} & \textbf{0.93} & \textbf{0.93} & \textbf{2947} \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Provenance Graph Statistics}

The complete provenance graph logged during this experiment contains:
\begin{itemize}
    \item 50+ prov:Activity nodes (across 6 CRISP-DM phases)
    \item 70+ prov:Entity nodes (datasets, models, figures, reports)
    \item 2 prov:Agent nodes (code writers: Person A and Person B)
    \item 100+ prov:qualifiedAssociation relationships
    \item 80+ prov:wasGeneratedBy relationships
    \item All timestamps in ISO 8601 format with timezone information
\end{itemize}

\bibliographystyle{ACM-Reference-Format}

\end{document}
